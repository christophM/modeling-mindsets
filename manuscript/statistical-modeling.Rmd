# Statistical Modeling {#statistical-modeling}


* Goal: Changing your mind under uncertainty.
* Assumes the world is best described by probability distributions.
* Requires additional assumptions for drawing conclusions. See [frequentism](#frequentism), [Bayesianism](#bayesian) and [likelihoodism](#likelihoodism).



<!-- statistician modeling mindset ingredients

- Ingredients: Random variables, distributions,
- Statistical estimators
- https://en.wikipedia.org/wiki/Statistical_model
- TODO: Distinguish between statistical model and statistical inference?
- TODO: IID assumption: talk about it
- TODO: Mention that most models make simplifying assumptions, such as typical for the linear model
- TODO: Write a paragraph about evaluation
-->
<!-- TODO: Mention all the types of statistical models one can build with this mindset: clustering, classification, regression, outlier detection, ... -->

<!-- 
TODO: write about

- interpretation of parameters important
- but also expected values and such, but that's often what the parameters stand for.

The parameters should have a well-defined meaning.
This is an (intuitive) assumption that is violated for [semiparametric models](#semiparametric).


Choosing a distribution is not enough.
The probability distributions have parameters.
And those parameters can be adapted so that the distributions fit to the data.

- more elaborate way than just have some distribution and paramters: statistical models pull these things together and there are many ways to fit 
-->


<!-- motivation -->
Do you become more productive when you drink a lot of water?
Is there a fixed "law", like a mathematical function, that expresses productivity as a function of water intake?
No.
No, because productivity depends on many factors: sleep duration and quality, distractions, noise pollution, ...
Because of all these factors and other contingencies, we won't get a perfect equation relating water intake to productivity.
Uncertainty remains.
Even the water intake varies from day to day and from person to person.

<!-- summary of likelihood mindset -->
Statistics is all about changing your mind under uncertainty.
One way to deal with the uncertainty of the world is to abstract aspects of the world as random variables and ascribe a probability distribution to them.
Daily water intake could be a random variable.
For productivity we would need some clever idea on how to best measure this abstract concept.
A not so clever example: Daily time spent in front of the computer.
To further relate these variables to each other, we can make assumptions on how the data were generated and connect the random variables in a statistical model.

Welcome to the **statistical modeling** mindset.

<!-- What is statistical model -->
A statistical model consists of a set of probability distributions that are fitted to data.
A probability distribution describes the frequency with which we expect to see certain values of random variables.
The second ingredient to a statistical model is the data, from which we estimate those probability distributions.
But let's start with the most elementary unit: the random variable.

<!-- 
[Semiparametric models](#semiparametric) and especially [distribution-free statistical modeling](#distribution-free) approach modelling with a different mindset, but are considered statistical modeling.
-->

<!-- 
Assumptions:

- Data are iid:
- independently drawn
- identically distributed
- no confounding effects
- linearity (commonly)
- 
-->




## Random Variables

<!-- Random Variables --> 
A random variable is a mathematical object that carries uncertainty.
Daily water intake can be a random variable.
Data are seen as **observations** or realizations of random variables.
If someone drank 2.5 liters of water, that is a realization of the random variable "daily water intake".

Other random variables:

* Outcome of a dice roll.
* Monthly sales of an ice cream truck. 
* Whether or not a customer has canceled their contract last month.
* Daily amount of rain in Europe.
* Pain level of patients arriving at the emergency room.


<!-- TOOD: Visualize World+random variable -> Data/Observations -> Model/estimated probability distributions -->

People with a statistical modeling mindset **think** in random variables.
Any problem related to data is translated into a set of random variables and solved based on that representation.
A random variable is a construct that we can't observe directly.
But we can observe the realizations of a random variable, as shown in Figure \@ref(fig:variable).
But the raw data are not that useful to humans.
Because humans aren't databases, we need a model that simplifies the noisy data for us.
Statisticians came up with probability distributions: a mathematical construct that describes potential outcomes of the random variable.

<!--
There are usually many choice how we translate a world property into a random variable : 
Should water intake be measured per week, day or even hour?
Do we respect the water contained in consumed food?
What about sweating?

Sometimes what later becomes a random variable is already dictated by the data we collected:
When the water intake was measured daily, then that's also the random variable.
-->
```{r variable, fig.cap = "Each dot represents a data point, a realizations of a random variable. The x-axis shows the value of the variable. Dots are stacked up for frequently occurring values. ", fig.height = 3, fig.width = 9}
library(ggplot2)
set.seed(1)
n = 25
xsample = rnorm(n, mean = 1, sd = 2)
xx =  density(xsample)
xx = data.frame(x = xx$x, y = xx$y, type = "real")
#xx = rbind(xx, dat)

xdat = data.frame(x = xsample, type = "egal", y = 0)

ggplot(xdat) +
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05) +
 theme_void() +
  theme(axis.text.x = element_text()) +
  scale_y_continuous("", limits = c(0, 3)) +
  coord_fixed()

```

## Probability Distributions

<!-- Definition of a distribution -->
A probability distribution is a function which gives each possible outcome of a random variable a probability.
Value in, probability out.
Not all functions can be used as probability functions.
A probability function must be larger or equal to zero for the entire range of the random variable.
For discrete variables such as the number of fish caught, the probability distribution must sum up to 1 over all possible outcomes.
And for continuous outcomes such as water intake, the probability distribution, also called density function, must integrate to 1 over the possible range of values.

<!-- examples of probability distributions -->
For the outcome $x$ of a fair dice, we can write the following probability function:

\[
P(x) = 
\begin{cases}
 1/6 & x \in \{1, \ldots, 6\} \\
 0 & x \notin \{1, \ldots, 6\} \\
\end{cases}
\]

The Normal distribution is for continuous variables and defined from minus to plus infinity:

$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

<!-- what each part means -->
In this equation, $\pi$ and $e$ are the famous constants $pi \approx 3.14$ and $e \approx 2.7$.
The distribution has two parameters: mean $\mu$ and standard deviation $\sigma$.
<!-- the parameters -->
These parameters are sometimes called location ($\mu$) and scale ($\sigma$) parameters, since they determine where on the x-axis the center of the Normal distribution is and how flat or sharp the distribution is. 
The larger the standard deviation $\sigma$, the flatter the distribution.

<!-- how it works -->
Let's try it out and set $\mu = 0$ and $\sigma = 1$, as visualized in Figure \@ref(fig:distributions), leftmost curve.
Now we can use this probability distribution function for telling us how probable certain values of our random variable are.
We get $f(1) \approx 0.24$ and for $f(2) \approx 0.05$, making $x=1$ much more likely then $x=2$.
We may not interpret $f(x)$ as probability directly.
But we can integrate $f$ over a region of the random variable to get a probability.
The probability for $x \in [0.9, 1.1]$ is 4.8\%.

<!-- supermarket of distributions -->
There is huge arsenal of probability distributions: The Normal distribution, the Binomial distribution, the Poisson distribution, ...
And there is an infinite number of probability distributions that you could invent yourself.

```{r distributions, fig.cap = "Distributions", fig.height = 3, fig.width = 10}
library(ggplot2)
library(patchwork)
n = 100
# Normal
x = seq(from = -4, to = 4, length.out = n)
y = dnorm(x) 
dat = data.frame(x = x, y = y, pd = "Normal")
p1 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        axis.text.y = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Normal")


x = seq(from = 0, to = 10, length.out = n)
y = dgamma(x, shape = 2, rate = 1) 
dat = data.frame(x = x, y = y, pd = "Gamma")
p2 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Gamma")



x = seq(from = 0, to = 8)
y = dpois(x, lambda = 1)
dat2 = data.frame(x = x, y = y, pd = "Poisson")
p3 = ggplot(dat2, aes(x = x, y = y)) +
  geom_col(fill = "white", color = "black", size = 0.8, width = 0.5) +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:max(x)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Poisson")



nrep = 5
x = seq(from = 0, to = nrep)
y = dbinom(x, p = 0.7, size = nrep)
dat3 = data.frame(x = x, y = y, pd = "Binomial")
p4 = ggplot(dat3, aes(x = x, y = y)) +
  geom_col(data = dat3, fill = "white", color = "black", size = 0.8, width = 0.5) +
  theme_void() +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:nrep) +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Binomial")

p1 | p3 | p2 | p4

```



## Assuming a Distribution

<!-- back to data -->
An important step in the statistical modeling mindset is to connect the random variables that we are interested in to distributions.
A common approach is to choose a distribution that matches the "nature" of your data:

* A numerical outcome such as IQ? That's Normal distribution.
* A count outcome such as number of fish caught in an hour? The Poisson distribution is a solid choice.
* A binary repeated outcome, like the number of successful free throws in basketball? It follows a Binomial distribution.

<!-- assumption vs. estimation -->


<!-- multidimensional distributions -->
These were examples 1-dimensional distributions that only consider a single variable.
The world is more complex than that:
Random variables can be correlated.
The distribution of one variable can depend on the value that another random variable takes.
Fortunately, it's possible to define so-called multivariate distributions.
A multivariate distribution is a function that takes as input two ore more values and returns a density (still a single number).
We might assume that the joint distribution of water intake and productivity (measured in minutes) follows a 2-dimensional Normal distribution.

<!-- conditional distribution -->
Another option is conditional distributions.
For example, we could assume that productivity, conditional on water intake, follows a Normal distribution.

<!-- dream land -->
With all these probability distributions, we are still in the realm of abstraction and assumptions.
On one side we have the data: messy and untamed.
On the other side, we have the probability distributions: clean and idealized.
Via random variables we have at least a theory how the two are connected:
The data are the realizations of variables, and distributions summarize the stochastic behaviour of variables.
But we still need to mathematically connect observed data and theoretical distributions.
How can we connect them?

The answer is **statistical models**.

## Statistical Model

<!-- definition of a statistical model -->
A statistical model connects theoretical distributions with observed data.
Statistical models are mathematical models that make assumptions about how the data are generated.
With these assumptions in the background, statistical models are then estimated using data.
More formally, a statistical model is the combination of the sample space from which the data comes from, and a set of probability distributions on this sample space.

<!-- TODO: Visualize statistical model as connection between data and distributions -->

<!-- Parameters -->
<!-- 
Other distributions have other parameters. [^semiparametric]
-->

<!-- TODO: regression models, non- and semi-parametric -->

<!-- Estimation intuition -->
The distributions are "fitted" to the data by changing the parameters.
Imagine the distribution as a cat.
And your data is a box.
Your cat fits it's shape and position to match the box.

<!-- TODO: visualize cat in box -->

<!-- where params come from -->
How does the cat know which form to take on?
Ah pardon, our question now is:
How do we find parameter values so that the distribution fits the data well?
The density function of the Normal distribution, for example, has mean and variance as parameters.
Given the parameters, the density or probability function can tell us how likely certain values of our random variables are.

<!-- from density to likelihood -->
We can also use the probability or density function to find our parameters -- by reversing the point of view and calling it the likelihood function.
The value of the random variable is the input of the probability function, and the parameters are seen as given.
The likelihood function $L(\theta,  X)$ is equal to the probability function.
Except that the parameters are now the input, and the values of the random variable are seen as given.
They are given, in the sense that we have data that are realizations of the random variable.

<!-- how to compute -->
We can take an observed value for $X$ from our data and plug it into the likelihood function.
Now we have a likelihood function that can tell us, for this one data point, which parameters would give the highest probability for observed this particular value.
But our data consist of multiple realizations of random variables.
To get the likelihood for the entire dataset, we multiply the likelihoods of the individual data points.
This data likelihood can tell us, for a given parameter value, how likely our data is.
For example we could try  $\mu = 1$ and $\sigma = 2$ and the likelihood function returns a value.
The larger the value, the better the distribution (with these parameters) fits the data.
That's very useful, because it helps us in finding the best parameters.


<!-- maximum likelihood -->
Finding the best parameters is a classical optimization problem:
Maximize the data likelihood $L$ with respect to the parameters.
We want to maximize the likelihood for all of our data: $L(\theta, \mathbf{x}_1, \ldots, \mathbf{x}_n) = \prod_{i=1}^n L_i(\theta, \mathbf{x}_i)$. <!-- vim_ -->
Note that $\mathbf{x}_i$ <!-- x_ --> can be a vector with values for multiple variables.
And we maximize the data likelihood:

$$\arg \max_{\theta} L(\theta | \mathbf{x}_1, \ldots, \mathbf{x}_n)$$ <!-- vim_ -->

### Maximum Likelihood Estimation {-}

<!-- Estimation technically -->
Maximum likelihood estimation is a classic optimization problem.
For simpler cases this can even be solved analytically.
For the Normal distribution, we know that the optimal $mu$ is the mean of the data: $\frac{1}{n} \sum_{i=1}^n x_i$.
When an analytical solution is not possible, other optimization methods like gradient descent, the Newton-Raphson method and Fisher's scoring can be used.

<!-- maximum likelihood mental model -->
Maximum likelihood estimation is a key element to understanding the statistical modeling midnset.
Maximizing the likelihood means bringing together the theoretical probability distributions and the observed data.

```{r fit, fig.cap = "Fitting distributions to data", fig.height = 2, fig.width = 10}
# different means
x = seq(from = -15, to = 15, length.out = 1000)
means = c(-5, 1, 8)
d1 = data.frame(x = x, y = dnorm(x, mean = means[1], sd = 2), type = "1")
d2 = data.frame(x = x, y = dnorm(x, mean = means[2], sd = 2), type = "2")
d3 = data.frame(x = x, y = dnorm(x, mean = means[3], sd = 2), type = "3")
dd = data.frame(data.table::rbindlist(list(d1, d2, d3)))
# Make smaller to match dotplot
dd$y = dd$y 
# position of the question marks
ypos = 0.7 * max(dd$y)
ggplot(dd, aes(x=x,y=y, group = type)) +
  geom_area(alpha = 0, position = "identity") + 
  scale_fill_discrete(guide = "none") +
  geom_line(size = 1.5, color = "black") +
  annotate("text", label = "?", x = means[1], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[2], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[3], y = ypos, size = 12) + 
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05, data = xdat) +
  scale_y_continuous(limits = c(0, max(dd$y))) +
  theme_void()

```

### Intepreting Parameters {-}

<!-- role of the parameters -->
The parameters are not only things to be optimized.
Their role goes beyond that of a mere technical controller.
The parameters are central to the statistical modeling mindset.

**Statisticians interpret the parameters as summary of the data.**

Statistical modeling is about understanding our data better.
The nice consequence of modeling the data with probability distributions is that we summarize our data with just a few numbers, the parameters.
In the case of the Normal distribution, we can describe the distribution of a single variable with only two numbers, the mean and the standard deviation.
Also for other types of statistical models, parameters are central to interpretation.



<!-- example for likelihood mindset -->
<!--
Let's go back to the water and productivity example.
You want to answer this question of water versus productivity  with data.

First step: You collect lots of data, measure your daily water intake, and the number of tasks you get done.
Fully embracing the statistical modeling mindset, we express information as random variables.
One variable is "water intake" $X_{water}$ and the other is "Number of productive hours" $X_{hours}$.
Next, we assume that water intake + words follow a 2-dimensional Normal distribution.
The multivariate Normal distribution has 5 parameters.
Mean and variance for water intake, and the same for productivity.
The fifth parameter is the covariance between water and productivity.

Our statistical model is now ready for parameter estimation.
We can use maximum likelihood to solve find the optimal parameter.
Optimal means: the values for the five parameters that maximize the likelihood function.
We write down the formula for the probability distribution of our data.
And then we maximize it for the parameters, given the data.
The resulting parameters are then our estimates for the parameters.
This now parameterized distribution is the statistical model of our little office world.
-->
<!-- end of example -->

<!-- what types of models are possible -->


## Types of Statistical Models

<!-- 
- 1D distributions
- but also mulitvariate
- with more than one variable, we have two options
- full joint distribution
- conditional distribution
- conditional distribution ->  build regression models
- also clustering is possible
- ANOVA, regression models, survival model / cox model, Justification
-->

All statistical models target an aspect of a probability distribution.
For most of the chapter, we talked about simpler cases, like the distribution of a single variable: $P(X)$.

<!-- joint versus conditional  -->
But we could have the case of multiple random variables $X_1, \ldots, X_p$.
To describe the full distribution of more than one variable, we have to work with the joint distribution.
The joint distribution can be difficult to estimate.
It's often much simpler to work with *conditional distributions*.
The conditional distribution tells us how probable the outcome of one or more random variable is, given we already know the values of some other random variables.

CONTINUE HERE

<!-- aspect of the distribuion -->
Depending on the analytical task, the statistician can target different "areas" of the distribution.
In many cases, the quantity of interest is a conditional mean: "What do we expect for a random variable $Y$ given $X= \mathbf{x}$?
But it can also be other parts of the distribution: "What's the 90\% quantile for $Y$ given $X = \mathbf{x}$?
At the 90\% quantile 10\% of the values for $Y$, given $X=\mathbf{x}$, would be greater than the 90\% quantile.
Think of insurance: An insurance company is noy only interested in the expected damages they have to cover, but also how much they would have to cover in the extreme case. 

<!-- Gaussian mixture, joint example -->
A Gaussian mixture model, for example, requires learning the entire distribution. 
Gaussian mixture models can be used for identifying clusters in the data, which are assumed to stem from a mixture of Normal distributions.
With Gaussian mixture models we also seen an example where maximum likelihood is not the sole optimization approach.
Instead, the expectation-maximization algorithm is used, which iteratively jumps between optimizing the model parameters and predicting the "mixture" of cluster centers for each data point.

<!-- motivation: regression models -->
The joint distribution is not always of interest.
So often, the modeler is only interested in the conditional distribution:
How does the probability distribution of a variable depend on other variables?
For example:

<!-- example of conditional outcomes -->
* What are risk factors that influence the probability of getting lung cancer?
* How do climatic conditions like temperature and humidity affect the occurence of bonobos?
* On which days is a hospital likely to be understaffed?  

<!-- advantages of conditional -->
The conditional distribution is the natural form for prediction tasks and usually simpler to estimate than the joint distribution.
Models of the conditional distribution are central to statistical modeling.
They are also known as regression models. 

## Regression Models {#regression-models}

<!-- regression models -->
Regression models are statistical models that don't learn the joint distribution, but a conditional distribution.
Let's say we have two variables: $Y$ and $X$.
With the join distribution $P(Y,X)$ we could answer "How likely is a certain combination of $X=x$, and $Y=y$.
But with the joint distribution, we can ask: "Given $X=x$, what is the probability distribution of $Y$"?

<!-- an example -->
For example, we want to know not only how often a disease is successfully treated.
We might want to know if a certain drug played a part in the disease outcome.
And other factors such as age of the patient, progression of the disease and so on might play a role as well.
And we would have to consider such factors to answer the question on whether the drug helps.

<!-- how it works -->
Our target is the distribution of outcome variable $Y$ conditionanl on variables $X_1, \ldots, X_p$.
That means that within the regression model the parameters of the distribution of $Y$ are linked to the other variables.
How exactly this link looks like depends on the distribution that the modeler assumed for $Y$ and the link they chose to connect the two.
The simplest case is a linear model.
We assume that $Y$ follows a Normal distribution and link the mean of $Y$'s distribution to a weighted sum of the other variables:

$$Y \sim N(\mu, \sigma)$$

$$\mu = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

<!-- Regression model = conditional distribution -->
The linear regression model expresses the mean of the target $Y$ as the weighted sum of the other variables. 
$Y$ given $\mathbf{X}$ follows a Normal distribution.
We only link the mean $\mu$ to the variables.
A typical assumption is that the $\sigma$ is independent of the value of the other variables.

<!-- what can we do with model -->
What can we do now with a regression model?
<!-- goal: prediction -->
We can make predictions.
We just have to fill in values for $X$ and get the expected value of the probability distribution of $X$.
<!-- goal: interpretation -->
Another typical goal is interpreting the relationship between the target with the other variables.
The coefficients in the regression model are subject to interpretation.
For the linear regression model, a positive coefficient $\beta_j$ means that increasing the value of variable $X_j$ increases the expected value of $Y$. 

<!-- glm and gam -->


<!--
So [frequentists](#frequentism) can derive confidence intervals, p-values and hypothesis tests for them ("Is coefficient $\beta_j$ significantly different from $0$?").
[Bayesians](#bayesian) would additionally assume a prior distribution for $\mathbf{\beta}$ and we would get as a result the posterior distributions of the coefficients to analyze. 
-->

<!-- bridge to ML -->
Thinking in regression models brings you closer to the [supervised machine learning](#mindset).
However, in supervised  machine learning, the focus is usually on getting a good predictive model.
Not on finding a finding a probabilistic representation of your data with random variables and distributions.

<!-- on classification models -->
But what if the outcome is a category, and our task is classification?
When you come from the [supervised machine learning](#supervised-ml) mindset, always the distinction between classification and regression models is made.
In statistical modeling, we don't model the categories, but their probability distribution.
From the statistical modeling mindset, a categorical variable is nothing special.
Like for all other variables, we model the conditional distribution of the variable.
And if we make predictions with our model, we get the probabilities for the categories.



## Model Evaluation

<!-- importance of evaluation for understanding mindset -->
Statistical models can be evaluated.
The evaluation is very revealing to understand the mindset.
And especially how the statistical modeling mindset differs from the [supervised machine learning](#supervised-ml) mindset.
Evaluation consists of model diagnostics and goodness-of-fit measures.

<!--  diagnostics -->
The role of model diagnostics is to check whether the modeling assumptions were meaningful.
If we assumed that a random variable follows a Normal distribution, we can visually check whether that's really the case using, for example, the Q-Q plot.
An assumption for linear regression model that I mentioned : The variance of the outcome does not depend on the values of the other variables.
This can be checked visually by plotting the residuals (value of $Y$ minus predicted value of $Y$) against each of the other variables.

<!-- goodness-of-fit -->
The role of goodness-of-fit measures is model comparison and evaluating modeling choices.
Typical measures here are the (adjusted) R-squared, Akaikes Information Criterion (AIC), the Bayes factor, and likelihood ratios.
Goodness-of-fit are, quite literally, measures that tell us how well our model fits the data.
Goodness-of-fit metrics can guide the model building process and decide which model to choose in the end.

<!-- compare evaluation to ML -->
Goodness-of-fit metrics are typically computed with the same data that were used for fitting the statistical models.
This choice may look like a minor detail, but it says a lot about the statistical modeling mindset.
The critical factor here is overfitting: The more flexible a model is, the better it adapts to ALL the randomness in the data instead of learning patterns that generalize.
Many goodness-of-fit metrics therefore account for model complexity, like the AIC or adjusted R-squared.
For the [supervised machine learning](#supervised-ml) mindset, you would always use new, unseen data for the evaluation.

## Data-Generating Process (DGP)

<!-- what is the DGP -->
A quite central, but fuzzy topic of the statistical modeling mindset is the data-generating process.
The statistical modeler thinks about the data-generating process all the time.
The data-generating process is a construct, an unknowable ideal of how the data was generated.
The data-generating process produces the unknown distributions that than produce the data.
We can only observe the data-generating indirectly by oberving data.
And we can come closer to the DGP by reasoning about it.
You won't find this in any statistics book explicitly.
But it's the mindset of statistical modeling to have this idea of the DGP.
It's a natural consequence when you think about the world as random variables and distributions.

<!-- DGP detective -->
I think the DGP is a very powerful idea, even when it's not well defined.
Assuming a DGP encourages you to intellectually dive deep into your data.
Having this image of a DGP in your mind let's you take on the mindset of a detective:
Statisticians are like detectives reconstructing a crime.
You can't observe the crime directly.
But the crime has generated a lot of "data" at the scene.
The stats detective then tries to uncover the data-generating process by making assumptions and learning from data.

<!-- examples of data-generating processes -->
"Defining" the data-generating process is neither simple nor well-defined, but I'll give it a try:

* Dice roll: the throw of the dice is the data-generating process. The dice is symmetric, which makes each side equally likely. We could factor in throwing angle, surface roughness and so on, but the chaotic behaviour of the dice jumping and spinning over the table makes us dismiss all these factors.
* We study the income of computer scientists via a survey. Instead of only reporting on the income distribution, we think about the entire data-generating process: For example, some income values are missing. Are they missing at random? Or are maybe people with higher incomes omitting the question? Are some companies overrepresentated in the survey? Is the sample truly random?
* A research team has collected chest x-ray images of patients with and without COVID-19 for building a COVID-19 prediction model. A closer look at the data-generating proces shows: the images not only differ in COVID-19 status, but they come from different data-generating processes. COVID-19 images are more likely from a horizontal position where the patient lies down because they are so exhausted. One of the non-COVID-19 datasets are even just children x-ray images. [^Wynants2020] I picked this example, because it is a paper from a [machine learning mindset](#machine-learning). Statisticians would think much more about the data-generating process, and are, IMHO, more likely to spot such mistakes.

If these examples of data-generating process sound like common sense to you, it's because they are.
But it's surprisingly uncommon among non-statistician mindsets.
For example, for [machine learning](#machine-learning) considerations of the data-generating process play a subordinate .
For machine learning competitions, for example, the winner is, most of the times, solely determined by the lowest loss function.
It's not considered when some approach has a more meaningful consideration of the data-generating process, and an understandable model.

Sometimes we want to control at least parts of the data-generating process.
In randomized clinical trials, for example, we control who gets the drug and who get the placebo.
In other controlled experiments we even control more factors.

## Drawing Conclusions About the World

<!-- from world to model -->
Statisticians collect data about the world and use that data to fit a statistical model.
The statistical model links the world and the data through random variables:
The world can be simplified by probability distributions;
the data are viewed as realizations of the random variables.

<!-- from model to world -->
Statistical modeling is a practical endeavour.
In most cases, statistical models are built for practical reasons:
To make a decision, to better understand some property of the world, or to make a prediction.
These goals requires us to interpret the model instead of the world.
But this interpretation does not come for free.
After all the model building and evaluation, the statistical modeler must consider the representativeness of the data, the interpretation of probability and causality.

<!-- Part 1: Representativeness -->
Considering the data-generating process also means thinking about the representativeness of the data, and thus the model.
Are the data a good sample and representative of the quantity of the world you are interested in?
Let's say a statistical modeler analyzes data on whether a sepsis screening tool successfully reduced the incidence of sepsis in a hospital.
They conclude that the sepsis screening tool has helped reduce sepsis-related deaths at that hospital.
Are the data representative of all hospitals in the region, the country, or even the world?
Are the data even representative of all patients at the hospital, or are data only available from patients in intensive care unit?
In the statistical modeling mindset, defining the "population" from which the data are a sample and discussing how representative the data are of that population is critical to any conclusions drawn from the model.
[Design-based inference](#design-based) fully embraces this mindset that the data are sampled from a larger population.


<!-- Part 2: Interpretation of probability -->
More philosophical is the modeler's attitude to causality, prior probabilities and the likelihood principle.
And, more general, how  probability is to be interpreted.

> "It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel."

-- Leonard Savage, 1972 [^savage1972] 

Statistical modeling is the foundation for learning from data.
But we need another mindset on top of that to make the models useful.

[Frequentist inference](#frequentism) is the most prominent mindset for inferring properties about the world from statistical models.
Frequentist statistics sees probability as the relative frequencies of events in long-run experiments.

[Bayesian inference](#bayesian) is based on an interpretation of probability as a degree belief about the world.
Bayesianism states that the model parameters also have a (prior) distribution.
And the goal of the statistician is to update the prior distribution by learning from data.
The resulting posteriori distribution of the parameter expresses our belief about the the parameter.

[Likelihoodism](#likelihoodism) is a lesser known modeling mindset. Like Bayesianism, it adheres to the likelihood principle, which states that the likelihood function captures all evidence from the data (which frequentist inference violates).
However, it does not require prior probabilities.

[Causal inference](#causality) adds causality to statistical modeling. It can be superimposed onto any of the other three mindsets.

A different but complimentary approach is [design-based inference](#design), which focuses on data sampling and experiments instead of models.

## Strengths

* The statistical modeling mindset is a *language to see the world*. Even when not used for inference, random variables and probability distributions are useful mental models for perceiving the world. 
* Statistical modeling has a long tradition and extensive theoretical foundation, from measurement theory as the basis of probability theory to convergence properties of statistical estimators.
* The data-generating process is an underestimated mental model. But it's a powerful mental model that encourages mindful modeling and asking the right questions.
* Conditional probability models can be used not only to learn about the parameters of interest, but also to make predictions 
* Probability distributions give us a language to express uncertainty. [Bayesianism](#bayesianism) arguably has the most principled focus on formalizing and modeling uncertainty.

## Limitations

* Statistical modeling quickly reaches its limits when defining probability distributions becomes difficult. Images and text don't easily fit into this mindset, and this where [supervised machine learning](#supervised-ml) and especially [deep learning](#deep-learning) shine.
* Working with the statistical modeling mindset can be quite "manual" and tedious. It's not easy to always think about the DGP, and sometimes more automatable mindsets such as supervised machine learning are more convenient.
* Statistical models require a lot of assumptions. Sometimes more, sometimes less. Just to name a few common assumptions: homoscedasticity, independent and identically distributed data (IID), linearity, independence of errors, lack of (perfect) multicollinearity, ... For most violations, there is a special version of a statistical model without the critical assumption whose estimation and interpretation is often more complicated.
* Statistical modeling, when used for prediction, is often outperformed by [supervised machine learning](#supervised-ml). To be fair, outperforming here requires an evaluation based on the supervised learning mindset. However, this means that a goodness-of-fit and diagnostics are no guarantee that a model will performs well on all metrics. <!-- requires citation. kaggle? -->
<!--
## Justification of Likelihood Mindset: Probability Theory

Let's start VERY basic.
What is a probability?
It's axiomatic, meaning we have to make some assumptions to build a framework of interpretability.
Introduced by Andrey Kolmogorov in 1933.
There's an entire field called measurement theory, which lays the theoretical groundwork that we can even speak of "probabilities" and events.
The idea is that we can describe a so-called probability space with $\Omega$, the set of all possible events that can happen, we call $B$ sub-spaces of $\Omega$ and $p$ is a measure on the event space.
The clue is how we define $p$.
And this is where axioms kick in: We have to build on axioms because we can't say what's right or wrong.
Kolmogorov's axioms:
For any event $b \in B$ we want $p(b) \in [0,1]$, and also $p(\Omega) = 1$.
This looks a lot like a probability: The probability that any of the events happens is 1 ($\Omega$ is an exhaustive set) and the probability is always between 0 and 1.
Also for disjunct events, the probability that any of them happens is the sum of their probability.
The probality that a person a person rolls a dice with "1" or a "2" is the sum of probabilities for both numbers. 
-->


<!-- representativeness -->
<!--
So to make claims about the world, we also have to make claims about what our sample represents.
Insights from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856970/
* Assumptions that the data comes from infinite population
* Targets are the coefficients: They characterize this infinite population 
* Fisher is about model-based inference
* TODO: Think about moving model-based inference to statistical learning?
-->

[^savage1972]: Savage, Leonard J. The foundations of statistics. Courier Corporation, 1972. 

[^Wynants2020]: Wynants, Laure, Ben Van Calster, Gary S. Collins, Richard D. Riley, Georg Heinze, Ewoud Schuit, Marc MJ Bonten et al. "Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal." bmj 369 (2020).


[^semiparametric]: Some approaches don't assume a closed form distribution. For example the Cox proportional hazards model. For the Cox model, we optimize the partial likelihood. These approaches are called semiparametric.


