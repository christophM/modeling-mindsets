# Statistical Modeling {#statistical-modeling}


* Goal: Changing your mind under uncertainty.
* Assumes the world is best described by probability distributions.
* Requires additional assumptions for drawing conclusions. See [Frequentism Inference](#frequentist-inference), [Bayesian Inference](#bayesian-inference) and [Likelihoodism](#likelihoodism).
* [Machine learning](#machine-learning) is an alternative mindset.



<!-- statistician modeling mindset ingredients

- Ingredients: Random variables, distributions,
- Statistical estimators
- https://en.wikipedia.org/wiki/Statistical_model
- TODO: Distinguish between statistical model and statistical inference?
- TODO: IID assumption: talk about it
- TODO: Mention that most models make simplifying assumptions, such as typical for the linear model
- TODO: Write a paragraph about evaluation
-->

<!-- TODO: Mention all the types of statistical models one can build with this mindset: clustering, classification, regression, outlier detection, ... -->

<!-- 
TODO: write about

- interpretation of parameters important
- but also expected values and such, but that's often what the parameters stand for.

The parameters should have a well-defined meaning.
This is an (intuitive) assumption that is violated for [semiparametric models](#semiparametric).


Choosing a distribution is not enough.
The probability distributions have parameters.
And those parameters can be adapted so that the distributions fit to the data.

- more elaborate way than just have some distribution and paramters: statistical models pull these things together and there are many ways to fit 
-->


<!-- motivation -->
Do you become more productive when you drink a lot of water?
Is there a fixed "law", like a mathematical function, that expresses productivity as a function of water intake?
No.
No, because productivity depends on many factors: sleep duration and quality, distractions, noise pollution, ...
Because of all these factors and other contingencies, we won't get a perfect equation relating water intake to productivity.
Uncertainty remains.
Even the water intake varies from day to day and from person to person.

<!-- summary of likelihood mindset -->
Statistics is all about changing your mind under uncertainty.
One way to deal with the uncertainty of the world is to abstract aspects of the world as random variables and ascribe a probability distribution to them.
Daily water intake could be a random variable.
For productivity we would need some clever idea on how to best measure this abstract concept.
A not so clever example: Daily time spent in front of the computer.
To further relate these variables to each other, we can make assumptions on how the data were generated and connect the random variables in a statistical model.

Welcome to the **statistical modeling** mindset.

<!-- What is statistical model -->
A statistical model consists of a set of probability distributions that are fitted to data.
A probability distribution describes the frequency with which we expect to see certain values of random variables.
The second ingredient to a statistical model is the data, from which we estimate those probability distributions.
But let's start with the most elementary unit: the random variable.

<!-- 
[Semiparametric models](#semiparametric) and especially [distribution-free statistical modeling](#distribution-free) approach modelling with a different mindset, but are considered statistical modeling.
-->

<!-- 
Assumptions:

- Data are iid:
- independently drawn
- identically distributed
- no confounding effects
- linearity (commonly)
- 
-->




## Random Variables

<!-- Random Variables --> 
A random variable is a mathematical object that carries uncertainty.
Daily water intake can be a random variable.
Data are seen as **observations** or realizations of random variables.
If someone drank 2.5 liters of water, that is a realization of the random variable "daily water intake".

Other random variables:

* Outcome of a dice roll.
* Monthly sales of an ice cream truck. 
* Whether or not a customer has canceled their contract last month.
* Daily amount of rain in Europe.
* Pain level of patients arriving at the emergency room.


<!-- TOOD: Visualize World+random variable -> Data/Observations -> Model/estimated probability distributions -->

People with a statistical modeling mindset **think** in random variables.
Any problem related to data is translated into a set of random variables and solved based on that representation.
A random variable is a construct that we can't observe directly.
But we can observe the realizations of a random variable, as shown in Figure \@ref(fig:variable).
But the raw data are not that useful to humans.
Because humans aren't databases, we need a model that simplifies the noisy data for us.
Statisticians came up with probability distributions: a mathematical construct that describes potential outcomes of the random variable.

<!--
There are usually many choice how we translate a world property into a random variable : 
Should water intake be measured per week, day or even hour?
Do we respect the water contained in consumed food?
What about sweating?

Sometimes what later becomes a random variable is already dictated by the data we collected:
When the water intake was measured daily, then that's also the random variable.
-->
```{r variable, fig.cap = "Each dot represents a data point, a realizations of a random variable. The x-axis shows the value of the variable. Dots are stacked up for frequently occurring values. ", fig.height = 3, fig.width = 9}
library(ggplot2)
set.seed(1)
n = 25
xsample = rnorm(n, mean = 1, sd = 2)
xx =  density(xsample)
xx = data.frame(x = xx$x, y = xx$y, type = "real")
#xx = rbind(xx, dat)

xdat = data.frame(x = xsample, type = "egal", y = 0)

ggplot(xdat) +
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05) +
 theme_void() +
  theme(axis.text.x = element_text()) +
  scale_y_continuous("", limits = c(0, 3)) +
  coord_fixed()

```

## Probability Distributions

<!-- Definition of a distribution -->
A probability distribution is a function which gives each possible outcome of a random variable a probability.
Value in, probability out.
Not all functions can be used as probability functions.
A probability function must be larger or equal to zero for the entire range of the random variable.
For discrete variables such as the number of fish caught, the probability distribution must sum up to 1 over all possible outcomes.
And for continuous outcomes such as water intake, the probability distribution, also called density function, must integrate to 1 over the possible range of values.

<!-- examples of probability distributions -->
For the outcome $x$ of a fair dice, we can write the following probability function:

\[
P(x) = 
\begin{cases}
 1/6 & x \in \{1, \ldots, 6\} \\
 0 & x \notin \{1, \ldots, 6\} \\
\end{cases}
\]

The Normal distribution is for continuous variables and defined from minus to plus infinity:

$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

<!-- what each part means -->
In this equation, $\pi$ and $e$ are the famous constants $pi \approx 3.14$ and $e \approx 2.7$.
The distribution has two parameters: mean $\mu$ and standard deviation $\sigma$.
<!-- the parameters -->
These parameters are sometimes called location ($\mu$) and scale ($\sigma$) parameters, since they determine where on the x-axis the center of the Normal distribution is and how flat or sharp the distribution is. 
The larger the standard deviation $\sigma$, the flatter the distribution.

<!-- how it works -->
Let's try it out and set $\mu = 0$ and $\sigma = 1$, as visualized in Figure \@ref(fig:distributions), leftmost curve.
Now we can use this probability distribution function for telling us how probable certain values of our random variable are.
We get $f(1) \approx 0.24$ and for $f(2) \approx 0.05$, making $x=1$ much more likely then $x=2$.
We may not interpret $f(x)$ as probability directly.
But we can integrate $f$ over a region of the random variable to get a probability.
The probability for $x \in [0.9, 1.1]$ is 4.8\%.

<!-- supermarket of distributions -->
There is huge arsenal of probability distributions: The Normal distribution, the Binomial distribution, the Poisson distribution, ...
And there is an infinite number of probability distributions that you could invent yourself.

```{r distributions, fig.cap = "Distributions", fig.height = 3, fig.width = 10}
library(ggplot2)
library(patchwork)
n = 100
# Normal
x = seq(from = -4, to = 4, length.out = n)
y = dnorm(x) 
dat = data.frame(x = x, y = y, pd = "Normal")
p1 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        axis.text.y = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Normal")


x = seq(from = 0, to = 10, length.out = n)
y = dgamma(x, shape = 2, rate = 1) 
dat = data.frame(x = x, y = y, pd = "Gamma")
p2 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Gamma")



x = seq(from = 0, to = 8)
y = dpois(x, lambda = 1)
dat2 = data.frame(x = x, y = y, pd = "Poisson")
p3 = ggplot(dat2, aes(x = x, y = y)) +
  geom_col(fill = "white", color = "black", size = 0.8, width = 0.5) +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:max(x)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Poisson")



nrep = 5
x = seq(from = 0, to = nrep)
y = dbinom(x, p = 0.7, size = nrep)
dat3 = data.frame(x = x, y = y, pd = "Binomial")
p4 = ggplot(dat3, aes(x = x, y = y)) +
  geom_col(data = dat3, fill = "white", color = "black", size = 0.8, width = 0.5) +
  theme_void() +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:nrep) +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Binomial")

p1 | p3 | p2 | p4

```

## Assuming a Distribution

<!-- back to data -->
An important step in the statistical modeling mindset is to connect the random variables that we are interested in to distributions.
A common approach is to choose a distribution that matches the "nature" of your data:

* A numerical outcome such as IQ? That's Normal distribution.
* A count outcome such as number of fish caught in an hour? The Poisson distribution is a solid choice.
* A binary repeated outcome, like the number of successful free throws in basketball? It follows a Binomial distribution.

<!-- assumption vs. estimation -->


<!-- multidimensional distributions -->
These were examples 1-dimensional distributions that only consider a single variable.
The world is more complex than that:
Random variables can be correlated.
The distribution of one variable can depend on the value that another random variable takes.
Fortunately, it's possible to define so-called multivariate distributions.
A multivariate distribution is a function that takes as input two ore more values and returns a density (still a single number).
We might assume that the joint distribution of water intake and productivity (measured in minutes) follows a 2-dimensional Normal distribution.

<!-- conditional distribution -->
Another option is conditional distributions.
For example, we could assume that productivity, conditional on water intake, follows a Normal distribution.

<!-- dream land -->
With all these probability distributions, we are still in the realm of abstraction and assumptions.
On one side we have the data: messy and untamed.
On the other side, we have the probability distributions: clean and idealized.
Via random variables we have at least a theory how the two are connected:
The data are the realizations of variables, and distributions summarize the stochastic behaviour of variables.
But we still need to mathematically connect observed data and theoretical distributions.
How can we connect them?

The answer is **statistical models**.

## Statistical Model

<!-- definition of a statistical model -->
A statistical model connects theoretical distributions with observed data.
Statistical models are mathematical models that make assumptions about how the data are generated.
With these assumptions in the background, statistical models are then estimated using data.
More formally, a statistical model is the combination of the sample space from which the data comes from, and a set of probability distributions on this sample space.

<!-- TODO: Visualize statistical model as connection between data and distributions -->

<!-- Parameters -->
<!-- 
Other distributions have other parameters. [^semiparametric]
-->

<!-- TODO: regression models, non- and semi-parametric -->

<!-- Estimation intuition -->
The distributions are "fitted" to the data by changing the parameters.
Imagine the distribution as a cat.
And your data is a box.
Your cat fits it's shape and position to match the box.

<!-- TODO: visualize cat in box -->

<!-- where params come from -->
How does the cat know which form to take on?
Ah pardon, our question now is:
How do we find parameter values so that the distribution fits the data well?
The density function of the Normal distribution, for example, has mean and variance as parameters.
Given the parameters, the density or probability function can tell us how likely certain values of our random variables are.

<!-- from density to likelihood -->
We can also use the probability or density function to find our parameters -- by reversing the point of view and calling it the likelihood function.
The value of the random variable is the input of the probability function, and the parameters are seen as given.
The likelihood function $L(\theta,  X)$ is equal to the probability function.
Except that the parameters are now the input, and the values of the random variable are seen as given.
They are given, in the sense that we have data that are realizations of the random variable.

<!-- how to compute -->
We can take an observed value for $X$ from our data and plug it into the likelihood function.
Now we have a likelihood function that can tell us, for this one data point, which parameters would give the highest probability for observed this particular value.
But our data consist of multiple realizations of random variables.
To get the likelihood for the entire dataset, we multiply the likelihoods of the individual data points.
This data likelihood can tell us, for a given parameter value, how likely our data is.
For example we could try  $\mu = 1$ and $\sigma = 2$ and the likelihood function returns a value.
The larger the value, the better the distribution (with these parameters) fits the data.
That's very useful, because it helps us in finding the best parameters.


<!-- maximum likelihood -->
Finding the best parameters is a classical optimization problem:
Maximize the data likelihood $L$ with respect to the parameters.
We want to maximize the likelihood for all of our data: $L(\theta, \mathbf{x}_1, \ldots, \mathbf{x}_n) = \prod_{i=1}^n L_i(\theta, \mathbf{x}_i)$. <!-- vim_ -->
Note that $\mathbf{x}_i$ <!-- x_ --> can be a vector with values for multiple variables.
And we maximize the data likelihood:

$$\arg \max_{\theta} L(\theta | \mathbf{x}_1, \ldots, \mathbf{x}_n)$$ <!-- vim_ -->

### Maximum Likelihood Estimation {-}

<!-- Estimation technically -->
Maximum likelihood estimation is a classic optimization problem.
For simpler cases this can even be solved analytically.
For the Normal distribution, we know that the optimal $mu$ is the mean of the data: $\frac{1}{n} \sum_{i=1}^n x_i$.
When an analytical solution is not possible, other optimization methods like gradient descent, the Newton-Raphson method and Fisher's scoring can be used.

<!-- maximum likelihood mental model -->
Maximum likelihood estimation is a key element to understanding the statistical modeling mindset.
Maximizing the likelihood means bringing together the theoretical probability distributions and the observed data.

```{r fit, fig.cap = "Fitting distributions to data", fig.height = 2, fig.width = 10}
# different means
x = seq(from = -15, to = 15, length.out = 1000)
means = c(-5, 1, 8)
d1 = data.frame(x = x, y = dnorm(x, mean = means[1], sd = 2), type = "1")
d2 = data.frame(x = x, y = dnorm(x, mean = means[2], sd = 2), type = "2")
d3 = data.frame(x = x, y = dnorm(x, mean = means[3], sd = 2), type = "3")
dd = data.frame(data.table::rbindlist(list(d1, d2, d3)))
# Make smaller to match dotplot
dd$y = dd$y 
# position of the question marks
ypos = 0.7 * max(dd$y)
ggplot(dd, aes(x=x,y=y, group = type)) +
  geom_area(alpha = 0, position = "identity") + 
  scale_fill_discrete(guide = "none") +
  geom_line(size = 1.5, color = "black") +
  annotate("text", label = "?", x = means[1], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[2], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[3], y = ypos, size = 12) + 
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05, data = xdat) +
  scale_y_continuous(limits = c(0, max(dd$y))) +
  theme_void()

```

### Interpreting Parameters {-}

<!-- role of the parameters -->
The parameters are not only things to be optimized.
Their role goes beyond that of a mere technical controller.
The parameters are central to the statistical modeling mindset.

**Statisticians interpret the parameters as summary of the data.**

Statistical modeling is about understanding our data better.
The nice consequence of modeling the data with probability distributions is that we summarize our data with just a few numbers, the parameters.
In the case of the Normal distribution, we can describe the distribution of a single variable with only two numbers, the mean and the standard deviation.
Also for other types of statistical models, parameters are central to interpretation.



<!-- example for likelihood mindset -->
<!--
Let's go back to the water and productivity example.
You want to answer this question of water versus productivity  with data.

First step: You collect lots of data, measure your daily water intake, and the number of tasks you get done.
Fully embracing the statistical modeling mindset, we express information as random variables.
One variable is "water intake" $X_{water}$ and the other is "Number of productive hours" $X_{hours}$.
Next, we assume that water intake + words follow a 2-dimensional Normal distribution.
The multivariate Normal distribution has 5 parameters.
Mean and variance for water intake, and the same for productivity.
The fifth parameter is the covariance between water and productivity.

Our statistical model is now ready for parameter estimation.
We can use maximum likelihood to solve find the optimal parameter.
Optimal means: the values for the five parameters that maximize the likelihood function.
We write down the formula for the probability distribution of our data.
And then we maximize it for the parameters, given the data.
The resulting parameters are then our estimates for the parameters.
This now parameterized distribution is the statistical model of our little office world.
-->
<!-- end of example -->

<!-- what types of models are possible -->


## Joint or Conditional Distribution 

<!-- 
- 1D distributions
- but also mulitvariate
- with more than one variable, we have two options
- full joint distribution
- conditional distribution
- conditional distribution ->  build regression models
- also clustering is possible
- ANOVA, regression models, survival model / cox model, Justification
-->

<!-- TODO: Visualize both joint and conditional distribution. Gaussian, 2 variables, one plot with margins. Also marginal distribution? -->
<!-- joint distribution -->
All statistical models target an aspect of a probability distribution.
For most of the chapter, we talked about simpler cases, like the distribution of a single variable: $P(X)$.
But we could have the case of multiple random variables $X_1, \ldots, X_p$.
To describe the full distribution of more than one variable, we have to work with the joint distribution.

<!-- Gaussian mixture, joint example -->
A Gaussian mixture model, for example, requires learning the entire distribution of multiple variables. 
Gaussian mixture models can be used for identifying clusters in the data, which are assumed to stem from a mixture of Normal distributions.
Gaussian mixture models are also an example of using a different optimization algorithm than the maximum likelihood algorithm: the expectation-maximization algorithm, which iteratively jumps between optimizing the model parameters and predicting the "mixture" of cluster centers for each data point.

<!-- joint versus conditional  -->

<!-- motivation: regression models -->
The joint distribution is not always of interest and can be difficult to estimate.
It's often much simpler to work with *conditional distributions*.
The conditional distribution tells us how likely the outcome of one or more random variables is given that we already know the values of some other random variables.
For example:

<!-- example of conditional outcomes -->
* Which risk factors influence the probability of getting lung cancer?
* How do climatic conditions like temperature and humidity affect the occurrence of bonobos?
* On what days is a hospital likely to be understaffed?  

<!-- advantages of conditional -->
The conditional distribution is the natural form for prediction tasks.
And it's usually simpler to estimate than the joint distribution.
Models of the conditional distribution are central to statistical modeling.
They are also known as regression models. 

## Regression Models {#regression-models}

<!-- regression models -->
Regression models are statistical models of the conditional and not the joint distribution.
Let's say we have two variables: $Y$ and $X$.
With the joint distribution $P(Y,X)$ we could answer "How likely is it that $X=x$ and $Y=y$ occur together?
But with the conditional distribution we can ask: "Given $X=x$, how likely are certain values for $Y$"?

<!-- an example -->
For example, we want to know not only how often a disease is successfully treated.
We might want to know if a certain drug played a part in the disease outcome.
Other factors such as the patient's age, disease progression, etc. may play a role.

<!-- how it works -->
Our target is the distribution of outcome variable $Y$ conditional on variables $X_1, \ldots, X_p$.
Within the regression model, the distribution of $Y$ is linked to the variables $X_1, \ldots, X_p$.
Often this means that we express the parameters $\theta$ of $Y$'s distribution (such as the mean $\mu$ in a Normal distribution) as a function of the other variables.
How exactly this link looks like depends on the distribution that the modeler assumed for $Y$ and the link they chose to connect $\theta$ and $X$.
The simplest case is a linear regression model.
We assume that $Y$ follows a Normal distribution and link the mean of $Y$'s distribution to a weighted sum of the other variables:

$$Y \sim N(\mu, \sigma)$$

$$\mu = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p$$

<!-- Regression model = conditional distribution -->
The linear regression model expresses the mean of the target $Y$ as the weighted sum of the other variables. 
$Y$ given $\mathbf{X}$ follows a Normal distribution.
We only link the mean $\mu$ to the variables.
A typical assumption is that the standard deviation $\sigma$ is independent of the value of the other variables.

<!-- what can we do with model -->
What can we do now with a regression model?
<!-- goal: prediction -->
We can make predictions.
We just have to fill in values for $X$ and get the expected value of the probability distribution of $Y$.
<!-- target other parts of the conditional distribution -->
While the conditional mean of $Y$ is often the goal of the regression model, the statistician can also target other parts of the conditional distribution.
For example the median or other quantiles.
These types of regression models are quantile regression models.
For survival models the target is often the hazard rate, which is a function of the probability of an event happening in a time interval.
See for example the Cox proportional odds model.

<!-- goal: interpretation -->
**Interpretation of parameters** is usually even more important than prediction.
The parameters contain information about the relationship between the target and the other variables.
For regression models, we usually speak of the coefficients.
In the linear regression model, a positive coefficient $\beta_j$ means that increasing the value of variable $X_j$ increases the expected value of $Y$. 

A large part of statistical modeling consists of regression models in different flavors.

* Generalized linear models (GLMs)
* Generalized additive models (GAMs)
* Quantile regression
* Mixed effect models
* ...

Most of these models were "invented" to address a shortcoming of another regression model.
GLMs can model targets that don't follow a Normal distribution but some other distribution.
GAMs can model non-linear relationships.

<!-- bridge to ML -->
Regression models also have a place in [supervised machine learning](#supervised-ml).
But in supervised  machine learning, the goal is to achieve good predictive performance on unseen data.
And if a random forest performs better than a GAM, then the GAM gets thrown out, even if it has nicer statistical properties.

<!-- on classification models 
But what if the outcome is a category, and our task is classification?
When you come from the [supervised machine learning](#supervised-ml) mindset, always the distinction between classification and regression models is made.
In statistical modeling, we don't model the categories, but their probability distribution.
From the statistical modeling mindset, a categorical variable is nothing special.
Like for all other variables, we model the conditional distribution of the variable.
And if we make predictions with our model, we get the probabilities for the categories.
-->


## Model Evaluation

<!-- importance of evaluation for understanding mindset -->
Good statisticians evaluate their models.
Understanding how this evaluation works will help you better understand the statistical modeling mindset.
It's particularly interesting to see how much the evaluation in the statistical modeling mindset differs from that of [supervised machine learning](#supervised-ml).
The evaluation of statistical models consists of two parts: **model diagnostics and goodness-of-fit**.

<!--  diagnostics -->
The role of model diagnostics is to check whether the modeling assumptions were reasonable.
If we have assumed that a random variable follows a Normal distribution, we can check this assumption visually.
For example, with a Q-Q plot.
<!-- TODO: visualize a few qq-plots -->
Another typical assumption is homoscedasticity: The variance of the target is independent of other variables.
Homoscedasticity can be checked visually by plotting the residuals ($y_i$ minus its predicted value) against each of the other variables.

<!-- goodness-of-fit -->
A model that passes these diagnostics is not automatically a good model.
Statisticians use goodness-of-fit measures to compare models and evaluate modeling choices, such as which variables to have in the model.  
Typical goodness-of-fit measures are the (adjusted) R-squared, Akaikes Information Criterion (AIC), the Bayes factor, and likelihood ratios.
Goodness-of-fit is literally a measure of how well the model fits the data.
The goodness-of-fit can guide the model building process and decide which model is chosen in the end.

<!-- compare evaluation to ML -->
Goodness-of-fit is typically computed with the same data that were used for fitting the statistical models.
This choice may look like a minor detail, but it says a lot about the statistical modeling mindset.
The critical factor here is overfitting: The more flexible a model is, the better it adapts to ALL the randomness in the data instead of learning patterns that generalize.
Many goodness-of-fit metrics therefore account for model complexity, like the AIC or adjusted R-squared.
Compare this to [supervised machine learning](#supervised-ml): in this mindset, there is a strict requirement to always use "fresh" data for evaluation.

## Data-Generating Process (DGP)

<!-- what is the DGP -->
A quite central, but fuzzy also topic of the statistical modeling mindset is the data-generating process.
The statistical modeler thinks about the data-generating process all the time.
The data-generating process is a construct, an unknowable ideal of how the data were generated.
The data-generating process produces the unknown distributions that then produce the data.
We observe the data-generating process indirectly through the data.
With data and reflections on the DGP, sometimes together with experts, the statistician tries to decipher the DGP.
Statisticians talk about the DGP all the time, but it remains more of a mental model than a  clearly defined concept.
When I was studying statistics, the DGP was mentioned in most lectures.
But I never had a formal lecture on what the DGP is.
It's also difficult to find textbooks, lectures, or blog posts that define what the DGP really is.
It seems to me that the mental model of a DGP is a natural consequence of viewing the world in terms of probability distributions.

<!-- DGP detective -->
The DGP is a powerful idea, even if it's not well defined.
Assuming a DGP encourages you to intellectually dive deep into your data.
Having this image of a DGP in your mind let's you take on the mindset of a detective:
Statisticians are like detectives reconstructing a crime.
You can't observe the crime directly.
But the crime has generated a lot of "data" at the scene.
The stats detective then tries to uncover the data-generating process by making assumptions and learning from the observed data.

<!-- examples of data-generating processes -->
There is no definition of the data-generating process, so I'll give you a some examples:

* Rolling dice is a data-generating process. A die is symmetric, making each side equally likely. We could factor in the angle of the throw, surface roughness and so on. But the chaotic behaviour of the dice bouncing and spinning across the table makes us disregard all these factors.
* We study the income of computer scientists via a survey. Instead of mindlessly reporting the income distribution, we think about the entire data-generating process: For example, some income values are missing. Are they missing at random? Are higher-income individuals more likely to leave the income answer empty? Are some businesses overrepresented in the survey? Is the sample truly random?
* A research team has collected chest x-ray images of patients with and without COVID-19 for building a COVID-19 prediction model. A closer look at the data-generating process shows: the images not only differ in COVID-19 status, but they come from different data-generating processes. COVID-19 images are often taken with patients lying down because of exhaustion. One of the non-COVID-19 datasets are even children x-ray images.[^Wynants2020] Considering the DGP casts doubt on whether such data can be used to build a COVID-19 classifier or whether, in reality, it classifies data into child/adult, standing/lying down, ...  I chose this example because it's a paper from the [supervised machine learning mindset](#supervised-ml). A good statisticians would check the DGP much more carefully, making it more likely to detect such problems.

If these examples of data-generating processes sound like common sense to you, it's because they are.
But it's surprisingly uncommon among non-statistician mindsets.
For example, for [supervised machine learning](#supervised-ml), considerations of the data-generating process play a subordinate .
For most machine learning competitions the winner is determined solely by the lowest prediction error on new data.
It doesn't matter whether the model meaningfully reflects the data-generating process.

## Drawing Conclusions About the World

<!-- from world to model -->
<!--
Statisticians collect data about the world and use that data to fit a statistical model.
The statistical model links the world and the data through random variables:
The world can be simplified by probability distributions;
the data are viewed as realizations of the random variables.
-->

<!-- from model to world -->
In most cases, statistical models are built for practical reasons:
To make a decision, to better understand some property of the world, or to make a prediction.
These goals require the interpretation of the model instead of the world.
But using the model as a representation of the world isn't for free.
The statistician must consider the representativeness of the data and choose a mindset that allows the findings of the model to be applied to the world.

<!-- Part 1: Representativeness -->
Considering the data-generating process also means thinking about the representativeness of the data, and thus the model.
Are the data a good sample and representative of the quantity of the world the statistician is interested in?
Let's say a statistical modeler analyzes data on whether a sepsis screening tool successfully reduced the incidence of sepsis in a hospital.
They conclude that the sepsis screening tool has helped reduce sepsis-related deaths at that hospital.
Are the data representative of all hospitals in the region, the country, or even the world?
Are the data even representative of all patients at the hospital, or are data only available from patients in intensive care unit?
A good statistical modeler define and discuss the "population" from which the data are a sample of.
[Design-based inference](#design-based-inference) fully embraces this mindset that the data are sampled from a larger population.


<!-- Part 2: Interpretation of probability -->
More philosophical is the modeler's attitude toward causality, the nature of probability, and the likelihood principle.

> "It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel."

-- Leonard Savage, 1972[^savage1972] 

Statistical modeling is the foundation for learning from data.
But we need another mindset on top of that to make the models useful.

[Frequentist inference](#frequentism-frequentism) is the most prominent mindset for inferring properties about the world from statistical models.
Frequentist statistics sees probability as relative frequencies of events in long-run experiments.

[Bayesian inference](#bayesian-inference) is based on an interpretation of probability as a degree belief about the world.
Bayesianism states that the model parameters also have a (prior) distribution.
And the goal of the statistician is to update the prior distribution by learning from data.
The resulting posteriori distributions of the parameters express our belief about the world.

[Likelihoodism](#likelihoodism) is a lesser known modeling mindset. Like Bayesianism, it adheres to the likelihood principle, which states that the likelihood function captures all evidence from the data (which frequentist inference violates).
However, it does not require prior probabilities.

[Causal inference](#causal-inference) adds causality to statistical modeling. It can be superimposed onto any of the other three mindsets.

A different but complimentary approach is [design-based inference](#design-based-infernce), which focuses on data sampling and experiments instead of models.

## Strengths

* The statistical modeling mindset is a *language to see the world*. Even when not used for inference, random variables and probability distributions are useful mental models for perceiving the world. 
* Statistical modeling has a long tradition and extensive theoretical foundation, from measurement theory as the basis of probability theory to convergence properties of statistical estimators.
* The data-generating process is an underestimated mental model. But it's a powerful mental model that encourages mindful modeling and asking the right questions.
* Conditional probability models can be used not only to learn about the parameters of interest, but also to make predictions 
* Probability distributions give us a language to express uncertainty. [Bayesianism](#bayesian-inference) arguably has the most principled focus on formalizing and modeling uncertainty.
* Can naturally handle different types of variables: Categorical, ordinal, numerical, ...

## Limitations

* Statistical modeling reaches its limits whenever defining probability distributions becomes difficult. Images and text don't easily fit into this mindset, and this where [supervised machine learning](#supervised-ml) and especially [deep learning](#deep-learning) shine.
* Working within the statistical modeling mindset can be quite "manual" and tedious. It's not easy to always think about the DGP, and sometimes more automatable mindsets such as supervised machine learning are more convenient.
* Statistical models require a lot of assumptions. Sometimes more, sometimes less. Just to name a few common assumptions: homoscedasticity, independent and identically distributed data (IID), linearity, independence of errors, lack of (perfect) multicollinearity, ... For most violations, there is a special version of a statistical model that doesn't require the critical assumption. The price is more complex and less interpretable models.
* Statistical modeling, when used for prediction, is often outperformed by [supervised machine learning](#supervised-ml). To be fair, outperforming here requires an evaluation based on the supervised learning mindset. However, this means that goodness-of-fit and diagnosis are no guarantee that a model will performs well on all metrics. <!-- requires citation. kaggle? -->
<!-- representativeness -->
<!--
So to make claims about the world, we also have to make claims about what our sample represents.
Insights from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856970/
* Assumptions that the data comes from infinite population
* Targets are the coefficients: They characterize this infinite population 
* Fisher is about model-based inference
* TODO: Think about moving model-based inference to statistical learning?
-->

[^savage1972]: Savage, Leonard J. The foundations of statistics. Courier Corporation, 1972. 

[^Wynants2020]: Wynants, Laure, Ben Van Calster, Gary S. Collins, Richard D. Riley, Georg Heinze, Ewoud Schuit, Marc MJ Bonten et al. "Prediction models for diagnosis and prognosis of covid-19: systematic review and critical appraisal." bmj 369 (2020).

<!-- [^semiparametric]: Some approaches don't assume a closed form distribution. For example the Cox proportional hazards model. For the Cox model, we optimize the partial likelihood. These approaches are called semiparametric. -->


