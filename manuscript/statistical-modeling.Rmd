# Statistical Modeling {#statistical-modeling}

<!-- statistician modeling mindset ingredients

- Ingredients: Random variables, distributions,
- Statistical estimators
- https://en.wikipedia.org/wiki/Statistical_model
- TODO: Distinguish between statistical model and statistical inference?
- TODO: IID assumption: talk about it
- TODO: Mention that most models make simplifying assumptions, such as typical for the linear model
- TODO: Write a paragraph about evaluation
-->
<!-- TODO: Mention all the types of statistical models one can build with this mindset: clustering, classification, regression, outlier detection, ... -->

<!-- motivation -->
Do you become more productive when you drink a lot of water?
Is there a fixed "law", like a mathematical function, that expresses productivity as a function of water intake?
No.
No, because productivity depends on many factors: sleep duration and quality, distractions, noise pollution, ...
Because of all these factors and other contingencies, we won't get a perfect equation relating water intake to productivity.
Uncertainty remains.
Even the water intake varies from day to day and from person to person.

<!-- summary of likelihood mindset -->
One way to deal with the uncertainty of the world is to abstract aspects of the world as random variables and ascribe a probability distribution to them.
Daily water intake could be a random variable.
For productivity we would need some clever idea on how to best measure this abstract concept.
A not so clever example: Daily time spent in front of the computer.
To further relate these variables to each other, we can make assumptions on how the data were generated and connect the random variables in a statistical model.
Welcome to the statistical modeling mindset.

<!-- parent mindset -->
Statistical modeling is a parent to these other mindsets: [frequentism](#frequentism), [Bayesianism](#bayesianism), [likelihoodism](#likelihoodism) and [causal inference](#causality).
These mindsets provide different ideas for how to do inference -- drawing conclusions about the world from data.
The mindsets differ in their interpretation of probability, their stance on causality and their faithfulness to the likelihood principle.
Their use of statistical models unites these mindsets.


<!-- What is statistical model -->
A statistical model consists of a set of probability distributions.
A probability distribution describes the frequency with which we expect to see certain values of random variables.
The second ingredient to a statistical model is the data, from which we estimate those probability distributions.
Random variables are  capture some aspect of the world.
But let's start with the most elementary unit: the random variable.

<!-- 
[Semiparametric models](#semiparametric) and especially [distribution-free statistical modeling](#distribution-free) approach modelling with a different mindset, but are considered statistical modeling.
-->

<!-- 
Assumptions:

- Data are iid:
- independently drawn
- identically distributed
- no confounding effects
- linearity (commonly)
- 
-->




## Random Variables

<!-- Random Variables --> 
A random variable is a mathematical object that carries uncertainty.
Daily water intake can be a random variable.
Observed data are seen as **observations** or realizations of random variables.
If someone drank 2.5 liters of water, that is a realization of the random variable "daily water intake".

Other random variables:

* Outcome of a dice roll.
* Monthly sales of an ice cream truck. 
* Whether or not a customer has canceled their contract last month.
* Daily amount of rain in Europe.
* Pain level of patients arriving at the emergency room.


<!-- TOOD: Visualize World+random variable -> Data/Observations -> Model/estimated probability distributions -->

People with a statistical modeling mindset **think** in random variables.
Any problem related to data is translated into a set of random variables and solved based on that representation.
A random variable is a construct that we can't observe directly.
But we can observe the realizations of a random variable, as shown in Figure \@ref(fig:variable).
But the raw data are not that useful to humans.
Because humans aren't databases, we need a model that simplifies the noisy data for us.
Statisticians came up with probability distributions: a mathematical construct that describes potential outcomes of the random variable.

<!--
There are usually many choice how we translate a world property into a random variable : 
Should water intake be measured per week, day or even hour?
Do we respect the water contained in consumed food?
What about sweating?

Sometimes what later becomes a random variable is already dictated by the data we collected:
When the water intake was measured daily, then that's also the random variable.
-->
```{r variable, fig.cap = "Each dot represents a data point, a realizations of a random variable. The x-axis shows the value of the variable. Dots are stacked up for frequently occurring values. ", fig.height = 3, fig.width = 9}
library(ggplot2)
set.seed(1)
n = 25
xsample = rnorm(n, mean = 1, sd = 2)
xx =  density(xsample)
xx = data.frame(x = xx$x, y = xx$y, type = "real")
#xx = rbind(xx, dat)

xdat = data.frame(x = xsample, type = "egal", y = 0)

ggplot(xdat) +
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05) +
 theme_void() +
  theme(axis.text.x = element_text()) +
  scale_y_continuous("", limits = c(0, 3)) +
  coord_fixed()

```

## Probability Distributions

<!-- Definition of a distribution -->
A probability distribution is a function which gives each possible outcome of a random variable a probability.
Value in, probability out. [^density]
Not all functions can be used as probability functions.
A probability function must be larger or equal to zero for the entire range of the random variable.
For discrete variables such as the number of fish caught, the probability distribution must sum up to 1 over all possible outcomes.
And for continuous outcomes such as water intake, the probability distribution, also called density function, must integrate to 1 over the possible range of values.

<!-- examples of probability distributions -->
For the outcome $x$ of a fair dice, we can write the following probability function:

\[
P(x) = 
\begin{cases}
 1/6 & x \in \{1, \ldots, 6\} \\
 0 & x \notin \{1, \ldots, 6\} \\
\end{cases}
\]

The Normal distribution is for continuous variables and defined from minus to plus infinity:

$$f(x) = \frac{1}{\sigma \sqrt{2\pi}} e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}$$

<!-- what each part means -->
In this equation, $\pi$ and $e$ are the famous constants $pi \approx 3.14$ and $e \approx 2.7$.
The distribution has two parameters: mean $mu$ and standard deviation $\sigma$.

<!-- the parameters -->
These parameters called location ($\mu$) and scale ($\sigma$) parameters, since they determine where on the x-axis the center of the Normal distribution is and how flat or sharp the distribution is. 
The larger the standard deviation $\sigma$, the flatter the distribution.

<!-- how it works -->
Let's try it out and set $\mu = 0$ and $\sigma = 1$, as visualized in Figure \@ref(fig:distributions), leftmost curve.
Now we can use this probability distribution function for telling us how probable certain values of our random variable are.
We get $f(1) \approx 0.24$ and for $f(2) \approx 0.05$, making $x=1$ much more likely then $x=2$.
We may not interpret $f(x)$ as probability directly.
But we can integrate $f$ over a region of the random variable to get a probability.
The probability for $x \in [0.9, 1.1]$ is 4.8\%.

<!-- supermarket of distributions -->
There is huge arsenal of probability distributions: The Normal distribution, the Binomial distribution, the Poisson distribution, ...
And there is an infinite number of probability distributions that you could invent yourself.

```{r distributions, fig.cap = "Distributions", fig.height = 3, fig.width = 10}
library(ggplot2)
library(patchwork)
n = 100
# Normal
x = seq(from = -4, to = 4, length.out = n)
y = dnorm(x) 
dat = data.frame(x = x, y = y, pd = "Normal")
p1 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        axis.text.y = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Normal")


x = seq(from = 0, to = 10, length.out = n)
y = dgamma(x, shape = 2, rate = 1) 
dat = data.frame(x = x, y = y, pd = "Gamma")
p2 = ggplot(dat, aes(x = x, y = y)) +
  geom_line() +
  scale_y_continuous("", limits = c(0,0.4)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Gamma")



x = seq(from = 0, to = 8)
y = dpois(x, lambda = 1)
dat2 = data.frame(x = x, y = y, pd = "Poisson")
p3 = ggplot(dat2, aes(x = x, y = y)) +
  geom_col(fill = "white", color = "black", size = 0.8, width = 0.5) +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:max(x)) +
  theme_void() +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Poisson")



nrep = 5
x = seq(from = 0, to = nrep)
y = dbinom(x, p = 0.7, size = nrep)
dat3 = data.frame(x = x, y = y, pd = "Binomial")
p4 = ggplot(dat3, aes(x = x, y = y)) +
  geom_col(data = dat3, fill = "white", color = "black", size = 0.8, width = 0.5) +
  theme_void() +
  scale_y_continuous("", limits = c(0,0.4)) +
  scale_x_continuous("", breaks = 0:nrep) +
  theme(axis.text.x = element_text(),
        plot.title = element_text(hjust = 0.5)) +
  ggtitle("Binomial")

p1 | p3 | p2 | p4

```



## Assuming a Distribution

<!-- back to data -->
An important step in the statistical modeling mindset is to connect the random variables that we are interested in to distributions.
A common approach is to choose a distribution that matches the "nature" of your data:

* A numerical outcome such as IQ? That's Normal distribution.
* A count outcome, like number of fish caught per hour? The Poisson distribution is a solid choice.
* A binary repeated outcome, like the number of successful free throws in basketball? It follows a Binomial distribution.

<!-- assumption vs. estimation -->


<!-- multidimensional distributions -->
These were all examples of the distributions of single random variables.
The world is more complex than that:
Distributions of random variables can be intertwined.
The distribution of one variable can depend on the value that another random value takes on.
Fortunately, it's possible to define so-called multivariate distributions.
A multivariate distribution is a function that takes more than one value and returns a density (still a single number).
We might assume that the joint distribution of water intake and productivity (measured in minutes) follows a 2-dimensional Normal distribution.
<!-- conditional distribution -->
Or we could assume that the productivity, conditional on water intake, follows a Normal distribution.
We can be very creative in defining distributions and so on.

<!-- dream land -->
With these probability distributions, we are still in the realm of abstraction and assumptions.
On one side we have the data: messy and untamed.
On the other side, we have the probability distributions: clean and idealized.
Statistician ideologically connect the two via random variables:
The data are the realizations of the random variables.
And the distributions summarize the stochastic behaviour of random variables.
But we still need to mathematically connect data and theoretical distributions.
How do we bring data and distributions together?

The answer is **statistical models**.

## Statistical Model

CONTINUE HERE DEEPL

<!-- 
TODO: write about

- interpretation of parameters important
- but also expected values and such, but that's often what the parameters stand for.

The parameters should have a well-defined meaning.
This is an (intuitive) assumption that is violated for [semiparametric models](#semiparametric).


Choosing a distribution is not enough.
The probability distributions have parameters.
And those parameters can be adapted so that the distributions fit to the data.

- more elaborate way than just have some distribution and paramters: statistical models pull these things together and there are many ways to fit 
-->


<!-- definition of a statistical model -->
A statistical model connects theoretical distributions with observed data.
Statistical models are mathematical models that make assumptions about how the data are generated, and are estimated using data.
More formally, a statistical model is the combination of the sample space from which the data comes from, and a set of probability distributions on this sample space.

<!-- TODO: Visualize statistical model as connection between data and distributions -->

<!-- Parameters -->
<!-- 
Other distributions have other parameters. [^semiparametric]
-->

<!-- TODO: regressino models, non- and semi-parametric -->

<!-- Estimation intuition -->
The distributions are "fitted" to the data by changing the parameters.
Imagine the distribution as a cat.
And your data is a box.
Your cat fits it's shape and position to match the box.

<!-- Likelihood function -->
How do we know which parameters to set in our distribution?
Here we can work with the probability distribution.
As I said, the probability distribution is a function that gives the probability or density, given the value of a data point.
And it's parameterized, for example by the mean and variance for the Normal distribution.
We can use the same function to also find our parameters -- by switching the point of view.
The likelihood function $L(\theta | X)$ is the probability function, but seeing the parameters as changeable, and the data as fixed. 
For each data point, we can compute the likelihood function.
And we can multiply these likelihoods, which gives us the likelihood for all of our data.
This is a function where we now can fill in some parameters, for example $\mu = 1$ and $\sigma = 2$ and get a value in return.
The larger the value, the better the distribution fits the data.

<!-- maximum likelihood -->
We have ourselves an optimization problem at hand: Maximize the likelihood function $L$ with respect to the parameters, and given the data.
We want to maximize the likelihood for all of our data, so first we form the likelihood for our entire dataset $L(\mathbf{x}_1, \ldots, \mathbf{x}_n) = \prod_{i=1}^n f(\mathbf{x}_i)$. <!-- vim_ -->
Emphasizing that $\mathbf{x}_1$ <!-- x_ --> can be a vector multiple variables, in this case for data point 1.
And we maximize the data likelihood:

$$\arg \max_{\theta} L(\theta | \mathbf{x}_1, \ldots, \mathbf{x}_n)$$ <!-- vim_ -->

<!-- Estimation technically -->
Maximum likelihood estimation is a classic optimization problem.
For simpler cases this can even be solved analytically, for example for the Normal distribution, we know that the ideal $mu$ is the mean of the data: $\frac{1}{n} \sum_{i=1}^n x_i$.
For likelihood maximization where the analytical solution is not possible, other optimization methods exists.

<!-- maximum likelihood mental model -->
If you have understood the idea of maximum likelihood, you understand a key element the statistical modeling mindset.
Maximizing the likelihood means bringing together the theoretical probability distributions and the observed data.
We fit the distributions, aka our statistical model, to the data.

```{r fit, fig.cap = "Fitting distributions to data", fig.height = 2, fig.width = 10}
# different means
x = seq(from = -15, to = 15, length.out = 1000)
means = c(-5, 1, 8)
d1 = data.frame(x = x, y = dnorm(x, mean = means[1], sd = 2), type = "1")
d2 = data.frame(x = x, y = dnorm(x, mean = means[2], sd = 2), type = "2")
d3 = data.frame(x = x, y = dnorm(x, mean = means[3], sd = 2), type = "3")
dd = data.frame(data.table::rbindlist(list(d1, d2, d3)))
# Make smaller to match dotplot
dd$y = dd$y 
# position of the question marks
ypos = 0.7 * max(dd$y)
ggplot(dd, aes(x=x,y=y, group = type)) +
  geom_area(alpha = 0, position = "identity") + 
  scale_fill_discrete(guide = "none") +
  geom_line(size = 1.5, color = "black") +
  annotate("text", label = "?", x = means[1], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[2], y = ypos, size = 12) + 
  annotate("text", label = "?", x = means[3], y = ypos, size = 12) + 
  geom_dotplot(aes(x = x), binwidth = 0.5, stackdir = "up", method = "histodot", dotsize = 0.8, stackratio = 1.05, data = xdat) +
  scale_y_continuous(limits = c(0, max(dd$y))) +
  theme_void()

```

<!-- role of the parameters -->
The role of the distribution parameters goes well beyond a technical one.
The parameters are central to the statistical modeling mindset.
We can interpret the parameters as summarization of our data!
Statistical modeling is about understanding our data better.
The nice consequence of modeling our data with probability distributions is that we summarize our data with a few numbers, the parameters.
In the case of the Gaussian distribution, we can describe the distribution of a single variable with only two numbers, the mean and the standard deviation.
Also for other types of statistical models, parameters are central to interpretation.



<!-- example for likelihood mindset -->
<!--
Let's go back to the water and productivity example.
You want to answer this question of water versus productivity  with data.

First step: You collect lots of data, measure your daily water intake, and the number of tasks you get done.
Fully embracing the statistical modeling mindset, we express information as random variables.
One variable is "water intake" $X_{water}$ and the other is "Number of productive hours" $X_{hours}$.
Next, we assume that water intake + words follow a 2-dimensional Normal distribution.
The multivariate Normal distribution has 5 parameters.
Mean and variance for water intake, and the same for productivity.
The fifth parameter is the covariance between water and productivity.

Our statistical model is now ready for parameter estimation.
We can use maximum likelihood to solve find the optimal parameter.
Optimal means: the values for the five parameters that maximize the likelihood function.
We write down the formula for the probability distribution of our data.
And then we maximize it for the parameters, given the data.
The resulting parameters are then our estimates for the parameters.
This now parameterized distribution is the statistical model of our little office world.
-->
<!-- end of example -->

<!-- what types of models are possible -->


## Types of Statistical Models

CONTINUE HERE WRITING

All statistical models have some form of probability distributions at it's core.
For most of the chapter, we talked about simpler cases, like having the distribution of a single variable and fitting a distribution.
So the case was of learning $P(X)$.
But we could have the case of multiple random variables $X_1, \ldots, X_p$.

We have two options (bit oversimplifying here):
Fit the joint distribution, or a conditional distribution.
For example for Gaussian mixture models, we would learn the entire distribution.
Gaussian mixture models are a form of clustering model, where the data forms clusters.

* Difference between joint and conditional
* In other terms: conditional is supervised (at least if only for one variable)
* joint is a form of unsupervised learning.

<!-- 

- 1D distributions
- but also mulitvariate
- with more than one variable, we have two options
- full joint distribution
- conditional distribution
- conditional distribution ->  build regression models
- also clustering is possible
- ANOVA, regression models, survival model / cox model, Justification
-->



<!--
## Justification of Likelihood Mindset: Probability Theory

Let's start VERY basic.
What is a probability?
It's axiomatic, meaning we have to make some assumptions to build a framework of interpretability.
Introduced by Andrey Kolmogorov in 1933.
There's an entire field called measurement theory, which lays the theoretical groundwork that we can even speak of "probabilities" and events.
The idea is that we can describe a so-called probability space with $\Omega$, the set of all possible events that can happen, we call $B$ sub-spaces of $\Omega$ and $p$ is a measure on the event space.
The clue is how we define $p$.
And this is where axioms kick in: We have to build on axioms because we can't say what's right or wrong.
Kolmogorov's axioms:
For any event $b \in B$ we want $p(b) \in [0,1]$, and also $p(\Omega) = 1$.
This looks a lot like a probability: The probability that any of the events happens is 1 ($\Omega$ is an exhaustive set) and the probability is always between 0 and 1.
Also for disjunct events, the probability that any of them happens is the sum of their probability.
The probality that a person a person rolls a dice with "1" or a "2" is the sum of probabilities for both numbers. 
-->

## Regression Models

<!-- motivation: regression models -->
The distribution of our data would be fully described by the joint distribution.
But often there are too many variables.
And further, you might only be interested in one of the variables (like the outcome of an experiment).
And how other random variables influence this variable.

<!-- regression models -->
Regression  models take on a specific position in statistical modeling.
But they are severely reduced in their statistical ambition, but make it manageable to desribe indivudal variables distribution. 
Regression models are statistical models.
But instead of the joint distribution, they model the conditional distribution of a random variable.
This variable is often called $Y$.
But it's just a random variable, like the others.
Except that we have some special interest.
Then we have $\mathbf{X}$ the random variables that might influence $Y$, or where we are also interested in how they affect $Y$.
It's actually not true that regression models model the conditional distribution of $Y$.
Regression models usually just model an aspect of this distribution: the expected value of $Y$: $E(Y | X)$.

<!-- an example -->
For example, we want to know not only how often a disease is successfully treated.
We might want to know if a certain drug played a part in the disease outcome.
And other factors such as age of the patient, progression of the disease and so on might play a role as well.
And we would have to consider such factors to answer the question on whether the drug helps.

<!-- how it works -->
This model is of course parameterized.
We give parameters to the random variables.
And an equation that links the weighted random variables with the expectation of the target $Y$.
The simplest is a linear model:

$$Y = \beta_0 + \beta_1 X_1 + \ldots + \beta_p X_p + \epsilon, \epsilon \sim N(0, \sigma)$$

<!-- Regression model = conditional distribution -->
So we say that the target can be expressed as a weighted sum of the other random variables.
And there is some random error $\epsilon$ (also a random variable) that follows a Normal distribution with unknown variance $\sigma$.
And with this regression model, we specified the conditional distribution of our target $Y$: $Y|X \sim N(\beta_0 + \beta_1 X_1 + \ldots + \beta_p, X_p, \sigma)$.
In words: $Y$ given $\mathbf{X}$ follows a Normal distribution.
The mean or expected value of this conditional distribution is the weighted some of the other variables.
And the variance is inherited from the $\epsilon$.

<!-- goal: frequentist parameters -->
The goal in this regression modeling are the coefficients vector $\mathbf{\beta}$.
Parameters make the model interpretable.
Like the estimators of the mean or variance in the sections above, they are random variables.
So [frequentists](#frequentism) can derive confidence intervals, p-values and hypothesis tests for them ("Is coefficient $\beta_j$ significantly different from $0$?").
[Bayesians](#bayesian) would additionally assume a prior distribution for $\mathbf{\beta}$ and we would get as a result the posterior distributions of the coefficients to analyze. 

<!-- bridge to ML -->
They also build the bridge to [machine learning](#machine-learning), since they are framed in a similar way.
However in machine learning, the focus is not necessarily on building a probabilistic model.
But just to frame $Y$ in terms of a preditive target.
The regression models can also be used for making predictions.
But I would say that at least a focus is on the interpretation of the parameters and drawing conclusions from them about the world.

## Statistical Inference: Drawing Conclusions About the Real World

Assumption: The data come from larger population.

<!-- Part 1: Representativeness -->
TODO: Write about representative samples. or move that to the individual chapters frequentism, bayesianism

<!-- goal: learn about the world -->
Alright, we made our assumptions that the world can be described with a statistical model.
So we have this direction: The world informs the model. World -> Model
The statistical model is a simplified version of some properties of the world.
Ideally, we want to infer properties of the world now from the model.
We also know that our statistical model is subject to variance.
Would we have different data, we would have gotten a different model.
Hopefully only slightly different.

<!-- example of real world analysis -->
A typical analysis would be:
We want to know how some environmental policy affects, let's say water pollution.
A statistician sets up linear regression model that relates water pollution and policy used yes/no for different lakes.
Now we want these results to be translated again into real world properties.
We have a coefficient for the effect of the policy.
Let's say it is negative, indicating that the policy reduces pollution.
Just one problem: Is it negative by randomness, or is it a reliable effect.
There are now multiple approaches to deal with this problem.
Each got their own chapter, but I want to tease here how these mindsets look like.
The statistical modeling chapter you are currently reading is, as I mentioned, a meta-mindset, since it's not a complete mindset, but for the final step to infer properties, we need to extent this mindset.
It depends on our idea of what a probability is, how we perceive our data sample and what we think of causality.
They all have in common that they optimize likelihood functions and make most of the assumptions mention in this chapter.

<!-- statistical model of coin toss -->
For example, we toss a coin 10 times, and want to know if it's fair, meaning probability for head is 0.5.
A statistical model of this experiment can be a Binomial distribution.
Can we just compute this and report the $p$ the probability for heads from the Binomial distribution?
What if our best guess for $p = 0.6$?
Does it mean the coin is not fair? 


It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. [^savage1972] 

<!-- different mindsets -->
To answer these question, we have to make assumptions of what a probability is.

[Frequentist inference](#frequentism) is the most prominent mindset for inferrring properties about the world from statistical models.
Frequentist statistics sees probability as relative frequencies of events in long-run experiments.
It's a physical interpretation of probability.
Frequentism allows one to ask "What should I do?".
And statisticians get yes or no type of answers through hypothesis tests and p-values for the model paramters.

[Bayesian inference](#bayesian) has a different interpretation of probability as belief about the world.
Bayesianism says that also the parameters have a (prior) distribution.
And the goal of the statistician is to update the prior distribution with data.
The resulting posteriori distribution of the parameter expresses our belief of the parameter.
The question is: "What should I believe?"

[Likelihoodism](#likelihoodism) takes the likelihood-mindset quite literal by saying that all evidence for the data are in the likelihood function.
Unlike the other two, we don't learn about decisions or beliefs.
The likelihoodism mindset makes it impractical to use statistifal modeling in science, which makes this mindset less popular.
But nonetheless it's an insightful mindset to explore.

A different, yet complimentary, approach is [design-based inference](#design).
Design-based approaches focus on the data sampling and experiments, instead of models.


## Model Evaluation

* Evaluation important to understand the mindset
* Evaluation has following parts
  * Goodness-of-fit
  * Diagnostics
* Diagnostics are for checking whether basic assumptions are okay
* Diagnostics examples: QQPlot, Analyzing residuals. It's often visual.
* Diagnostics is for checking whether the assumptions made were ok.
* Assumptions are about distribution.
* Goodness-of-fit can compare models
* Models might encode different assumptions
* Maybe assumption of linearity violated?
* Try model with splines, see if (adjusted) goodness-of-fit better for thenew model.
* Part of "evaluation" is also just having good argumentation for why c
* For Bayrs there are more evaluations
* Evaluation here only for the likelihood, but which is also relevant to Bayes
* Goodness-of-fit evaluation usually with the same data
* AIC and such can account for flexibility of the model
* If not accounted for, then model with more flexiblity will have higher goodness-of-fit
* Vastly different appraoch than machine learning, where new data has to be used
* Note here again: it's archetype. Some might even use other data.
* 

## Data-Generating Process

A central theme to understand the mindset of statistical modeling is the data-generating process.
If you already developed a feeling for the idea of random variables and distributions, you are already there.
The data-generating process is a construct.
An idea of how the data that we observed was generated.
The thing with the data-generating process is: While it's just a model, you have to assume that the data-generating process really is how nature behaves, at least in an abstract way.
The thing is: The data-generating process is unknown.
It's almost an idealization of a process.
We can define it when we do simulations ourselves, but we cannot know it in nature.
The result of the data-generating process are distributions.
These we can also not observed, but at least we can observe the realizations of the respective random variables.
We can only observe its consequences by oberving data.

I think the data-generating process is a central element to the statistician mindset.
Proof: Also machine learning methods can rely on random variables and distributions.
But machine learners make the stance that the data-generating process is best approximated by some function we don't necessarily have to understand.
It's maybe not ever understandable.
The machine learners don't even speak about the data-generating process.

I think the data-generating process is a very powerful idea.
It enrourages you to think about how the data was generated.
When you explicitly have to model some hidden data-generating process, you take on the mindset of a detective uncovering the plot from clues.
It's a mental model that encourages to look at detailed questions:
How was a variable generated?
Is there some hidden variable that influence whether another variable has missing values?

<!-- examples of data-generating processes -->
"Defining" the data-generating process is neither simple nor well-defined, but I'll give it a try:

* For a dice roll, the data-generating process is the roll itself, and the fact that the dice has a symmetric shape, and enough randomness that we dismiss factors such as throwing angle, surface roughness and so on.
* We study the income of computer scientists via survey. Instead of only reporting on the income distribution, we think about the entire data-generating process: For example, some income values are missing. Are the missing at random? Or are maybe people with higher incomes omitting the question? Are their imbalances regarding the companies, namely that one companies worker are much more often in the survey? 
* We want to have rent index: A statistical model that sets the rent (per square meter) into relation with factors such as age of building, number of rooms, ...
* A research team has collected chest x-ray images of patients with and without COVID-19. These are used to build a COVID-19 classifier based on inflammation of the lung. A closer look at the data-generating proces shows: non-COVID-19 and COVID-19 images do not only differ in the medical condition, but the x-ray images come from different data-generating processes. COVID-19 images are more likely from a horizontal position where the patient lies down because they are so exhausted. One of the non-COVID-19 dataets are even just children x-ray images. I picked this example, because it is a paper from a [machine learning mindset](#machine-learning). Statisticians would think much more about the data-generating process, and are, IMHO, more likely to spot such mistakes. TODO: CITE critique paper.

If these thoughts relating to the data-generating process sound like common sense to you, it's because I think they are.
Common among statisticians.
Surprisingly uncommon among non-statistician mindsets.
For example, for [machine learning](#machine-learning) considerations of the data-generating process play a subordinate or no role at all.
For machine learning competitions, for example, the winner is, most of the times, solely determined by the lowest loss function.
It's not considered when some approach has a more meaningful consideration of the data-generating process, and an understandable model.

Sometimes we want to control the data-generating process.
For example by using randomized clinical trials, or experiments with specific designs.


The idea of the data-generating process also plays an important role for statistical inference.
Inference is about transporting the insights from the statistical model to the real world.
A good statistician makes a good argument why this is allowed.
For example, the mean income would be wrong to assume for all computer scientists when we identified that people with high income are more likely to omit their income.
A statistician would have to adjust for that before we make inference.




## Strengths

The statistical modeling mindset is a *language to see the world*.
The language consists of random variables and probability distributions.
Even if not used for inference, it can be beneficial to think in terms of variables and distributions.

* Especially good when, well, the true underlying process really is a long-run, repeated experiment. Like probabilities in casino such as roulette. 
* Solid theoretical foundation.
* Data-generating process is a powerful mental model that encourages a mindful modeling approach, and asking detailed question.
* DGP Also invites to work closely with experts.
* Conditional probability models can not only be used for learning the parameters of interest, but also for making predictions 
* Probability distributions give us a language to express uncertainty. Arguably [Bayesianism](#bayesianism) has the most coherent and emphasized focus on expressing uncertainty.

## Limitations

* World not necessarily as neat and organizable into distributions.
* Statistical modeling approaches (as defined here) don't work so well for images and text, for example.
* Very "manual": You have to define distributions by yourself.
* Needs a lot of assumptions: That you picked the right distributions, and so on.
* Not as good in making predictions as [supervised learning](#supervised).

[^savage1972]: Savage, Leonard J. The foundations of statistics. Courier Corporation, 1972.

[^density]: For continuous variables like water intake, technically we may not interpret probability density functions at a certain value, like P(intake = 2.5 liter) as probability. Instead we have to take the integral, for example over the range of 2.4 to 2.6 liters and then may interpret as the probability.

[^semiparametric]: Some approaches don't assume a closed form distribution. For example the Cox proportional hazards model. For the Cox model, we optimize the partial likelihood. These approaches are called semiparametric.

