# Frequentist Inference {#frequentist-inference}


* Popular modeling mindset in science.
* The world consists of probability distributions with fixed parameters that have to be uncovered.
* Interprets probability as long-run relative frequencies from which hypothesis tests, confidence intervals and p-values are derived.
* A statistical mindset, with [Bayesian inference](#bayesian-inference) and [likelihoodism](#likelihoodism) as alternatives.


<!-- TODOs:

-->

<!-- CONTENT TO ADD 
*  Write about difficulty of writing this chapter: I grew up with frequentist mindset, so it was an act to frame what is really special about this mindset. like growing up in a city and being asked what is special about this city: you have to know other cities well to do it. you then have to question what you take for granted.
*  Power of statistical tests
* Lindley Paradox https://en.wikipedia.org/wiki/Lindley%27s_paradox
* https://en.wikipedia.org/wiki/Foundations_of_statistics#Comparisons_of_characteristics
-->


<!-- example of frequentist study -->
<!-- source of study:  https://academic.oup.com/aje/article/154/8/748/131397 -->
Drinking alcohol is associated with a higher risk of diabetes in middle-aged men.
At least this is what a study claims. [@kao2001alcohol]

The study modeled type II diabetes as a function of various risk factors.
The researchers found out that alcohol significantly increases the diabetes risk for middle-aged men by a factor of $1.81$.

<!-- familiar frequentist terms -->
"Significant" and "associated with" are familiar terms when reading about scientific results.
The researchers in the study used a popular modeling mindset to draw conclusions from the data: frequentist inference.
There is no particular reason why I chose this study other than it is not exceptional.
When someone thinks in significance levels, p-values, hypothesis tests, null hypotheses, and confidence intervals, they are probably frequentist.

<!-- dominant mindset -->
In many scientific fields, such as medicine and psychology, frequentist inference is the dominant mindset. 
All frequentist papers follow similar patterns, make similar assumptions, and contain similar tables and figures.
Knowing how to interpret model coefficients, confidence intervals and p-values is like a key to contemporary scientific progress.
Or at least a good part of it.
Frequentism not only dominates science, but has a firm foothold in industry as well:
Statisticians, data scientists, and whatever the role will be called in the future, use frequentist inference to create value for businesses:
From analyzing A/B tests for a website to calculating portfolio risk to monitoring quality on production lines.

<!-- frequentist criticized -->
As much as frequentism dominates the world of data, it's also criticized.
Frequentist inference has been the analysis method for scientific "findings" that turned out to be a waste of research time.
You may have heard about the replication crisis.[@ioannidis2005most]
Many scientific findings in psychology, medicine, social sciences and other fields could not be replicated.
The problem with that is that replication is at the center of the scientific method.
Many causes have contributed to the replication crisis
But frequentist statistics is right in the middle of it.
The frequentist mindset enables practices such as multiple testing and p-hacking.
Mix this with the pressure on academics to "publish or perish".
The result is a community that is incentivized to squeeze out "significant" results at a high rate.
Frequentism is a decision-focused mindset and can give seemingly simple yes/no answers.
Humans are lazy.
So we tend to forget all the footnotes and remarks that come with the model.

<!-- frequentism is statistical model -->
Frequentist inference is a statistical modeling mindset:
It depends on random variables, probability distributions, and statistical models.
But as mentioned in the chapter [Statistical Modeling](#statistical-modeling), these ingredients are not sufficient to make statements about the world.

<!-- probability interpretation --> 
Frequentism comes with a specific interpretation of probability:
Probability is seen as the relative frequency of an event in infinitely repeated trials.
That's why it's called frequentism: frequentist inference emphasizes the (relative) frequency of events.
But how do these long-run frequencies help to gain insights from the model?

<!-- wine example continued -->
Let's go back to the $1.81$ increase in diabetes risk among men who drink a lot of alcohol.
$1.81$ is larger than $1$, so there seems to be a difference between men who drink alcohol and the ones who don't.
But how can the researchers be sure that the $1.81$ is not a random result?
<!-- dice example -->
For fair dice, the average eyes in the long-run series of experiments is 3.5.
If I roll a die 10 times and the average is 4, would you say it's an unfair die?
No? Would you say it's unfair if the average is 4.5? 5? Or if a 6 shows up 10 times?

<!-- uncertainty in the wine example -->
The researchers applied frequentist thinking to decide between randomness and true effects.
The parameter of interest is a coefficient in a logistic regression model.
The logistic regression model links variables such as alcohol to diabetes.
In the diabetes study, a 95\% confidence interval for the alcohol coefficient was reported:
The interval goes from $1.14$ to $2.92$.
This interval settles the question of randomness versus signal: 
The interval doesn't contain $1$, and so the researchers concluded that alcohol is a risk factor for diabetes (in men).
This confidence interval describes uncertainty regarding the alcohol coefficient.
If we were to repeat the analysis many times with new samples, the respective 95\% confidence interval would cover the "true" parameter 95% of the time. 
Always under the condition that the model assumptions were correct. 

## Frequentist probability

<!-- frequentist interpretation of example -->
The interpretation of the confidence interval reveals the **core philosophy of frequentism**:

* The world can be described by probability distributions;
* The parameters of the probability distributions are constant and unknown;
* Repeated measurements/experiments reveal the true parameter values in the long-run.

In contrast, [Bayesianism](#bayesian-inference) assumes that the parameters of the distributions are themselves random variables.
As the frequentists collect more and more data ($n \to \infty$), their parameter estimation gets closer and closer to the true parameter (if the estimator is unbiased).
With each additional data point, the uncertainty of the estimated parameter shrinks and the confidence interval becomes narrower.


<!-- other examples -->
The frequentist interpretation of probability requires imagination.
Frequentists start with a population in mind.
The population can be adults between 20 and 29 living in Iceland, daily measurements of water quality of the Nile River, or 1-inch wood screws manufactured in a factory in the U.S. state of Texas.
These populations can be described by finding out their probability distributions.
Going back to the initial example:
What's the probability that a middle-aged man will develop diabetes in the next 12 months?
Frequentists would say: There is an unknown and fixed probability for diabetes.
The more people we observe, the more accurate our estimate of the probability of diabetes becomes.
We estimate the probability of diabetes as the relative frequency of diabetes in the population.
Probabilities are frequencies in the long-run:

$$P(X=1) = \lim_{n \mapsto \infty} \frac{1}{n}\sum_{i=1}^{n} I(x_i = 1)$$.

```{r dice, fig.cap = "The line shows how the relative frequency of 6 eyes changes as the number of dice roles increases from 1 to 100 (left to right).", fig.height = 2, fig.width = 9, cache = TRUE}
library(ggplot2)
set.seed(6)
n = 150

xsample = sample(1:6, size = n, replace = TRUE)
x3 = xsample == 3
dice_dat = data.frame(xsample = xsample, x3 = x3, cs = cumsum(x3),  p = cumsum(x3)/(1:n), n = 1:n)

ggplot(dice_dat, aes(x = n, y = p)) +
geom_segment(x = 0, xend = n, y = 1/6, yend = 1/6, linetype = 2) +
geom_line(size = 1.5, color = "darkgrey") +
annotate("label", x = 0, y = 1/6, label = "1/6", size = 10) +
theme_void()
```


<!-- imagined repetition of the experiment -->
<!-- TODO: Move further down? -->
Imagining experiments that will never take place is essential to the frequentist mindset.
By defining probability in terms of long-run frequencies, frequentism requires imagining that the sampling and experiment will be done many times.
These "imagined" experiments are central to the interpretation of confidence intervals, p-values, and hypothesis tests.
And *every* interpretation of probability in the frequentist mindset has to be connected to frequencies of events in long-run samples / experiments.

<!-- likelihood principle violated -->
These imagined experiments have a curious implication for frequentism.
Frequentism violates the likelihood principle, which says that all evidence about the data is contained in the likelihood function.
But with frequentism, it's important to know what experiments we are further imagining.
You can find a simple example involving coin tosses in the chapter on [Likelihoodism](#likelihoodism).
[Likelihoodism](#likelihoodism) and [Bayesianism](#bayesianism-inference) adhere to the likelihood principle.


## Estimators are Random Variables

<!-- estimands are fixed -->
We can learn a lot about frequentist inference, especially in contrast to Bayesian inference, by understanding which "things" are random variables and which are not.
In the frequentist mindset, the estimand, the "true" but unknown parameter is assumed to be fixed.
Mean, variance and other distribution parameters, model coefficients, nuisance parameters, all are seen as having some unknown but fixed value.
And the values can be uncovered with frequentist inference.
Bayesians, in contrast, view all these parameters as random variables.

<!-- estimators are random variables -->
Since the quantities of interest are seen as fixed but unknown, the frequentist's job is to estimate them from data. 
The estimation is done with a statistical estimator: A mathematical procedure for inferring the estimand from data.
The estimator is a function of the data.
And data are realizations of random variables.
As a consequence, the estimators themselves become random variables.
Let's compare this with the Bayesian mindset:
Bayesians assume that the  parameters are random variables.
Bayesian inference updates the (prior) probability distributions of the parameters, which results in the posterior distributions of the parameters.

<!-- other parameters -->
Typical frequentist constructs like confidence intervals, test statistics and p-values are also random variables.
Mix this with the long-run frequencies and you get a special interpretation, for example, for [confidence intervals](#confidence-intervals).

<!-- example -->
Let's say you want to know how many teas you drink on average per day.
If you are a frequentist, you would assume that there is a true but unknown daily number of teas.
Let's call this estimand $\lambda$.
The frequentist might assume that the daily number of teas follows a Poisson distribution.
The Poisson distribution can handle count data well, and is described by the "intensity" $\lambda$ with which events happen.
The intensity parameter $\lambda$ is also the expected number of events.
Teas in our case.
We could estimate the tea intensity using the maximum likelihood estimator: $\hat{\lambda}= \frac{1}{n} \sum_{i=1}^n k_i$, where $k_i$ is the number of teas on day $i$.
Our estimator $\hat{\lambda}$ is a random variable.
If the model assumptions are correct and if the world is truly frequentist, then the estimator $\hat{\lambda}$ will get closer and closer to the true $\lambda$ as $n$ increases.
The estimator $\hat{\lambda}$ approximately follows a Normal distribution.

<!-- connecting estimators with the world -->
Frequentist inference builds on the fact that the estimators are random variables.
Combined with the idea of fixed true parameters, it becomes possible to connect the analysis results to the world.
A commonly used tool to derive insights about the world is null hypothesis significance testing. 

## Null Hypothesis Significance Testing

<!-- example of decision under frequentism -->
Let's say, your estimator $\hat{\lambda}$ says that you drink 2.5 teas per day on average.
Initially you had the hypothesis that you drink at least 3.0 teas per day.
Obviously, $2.5 \neq 3.0$, so the initial hypothesis is incorrect.
Case closed.
But that would be too simple an answer, wouldn't it?
You also wouldn't say that a coin is unfair if heads come up in 51/100 tosses just because $51 \neq 50$.
But when would a frequentist reject the initial hypothesis of 3.0 teas?
Would we reject the hypothesis if we get $\hat{\lambda}<2.9$, or $\hat{\lambda}<2.5$ or maybe must it be much lower, like $\hat{\lambda}<1.5$?
With the [statistical modeling mindset](#statistical-modeling) alone, we can't answer this question.

<!-- frequentist answer: NHST -->
The frequentist mindset has an answer to this question of whether to accept or reject a hypothesis.
The frequentist estimator for the number of teas is a random variable that is supposed to approximate the true number of teas.
We can make (frequentist) probabilistic statements about this estimator.
And while the true value for $\lambda$ is unknown, we can study the hypothesis of $\lambda = 3.0$ by examining the random variable $\hat{\lambda}$.

<!-- central idea of frequentism -->
This idea of proposing a hypothesis, and then accepting or rejecting it based on a statistical model or test is called null hypothesis significance testing. [^other-testing-procedures]
Hypothesis testing is a central method in the frequentist modeling mindset.
Hypothesis tests simplify decisions:
The frequentist accepts or rejects the so-called null hypothesis based on the results of the statistical model.
A statistical model can be very simple:
It can be as simple as assuming that the data follow a Normal distribution and comparing two means with a Student t-test.

<!-- NHST example -->
How does a hypothesis test work?

* Start with a hypothesis.
* Formulate the **alternative or null hypothesis**.
* Decide which statistical test to use. This step includes modeling the data. In fact, all statistical tests are statistical models [@mcelreath2020statistical]
* Calculate the distribution of the parameter estimates under the null hypothesis (or rather, the test statistic $T$).
* Choose the significance level $\alpha$: the probability threshold at which to reject the null hypothesis assuming it would be true. Often $\alpha = 0.05$.
* Calculate the p-value: Assume that the null hypothesis is correct. Then p is the probability of getting a more extreme test statistic $T$ than was actually observed. See figure \@ref(fig:hypothesis).
* If $\text{p-value} <\alpha$, then the null hypothesis is rejected.


Some examples of tests and test statistics:

* Comparing the means of two distributions. Do Germans consume more pretzels than U.S. Americans? Hypothesis: Germans eat more pretzels. The "model" of the data simply assumes a Normal distribution for average pretzel consumption per person and year. The null hypothesis would be that Germans and U.S. Americans consume the same amount. Then we would run a t-test. The test statistic in the t-test is the (scaled) difference of the two means.
* Estimating the effect of one variable on another. Is surgery better than physiotherapy for treating a torn meniscus in your knee? The statistical model could be a linear regression model. The model could predict knee pain dependent on whether a patient had physiotherapy or surgery. The null hypothesis would be that there is no difference in pain, so a model coefficient of zero for surgery/physiotherapy. The test statistic $T$ would be the coefficient divided by its standard deviation.

<!-- the p-value -->
The p-value has a frequentist interpretation because it's based on long-run frequencies.
To interpret the p-value, we have to pretend that the null hypothesis is true.
Then the p-value is the probability of observing the outcome of our analysis or a more extreme one.
Again, the frequentist interprets probability with imagined future experiments.
A p-value of 0.03 for an estimated average of 3.0 daily teas would mean the following:
If we repeat the analysis many times and the null hypothesis $\lambda = 2.5$ is correct, 3% of the time we would observe an estimate of $\hat{\lambda} \geq 3$.
If $\alpha = 0.05$ was chosen, the null hypothesis would be rejected.

```{r hypothesis, fig.cap = "Frequentists make binary decisions based on hypothesis tests. Assuming the null distribution, the null hypothesis is rejected if the observed estimand is extreme.", fig.height = 3, fig.width = 9}
library(dplyr)
library(tidyr)
library(ggplot2)
library(patchwork)
xseq = seq(from = -4, to = 4, length.out = 1000)
dens  = dnorm(xseq)
dat = data.frame(x = xseq,  dens = dens)
q95 = qnorm(0.95)
dat$significant = dat$x > q95
p = ggplot(dat, aes(x = x, y = dens)) +
  geom_line() +
  geom_area(data = dat[dat$significant,], fill = "lightgrey") +
  scale_x_continuous("Value of Random Variable") +
  scale_y_continuous("Density") +
  annotate("segment", x = q95, xend = q95, y = 0.25, yend = 0.15, arrow = arrow()) +
  annotate(label = "significance \n threshold", x = q95, y = 0.27, geom = "label") +
  annotate(label = "Distribution under \n Null Hypothesis", x = 0, y = 0.2, geom = "label") +
  theme_void() +
  ggtitle("One-sided hypothesis")

dat2 = dat
q2.5 = qnorm(0.025) 
q97.5 = qnorm(0.975)
dat2$significant1 = dat$x > q97.5
dat2$significant2 = dat$x < q2.5

p3 = ggplot(dat2, aes(x = x, y = dens)) +
  geom_line() +
  geom_area(data = dat2[dat2$significant1,], fill = "lightgrey") +
  geom_area(data = dat2[dat2$significant2,], fill = "lightgrey") +
  scale_x_continuous("Value of Random Variable") +
  scale_y_continuous("Density") +
  annotate("segment", x = q97.5, xend = q97.5, y = 0.25, yend = 0.15, arrow = arrow()) +
  annotate("segment", x = q2.5, xend = q2.5, y = 0.25, yend = 0.15, arrow = arrow()) +
  annotate(label = "significance \n threshold", x = q2.5, y = 0.27, geom = "label") +
  annotate(label = "significance \n threshold", x = q97.5, y = 0.27, geom = "label") +
  annotate(label = "Distribution under \n Null Hypothesis", x = 0, y = 0.2, geom = "label") +
  ggtitle("Two-sided hypothesis") +
  theme_void()


dat$p1 = dat$x > qnorm(0.95)
dat$p3 = dat$x > qnorm(0.68)
dat$p4 = dat$x > qnorm(0.99)
dat = dat %>% pivot_longer(cols = c(p1, p3, p4))
relabel = c(p1 = "p = 0.05", p3 = "p = 0.32", p4 = "p = 0.01")
dat$name = relabel[dat$name]
dat2  = dat %>% group_by(name) %>% summarize(xmin = min(x[value])) 

p2 = ggplot(dat, aes(x = x, y = dens)) +
  #geom_area(data = dat[dat$value,], fill = "lightgrey") +
  geom_area(data = dat[dat$significant,], fill = "lightgrey") +
  scale_x_continuous("Value of Random Variable") +
  geom_line() +
  scale_x_continuous("Value of Random Variable") +
  scale_y_continuous("Density", limit = c(-0.1, NA)) +
  geom_segment(data = dat2, aes(x = xmin, xend = xmin), y = -0.07, yend = -0.01, arrow = arrow(length = unit("0.3", "cm"))) +
  #annotate(x = q95, xend = q95, y = 0.25, yend = 0, lty = 2, geom = "segment") +
  facet_grid(. ~ name) +
#  annotate(label = "Distribution under \n Null Hypothesis", x = 0, y = 0.2, geom = "label") +
  theme_void() +
  theme(strip.text.x = element_text(size = 15))
(p | p3) / p2

```

<!-- Weirdness of H_0 -->
Null hypothesis testing is very weird.
It's like answering the question around two corners.
Let's say a researcher wants to prove that a drug prevents migraines.
They test the drug because they expect it to work, so the hypothesis they assume to be true is that patients that take the drug have fewer migraines.
But the null hypothesis is formulated the other way around:
The null hypothesis assumes that the drug has no effect.
Suddenly the goal of the researcher becomes to show that the null hypothesis is false, rather than showing that their hypothesis is correct.
The problem with statistical models is: We can't prove that they are true because our assumptions are not testable.
With frequentist inference, however, we can tell how likely a model result is under a given hypothesis and given the data.
That's why hypothesis tests work by rejection.
The [likelihoodist mindset](#likelihoodism) navigates this issue:
two statistical models are compared in terms of the evidence through the likelihood.


Null hypothesis tests are even more troublesome.

* The choice of the null hypothesis is arbitrary.
* If the null hypothesis is accepted, it's not evidence that it's true. It just means that the data that were observed are not in conflict with the null hypothesis. But there is still an infinite number of models that could have produced the same results.
* If the null hypothesis is rejected, it doesn't mean that the hypothesis of interest is true.
* A significant result doesn't mean that the deviation from the null hypothesis is relevant. The drug has a significant effect on the disease progression? The difference might be too small to be relevant.
* Especially the larger the data sample, the more likely the null hypothesis is rejected, because the tiniest differences to the null hypothesis are enough to produce significant results when $n$ becomes large.

## Confidence Intervals {#confidence-intervals}

<!-- Alternative, but equivalent: Confidence interval -->
Frequentists use confidence intervals as an alternative to statistical tests.
Hypothesis tests and confidence intervals ultimately lead to the same decisions, but confidence intervals are more informative. <!-- citation needed -->
Many statisticians prefer confidence intervals over mere p-values. <!-- citation needed -->

<!-- CI ingredients -->
Remember that estimators, such as model parameters, are random variables?
That means that estimators have probability distributions.
A confidence interval describes where the mass of that distribution lies.
The interval consists of the estimator, and the lower and upper bounds for the mass of the distribution.
The modeler decides the percentage of the distribution in the confidence interval through the $\alpha$-level.
If $\alpha = 0.05$, then we get a 95\%-confidence interval.
The construction of the confidence interval depends on the probability distribution we have derived for the quantity of interest (coefficient, mean estimate, ...).

```{r dice-confint, fig.cap = "Dice roll example with 95\\% confidence intervals.", fig.height = 4, fig.width = 10, dependson="dice", eval = FALSE}
library("epitools")
ci = apply(dice_dat, 1, function(x){binom.exact(x["cs"], x["n"])[c("lower", "upper")]})
ci = data.frame(data.table::rbindlist(ci))
dat2 = cbind(dice_dat, ci)

ggplot(dat2, aes(x = n, y = p)) +
geom_segment(x = 0, xend = n, y = 1/6, yend = 1/6, linetype = 2) +
geom_line(size = 1.5, color = "darkgrey") +
annotate("label", x = 0, y = 1/6, label = "1/6", size = 8) +
theme_void() +
geom_line(aes(y = lower)) +
geom_line(aes(y = upper))

```

<!-- Interpretation of confidence intervals -->
How are the confidence intervals to be interpreted?
Well, in a frequentist manner, of course!
The "true" parameter value is fixed, so it's not a random variable.
To say that the true parameter is in the confidence interval with a 95\% probability would be false.
The true parameter is either in the interval or it's  not, we just don't know.
The confidence itself is a random variable since it's derived from data and therefore from other random variables.
So the interpretation of a 95\% confidence interval is:
If we were to repeat the analysis many times, the confidence interval would cover the true value of the quantity of interest 95% of the time.
Only given that the model assumptions are correct.
As you can see, this is a very frequentist point of view: the confidence interval is interpreted in the context of repeated experiments.

<!-- Compared CI to tests -->
<!-- TODO: Remove? -->
<!--
Confidence intervals look different from tests at first glance.
But there is an equivalence.
A confidence interval can be translated into a hypothesis tests that give the same accept/reject result for the same significance level $\alpha$.
But the confidence interval contains more information than a simple hypothesis test.
The interval gives the estimate, and lower and upper bounds.
This gives the modeler a better sense of the estimate and its uncertainty.
Rejection of the null hypothesis is equivalent to the null hypothesis being outside of the confidence interval.

-->

```{r ci, fig.cap = "100 95\\% confidence intervals and the true value."}
library(ggplot2)
library(data.table)
nrep = 40
n = 20
alpha = 0.05
set.seed(1)

dat = lapply(1:nrep, function(nrep){
x = rnorm(n)
mm = mean(x)
lupper = qnorm(1 - alpha/2)
llower = qnorm(alpha/2)
data.frame(nrep = nrep,
           x    = x,
           mean = mm,
           lower = mm + llower * sd(x)/sqrt(n),
           upper = mm + lupper * sd(x)/sqrt(n)
           )
})

dat = data.table::rbindlist(dat)
dat$outside = ifelse((dat$upper < 0) | (dat$lower > 0),
                     "Rejected",
                     "Accepted")
ggplot(dat) +
  geom_segment(x = 0, xend = 0, y = -2, yend = nrep + 2, lty = 2) +
  geom_segment(x = 0, xend = 0, y = nrep + 7, yend = nrep + 3, arrow = arrow()) +
  annotate(x = 0, y = nrep + 9, label = "True Mean", geom = "label") +
  geom_segment(aes(x = lower, xend = upper, y = nrep, yend = nrep, color = outside)) +
  geom_point(aes(x = mean, y = nrep, color = outside), size = 1) +
  theme_void() +
  scale_y_continuous(limits = c(-10, nrep + 10)) +
  scale_color_manual("Null Hypothesis", values = c("darkgrey", "black"))

```

## Strengths

* Once you understand frequentist inference, you have the key to understanding most modern research findings. I studied statistics and can now quickly grasp many research papers. For example, to figure out whether I should have knee surgery for my torn meniscus, I read papers comparing knee surgery and physiotherapy alone. All of those papers used frequentist methods, and although I didn't understand everything, I was able to quickly get an idea of their analyses and results.
* Frequentist methods are generally faster to compute than methods from [Bayesian inference](#bayesian-inference) or [machine learning](#supervised-ml).
* Compared to [Bayesianism](#bayesian-inference), no prior information about the parameters is required. This makes frequentism more objective.
* Frequentism allows binary decisions (accept/reject hypothesis). This simplicity is one of the reasons why frequentism is popular for both scientific publications and business decisions.
* Frequentism has all advantages of [statistical models](#statistical-modeling) in general: a solid theoretical foundation and an appreciation of the data-generating process.
* When the underlying process is a long-run, repeated experiment, frequentist inference shines. Casino games, model benchmarks, ...
* The scientific method requires that scientific experiments be repeatable. The frequentist idea that truth lies in long-run frequencies of experiments is  therefore well compatible with a core idea of science.  

## Limitations

* Frequentism makes it easy to **over-simplify questions** into yes/no-questions. Reducing models to binary decisions obscures critical model assumptions and the difficult trade-offs that had to be made for the analysis.
* Focusing on p-values encourages **p-hacking**: the either conscious or unconscious search for "positive" results. Guided by the lure of a significant result, researchers and data scientists may play around with their analysis until the p-value in question is small enough. With $\alpha$-level of 0.05, 1 in 20 null hypotheses are falsely rejected. P-hacking increases this percentage of false positive findings. 
* Similarly, if the analysis is exploratory rather than hypothesis-driven, a naive frequentist approach may produce many false positive findings. Look again at figure \@ref(fig:ci): Imagine these were confidence intervals for different variables. Again, for $\alpha = 0.05$, we would expect 1 in 20 hypothesis tests to yield false positives. Now imagine a data scientist testing hundreds of hypothesis tests. This problem is called the multiple testing problem. There are solutions, but they are not always used and multiple testing can be very subtle.
* The frequentist interpretation of probability is very awkward when it comes to confidence intervals and p-values. They are commonly misinterpreted. Arguably, frequentist confidence intervals are not what practitioners want. [Bayesian](#bayesian-inference) credibility intervals are more aligned with the natural interpretation of uncertainty.
* Frequentist analysis depends not only on the data, but also on the experimental design. This is a violation of the likelihood principle that says that all information about the data must be contained in the likelihood, see also the example in the [Likelihoodism](#likelihoodism) chapter.
* Frequentist probability can fail in the simplest scenarios: Imagine you are modeling the probability of rain in August. The data only has 20 August days, all of which are without rain. The frequentist answer is that there is absolutely no chance that it will ever rain in August. The frequentist recommendation is that to collect more data if we want a better answer. [Bayesianism](#bayesian-inference) offers a solution to involve prior information for such estimates.
* There is an "off-the-shelf"-mentality among users of frequentist inference. Instead of carefully adapting a probability model to the data, an off-the-shelf statistical test or statistical model is chosen. The choice is based on just a few properties of the data. For example, there are popular flow charts of choosing an appropriate statistical test. <!-- https://bookdown.org/content/4857/generalized-linear-madness.html --> <!-- citation needed -->
* Frequentist statistics says nothing about causality except that "correlation does not imply causation".
* Weird interpretation of probability: Often it does not make any sense to interpret every probability with imagined experiments. For example, the probability for a party to get the majority vote in an election requires to imagine multiple elections, yet under the same circumstances, like same year, same candidates, and so on.  

[^other-testing-procedures]: There are two other "main" approaches for hypothesis testing: The Fisher approach, and the Neyman-Pearson approach. Null hypothesis significance testing is a combination of the two.  [@perezgonzalez2015fisher]
