# Reinforcement Learning {#reinforcement-learning}

<!-- 
TODO: checkout https://spinningup.openai.com/en/latest/spinningup/rl_intro.html

-->

* The world is dynamic: The model is about an actor acting in an environment. The actions of the agent are only rewarded at the end.
* Reinforcement learning balances exploration and exploitation,
* A sub-mindset of [machine learning](#machine-learning).


<!-- motivation: joke about how static other mindsets are -->
- start with some joke about how static the other mindsets are
- reinforcement learner meets the others
- they are frozen in time


<!-- examples of reinforcement learning -->
I guess since AlphaGos win against human, even many layperson might have heard about reinforcement learning.
- AlphaGO, Starcraft, ...
- very general mindset, applicable in many areas
- TODO: Find list of application areas
- example 1: baking cookies in the Google cafeteria. TODO: research the story
- example 2: cooling servers (also google?). TODO: Checkout story
- example 3? TODO: research another cool application 


## The Reinforcement Learning Mindset

- Dynamic mindset
- instead of just inspecting static dataset, an agent "explores" and interacts with an environment
- machine learning mindset
- different from supervised: no labelled input output pairs needed. supervised would need for every agent decision some information how good the decision was.
- reinforcement learning, in contrast, gets the reward in the end. and has to propagate it all the way back to learn
- finding balance between exploration and exploitation
- exploitation: exploiting current knowledge of how to get a nice reward
- exploration: venture out into uncharterd territory and find out what rewards await there.


## Reinforcement Learning Theory

- goal: agent should find optimal policy to maximize the reward function
- the model of the  world is described with:
  - state of agent and environment ($S$), for each time step
  - time
    - time is a differentiator to the other mindsets
    - there is of course time series forecasting in supervised and in stats, but time is different in RL
  - an agent
  - a set of possible actions $A$ the agent could take. can also change at each time step
  - the action that was taken for each time step
  - a policy
  - the reinforcement learning algorithm
  - a reward, which the agent receives at the very end of the "game", or at intermediate time points
  - value: the expected long-term value (discounted) of a state?
  - value function: maps from state to value.
- agent interacts with environment via actions which changes the state
- the policy is a mapping/function from the states and a possible action to a probability. The probability of the agent to take a certain action given a certain state $P(a_t = a | s_t = s)$.
-

### Policies

- Policy is rule used for agent
- policy tells what action to take
- policy can be deterministic or stochastic
- policy: $a_t = \mu(s_t)$ or $a_t = \pi(\cdot | s_t)$
- you can see policy as the "brain" of the agent


### Trajectories

- Trajectory is a sequence of states and actions
- $\tau = (s_0, a_0, s_1, a_1, \ldots)$
- First state can be a random sample
- Random sample from start-state distribution
- But this depends on the specific simulation scenario
- If start is expected to be the same, can be just the same start anyways.
-

### Value Functions

- Value function tells us the value of a state
- Value is expected return for a specific state, or state-action pair
- Four way to define value functions:
  - on-policy value function. only depends on the state, and assumes that always acting based on policy
  - on-policy action-value function: taking some action a in state s, but then always following policy
  - optimal value function: expected return for starting in s and always following the optimal policy
  - optimal action-value function: expected return for starting in state s, taking arbitrary action a and the always following the optimal policy.


## Deep Reinforcement Learning

- policy is parameterized
- the connection between the state and the action is parameterized
- more specifically, a neural network is doing the decisions
- weights of the neural network are learnable
- so the policy can be learned by adjusting the neural network weights
-

## The Bellman Equations

TODO: check reinforcement learning book p. 79 
$$V(s) = \arg \max_{a} \left{R(s, a) + \gamma \sum_{S'} P(s,a,s')V(s') \right} $$

- Value of state s
- defined recursively, based on the value of the next state
- think of the last step where the value is clear when you get the reward, like win or lose a game
-

## Misc

- in general, there are many different types of reinforcement learning algorithms (next to deep reinforcement learning): Q-learning, SARSA, ...
- we talked about RL assuming full observability of all states.
  - but you might also have partial observability only. like fog of war in games, or the fact that you don't even know what's happening in the room next to you.
  - state is always all the information about the environment
  - observation might be partial. that's what the agent gets.
- Model-free vs model-based: Model-based means that the agent either learns a model of the environment or it has access to a model of it.

## Impressive to Look At


- Works well in games and simulations
- fun to see in action:
  - Go in action
  - watch starcraft matches
  - see little figures run around in some world, being adorable and such
- these applications make it seem as though we a just two steps away from AGI
- results are impressive
- but: in very particular enviroment
- hard to carry into the real world
- or hard to translate into different environment
-


## AI fails

- I always looked for opportunities to cite this paper
- mischieveous AI
- malicious compliance
- just too funny 
- TODO: Cite this paper where the reward fails
- example: jumping really high, and instead it learns to make a salto
-

## Strengths

- dynamic
- with interactions in mind
- often more realistic to think of a model interacting with an environemnt
- especially if understanding policy better
- works well in games and simulations

## Limitations

- requires lots of simulations or runs. since so many are required, usually have to resort to simulation
- requires that the task of interest can be sufficiently simulated in some way
- also the transfer from simulation to real world is tricky
- I think there is a reason why we are not seeing RL to be used very often.
- The big wins and big headlines are because of games. In those cases, the code could be directly adapted for simulation. And then when applied "in real life" the game was exactly the same environment
- Training can be difficult
- Difficult to define the reward correctly. RL algo might finish task in unexpected ways
-

## References

- AI Fail paper
- Reinforcement Learning, An introduction [@sutton2018reinforcement]


```{r rl, fig.cap = "Reinforcement Learning."}
r1 = rectFun(c(0, 0))
r2 = rectFun(c(0, 2)) 

ggplot(mapping = aes(x = x, y = y)) +
  geom_path(data = r1) +
  geom_path(data = r2)

```



visualizations

- state, policy, agent. arrow from agent to environment, labelled action. arrow from environment to agent, labelled state,reward. maybe should be two arrows, one for rewards, one for observation/state.
  - agent could be a box further containing policy, and rl algo: state arrow goes into agent, and splits into two, one to policy, one to rl algo, one arrow from algo to policy, labelled policy update; one arrow out of policy, splitting to one as action into environment, the other goes back into the algo.
  - see: https://www.mathworks.com/help/reinforcement-learning/ug/what-is-reinforcement-learning.html
-  
