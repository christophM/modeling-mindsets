# Reinforcement Learning {#reinforcement-learning}

<!-- 
TODO:

- find out examples of reward function: AlphaGo, Starcraft, self-driving cars
- List some reinforcement learning algorithms: Q-learning, MDP?, table-based methods, ... in general, there are many different types of reinforcement learning algorithms (next to deep reinforcement learning): Q-learning, SARSA, ...
- Write about reward shaping somewhere
-->

<!-- topics that were left out

- bellman equations
-

-->

* The world is dynamic and so is the reinforcement learning mindset: The "model" is an actor interacting with its environment, collecting rewards.
* Learning is a mix of exploiting knowledge about beneficial actions and exploring new options.
* Reinforcement learning is useful in robotics, games and simulations.
* Together with [supervised](#supervised) and [unsupervised](#unsupervised-ml), reinforcement learning is a [machine learning](#machine-learning) mindset.


<!-- motivation: joke about how static other mindsets are -->
Three machine learners of different minds attend a conference dinner.
The dinner is served in form of a self-service buffet, and the service staff constantly refills the lavish offerings all night long.
One of the dishes is super popular and not available most of the time: Dumplings.
Once dumplings are refilled, within minutes the plate is empty again.
Also the machine learners can't escape the pull of the dumplings.
So they start to strategize, in their own way, how to get more dumplings.
The supervised learner starts gathering data, trying to predict dumpling servings based on time, appearance of other dishes and so on.
The unsupervised learner expands the goal: Why only look at dumplings? It's better to find out in more general what foods there are.
Maybe dumplings are similar to other dishes offered?
Maybe there are some outlier foods that are much better?
The reinforcement learner was gone.
Where did she go?
The other two hadn't noticed her leaving.
Suddenly she comes back, with a full plate of dumplings.
The supervised and unsupervised learners gasp in surprise.
"How did you find all these dumplings," they demand to know.
She started sharing her success story: "Since my policy of just waiting at the buffet for dumplings didn't work out, I had to explore other actions.
I walked outside where one waitress was on here cigarette break.
We had some small talk, and I just ask her about the dumplings.
She basically then gave me some dumplings straight out of the kitchen".
The other two were flabbergasted.
How should they have known that talking to people, interacting with their environment was always an option?



<!-- applications -->
Chess, Go, StarCraft II, Minecraft, various Atari games, ...
All these games have in common that computers can play them too, beating human players by a long shot.
The computer can play those games with the help of reinforcement learning.
Reinforcement learning is not only to play, but also to  
But reinforcement learning is not only for playing.
It can also steer components in complex systems.
For example, reinforcement learning was used to control the [cooling of Googles datacenters](https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/).
Reinforcement learning has been used to control robots, to control self-driving cars and much more.
It's a versatile mindset that can be used in very dynamic situations.




Welcome to the reinforcement learning mindset!

## The Reinforcement Learning Mindset

<!-- reinforcement learning theory in a nutshell -->
In reinforcement learning, the model is an agent.
And this agent has to interact with its environment.
If the agent chooses the right actions, it might collect a reward.
A lot of things can be the agent: a robotic arm, a player in a video game, a controller of a system.
Reinforcement learning is for problems where we don't have the information what the right action is.
For image classification, we often have training data and the data are labelled.
The label tells us what the right action (read: classification) for the image is.
But when playing a game, for example, we might only know at the very end how well the action went.
But we never get to know, explicitly, for each action how good the choice was.
This makes RL a holistic mindset: It considers the entire interaction of agent with environment.

<!-- intuition -->
<!-- TODO: Split up into subsections and scatter around a bit -->
Imagine riding your bicycle to work.
You have many options to design the path.
But for now, you always pick the route suggested by your navigation device.
You desire to arrive at work, and that in a timely fashion.
But you also want to optimize the route! 
There is an extra "reward" by riding through a park or seeing something nice on the way, like a nice church.
So you start exploring.
At each intersection you have different actions, stay on route, turn left, turn right.
You either exploit the current best policy by following the navi.
Or you explore and find an improvement for the route.
You are the agent, the route is the environment.
The reward function is a balance between fast route and a nice route.
The rewards are sparse: The time you will know at the end of the route, and the nicety will spark sometimes along the route.
This example also show a tricky tradeoff between immediate and future rewards:
You could cycle all the way from immediate to immediate reward by following only the nicest routes.
But you will never get to your work place.
So the total reward can even end up being negative as you will get fired.

<!-- data structure is different -->
By this framing alone you can already see how different reinforcement learning is from the other mindsets.
It's a lot more dynamic: The agent does not just make predictions and finds out what's right or wrong.
Instead it has to interact with the environment, and sometimes the actions lead a lot later to a reward only.
It's a difficult trade-off between exploration and exploitation of rewarding behavior.

<!-- still ml mindset -->
Is reinforcement learning even a machine learning mindset?
Hell yes it is.
Maybe even more so than other mindsets.
The idea is to make a computer act in an environment in a smart way.
We also don't care about the algorithm itself.
We only want to maximize the reward.
The evaluation of the performance is external:
It's the cumulative reward.
And it's not important how a high reward was achieved.


<!-- the core of RL -->
To decide whether a problem can be solved with reinforcement learning, one has to:

- Define the problem as an actor interacting with an environment
- Define actions that can be taken.
- Define how the environment is encoded: Input pixels? game state?
- Define a reward function. This is very delicate.
- Choose a reinforcement learning algorithm, depending on cardinality of input and action space, difficulty of the problem and so on.


## Agent-Based and Policy-Guided
  
<!-- What is an agent -->
Reinforcement learning is an agent-based mindset.
That means it requires that the modeling task is formulated in such a way that an agent has to interact with an environment.
An agent is an entity.
Sometimes an entity even in the physical realm, but can also be merely a software entity.
What can be an agent?

- Robotic arm in an assembly line
- Self-driving car
- Opponent in a video game
- Control units in larger system
- ...

```{r rl, fig.cap = "Reinforcement Learning."}
r1 = rectFun(c(0, 0))
r2 = rectFun(c(0, 5)) 

p = ggplot(mapping = aes(x = x, y = y)) +
  geom_path(data = r1) +
  annotate("text", label = "Environment", x = 0, y = 5) + 
  geom_path(data = r2) +
  annotate("text", label = "Agent", x = 0, y = 0) + 
  annotate("curve", x = 1, y = 0, xend = 1, yend = 5, size = 1, curvature = 0.5, arrow = arrow(ends = "first"))  +
  annotate("label", x = 1.7, y = 2.5, label = "Observation, Reward") +
  annotate("curve", x = -1, y = 5, xend = -1, yend = 0, size = 1, curvature = 0.5, arrow = arrow(ends = "first")) +
  annotate("label", x = -1.7, y = 2.5, label = "Policy, Action") +
  scale_x_continuous(limits = c(-2.1, 2.1)) +
  theme_void()

add_cc(p)
```


<!-- agent requirement is also problematic -->
But that also means that only problems that can be centered around some agent are solvable with reinforcement learning.
But you can be very creative.
For example, image classification does not seem like an reinforcement learning task.
But it can be used for, for example detecting multiple labels for an images by focusing on different regions of the image. [@chen2018recurrent]
Here the agent is a bit more abstract, but you can think of it as a part of an algorithm that searches through regions of the image.
So, if you are fully in the reinforcement mindset, you will often find ways to find an agent to serve your purposes.

<!-- Observe and Act -->
The agent can observe the state of the environment, either in part or in full.
In a game of StarCraft II, the observation would include game stats such as the collected raw materials, or the position of the own units.
The entire environment would of course include all the positions of the enemy units.
Based on the observations the agent has to make a decision and take an action.
An agent has certain options for action.
These can be discrete or numeric, which has a big influence on the algorithms that one can use to model the actions.
A discrete action would be in the game Tic-tac-toe: There are a maximum of 9 options where to draw a cross/circle.
A continuous action would be in self-driving cars: There is a continuous scale for how strongly to steer left or right.
But how does the agent get from observations to actions?

```{r rl-trajectory, fig.cap = "Trajectory of a reinforcement learning agent through the state space, with a reward at the end."}
p = ggplot() +
  annotate("segment", x = 0, xend = 8, y = 1, yend = 1, arrow = arrow(), lty = 2) +
  annotate("segment", x = 1, xend = 2, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 1, y = 1, label = "State 0") +
  annotate("segment", x = 2, xend = 3, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 2, y = -1, label = "Actor") +
  annotate("segment", x = 3, xend = 4, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 3, y = 1, label = "State 1") +
  annotate("segment", x = 4, xend = 5, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 4, y = -1, label = "Actor") +
  annotate("segment", x = 5, xend = 6, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 5, y = 1, label = "State 3") +
  annotate("segment", x = 6, xend = 7, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 6, y = -1, label = "Actor") +
  annotate("segment", x = 7, xend = 7, y = 1, yend = 1.4) +
  annotate("label", x = 7, y = 1, label = "State 4") +
  annotate("label", x = 7, y = 1.4, label = "Reward") +
  coord_fixed() +
  theme_void() +
  scale_y_continuous(limits = c(-1.5, 1.5))

add_cc(p)

```



### Policy

<!-- Policy -->
That's where the main question of reinforcement learning lies.
The **brain** of the agent is called the policy.
The policy maps from the observations to the actions.
It's the "rules" the agent follows, and instructs what ations to take.
The policy may be deterministic (Take action A) or stochastic (Take action A with 90% probability, and B with 10%)
So once the problem is framed by designing action spaces, environment and so on, the prime goal of reinforcement learning is to learn a good policy. 
But there is a huge problem: An action does not necessarily lead to a reward.
But the reward is the signal that tells us whether an action was good given the current state.

TODO: Visualize mapping between observation and actions as policy function

### Value Functions

<!-- Value function -->
The solution is to assign a value to each state.
The value function says for a specific state, or combination of state and action how good it is for the agent to be in this state.
The value is the expected return for a specific state, or state-action pair.
While rewards are directly provided by the environment, the values are something that has to be estimated or learned.
That assignment does not come for free, of course.
Reinforcement learning is also concerned with learning this value function.
There are different ways to define this value function, depending on whether we define it only for the state, or pairs of state-action.
Also based on whether we assign the value on-policy (the current policy of the agent) or for a theoretical optimal policy.
The value function is a way to spread the possibly sparse rewards across all the states.
A way to grapple the value function are the Bellman equations.
It relates future rewards to current states:


<!--
- Four way to define value functions:
  - on-policy value function. only depends on the state, and assumes that always acting based on policy
  - on-policy action-value function: taking some action a in state s, but then always following policy
  - optimal value function: expected return for starting in s and always following the optimal policy
  - optimal action-value function: expected return for starting in state s, taking arbitrary action a and the always following the optimal policy.
-->

TODO: Visualize relationship between value function and reward. also regarding action and state

### The Learning In Reinforcement Learning

<!-- Learning a policy --> 
There are many different approaches to learn a policy.
A policy is a mapping between state observation and an action.
If the space of state and action are small, you can describe the policies with tables.
Table-based, neural networks, 
The policy is learned over multiple episodes.
An episode can be playing a game once, or doing a simulation once.
The policy is a mapping/function from the states and a possible action to a probability. The probability of the agent to take a certain action given a certain state.

<!-- between exploration and exploitation -->
The difficulty in finding the right policy is in finding a good trade-off between exploration and exploitation.
Exploitation means just greedily making the "best" decisions based on the current knowledge.
But this might not be optimal globally.
So some exploration must take place.
Think back of your bike ride.
You might have found a path that takes 25 minutes, and at one part goes through a park (small reward) and once along a lake.
You can now take this path each time and get a reward.
That would be exploitation.
But here and there, you try out slightly different paths.
Exploration.
You might find out that you can adapt the route, so you cycle longer through the park that you cycle through anyways.


<!-- model-free versus model-based -->
The learning can be model-free or model-based.
A model-based approach includes a model of the environment.
Having such a model allows simulating actions:
An agent could simulate what would happen given a certain action, what are the simulated rewards?
So this then tends to become more of a planning thing.

### Compare to Other Mindset

<!-- ML -->
If you take together supervised and unsupervised learning, just by the names it seems like they would make up completely the machine learning mindset.
But reinforcement learning is a third sub- mindset of machine learning.
Reinforcement learning is special because of the treatment of time and the goal-oriented actions.

<!-- RL versus supervised -->
Is reinforcement not learning not just like supervised learning?
In supervised learning we have the labels, in reinforcement learning we have the rewards.
But they are quite different!
The labels in supervised learning appear for every training instance, telling you what the right answer would have been.
Imagine you want to use supervised learning to play Go.
You would need for every move a label what a human would have decided in that case.
So we would build a classifier: Predict the next move.
If it was the same as a human does, the classification was correct, else it was wrong.
This would lead to, at best, human performance of the game.
Also, it completely misses the aspect of exploration, thinking ahead, and working with rewards.
It would rob the playing algorithm of all "creative freedom" to develop a policy that maximizes the reward.
The reward just tells us whether a sequence of actions was good or bad, but can't tell for individual action how meaningful they were.
Rewards are usually delayed, in supervised learning the labels are immediate.
Think of a manager leading a team of programmers.
One manager looks at every ticket and checks whether the job done by the programmer was fulfilled.
He gives feedback for every ticket:
Either it was correctly done, or the manager shows them what the solution would have been.
The other managers defines what the goal is (for example, make the app load under 1 second).
How the team gets there doesn't matter to the manager, she only gives the reward.
The first is obviously the supervised learning analogy, the second manager the reinforcement learning.
From the perspective of reinforcement learning, supervised learning seems so short-sighted, so narrow.
Supervised learning only considers parts of the problem, without connecting actions.
Reinforcement learning is a much more holistic approach, connecting all interactions.


<!-- RL versus unsupervised learning -->
Reinforcement learning is also not unsupervised learning.
The reward is a sparse but strong signal for what to learn.
Unsupervised learning does not have such a unique signal.
Not every reinforcement learning problem has such a clear reward, sometimes they have to be designed.
That brings the two mindsets a bit closer.
But still, they are very different for other reasons:
Unsupervised learning lacks all these ideas of interacting with an environment, delayed rewards and so on. 

<!-- RL versus statistical modeling -->
Reinforcement learning takes a good scoop from statistical modeling.
How we talk about many concepts in reinforcement learning is in statistical terms.
We talk about probablities for actions, Markov decision processes and so on.
The mindset, however, is very different.
And it boils down to reinforcement learning being a machine learning mindset.
How the policy is learned and so on is not as important as getting the job done.
The statistical modeling mindset would be all about modeling variables explictly, relating them to each other, ...
Again, statistics here is the language with which we describe reinforcement learning, but the mindsets are different.
There is an interesting link to [causal inference](#causal-inference):
Due to reinforcement learning time-dependency, and the reward and so on, the actions have to be causal.
Let's say that in StarCraft the winning player often has the most units.
But if an agent would build a lot of worthless units, it would not win the game.
While the mere number of units is correlated with winning, it's not strictly causal.
Having lots of resources and building the right units is causal for winning.
An agent would not learn a policy that proposes non-causal actions, simply because they will not lead to a reward.
However, the agent can learn to rely on non-causal observations, which makes it vulnerable.

<!-- RL in general -->
Reinforcement learning has links to psychology and neuroscience.
It draws from machien learning, but also operations research, control theory, statistics and optimization.
Reinforcement learning is the most dynamic setup.




The connection between deep learning and reinforcement learning is a bit more special, so let's go deeper here (haha):



## Deep Reinforcement Learning

Deep learning and reinforcement learning: A fantastic fusion of approaches.
Learning a policy can be quite challenging.
The input state might be visual, so it's only pixels.
Also the action space might be large.
Fortunately, deep learning can handle such complex and large data!

TODO: Check about reinforcement learning
Deep reinforcement learning, as it is called, allows to learn a parameterized policy.
The idea of deep reinforcement learning is to replace some components in reinforcement learning with deep neural networks.
So the policy is learned by a deep neural network, and the weights are learnable.
This allows to use, for example, the entire screen of a game to serve as input to the policy.
Deep Reinforcement learning is the reason why reinforcement learning has become so hyped.
Basically like anything that deep learning touches becomes a hype topic, but that's another story.

A successful example of deep reinforcement learning is Alpha Zero.
Alpha Zero is a reinforcement learning algorithm that can play Go.
It's predecessor, Alpha Go, has bested the Go world champion Lee Sedol.
While Alpha Go was a mixture of many approaches, Alpha Zero is a pure deep reinforcement learning approach.
By the way, it can also be trained to play other games such as chess or other fully observable games.
Alpha Zero only needs the rules of the game.
It's then trained with self-play, basically playing against itself.
This happens many many times, and with each game, the gameplay is improved.
AlphaZero features two deep neural networks: a value network and a policy network.
The value network accepts the game state as input and outputs a single number, the value of that state, something between -1 and 1.
A -1 would be a certain loss, a +1 a certain win, everything inbetween is less certain.
So this value network can now be used to evaluate how good a certain position would be.
And this can also be used to weight how good certain actions would be, by enumerating the possible actions and asking the value network how good the new positions are.
The value network can be trained by a huge list of state + outcome (who won).
This data set comes from self play.
It's very much like supervised learning, but it's only a part of the entire AlphaZero algorithm. 
It's not about acting, but rather similar to image classification.
The value network gives us a sense of what a good and what a bad board position is.
The policy network also has as input the game state, but it outputs a bunch of probabilities:
For each action, the probability to follow that action, also called the "prior policy".
It's called prior policy since this action is not followed automatically.
The policy network works in tandem with a Monte Carlo tree search algorithm.
The Monte Carlo tree search connects the policy with the value of the board, and expands simulations of next steps in the game.
The policy network and the Monte Carlo tree search are in a more complex interaction with each other, which would go to deep at this point.
The training of the policy network is intertwined / looped with the tree search.


## Impressive to Look At

With all the hype, it's quite surprising how little real world applications there are in practice.
The reason is that reinforcement learning is difficult to get right.
Training can be quite unstable.
And the most difficult is, that, in order to get in enough training, it's almost impossible to train it in the real world.
This means that it's either restricted to simulations or the application itself is completely digital, like a game.
And these are the prime starting points: games and simulations.
That's also why the most impressive headlines were games beating human players.
But if you want to train a robotic arm to grap an item and put it into another spot.
It's harder.
You first need a simulation and train the reinforcement learning agent in a simulation.
But it's also difficult to make the transfer from simulation to reality.
From the simulated robotic arm to the physical one.

## Strengths

- For many problems reinforcement learning is the only viable approach.
- Reinforcement learning is dynamic. To be honest, many, many problems have such a dynamic, they are time-dependent, they require a holistic interaction approach. And yet, we use other mindsets, either because it's simpler, or because of historic reasons.
- This dynamic involves interactions -- the idea that the actions of the agent **changes** the environment. In other mindsets, the model is a mere "observer", which often is a false simplification. For example, the predictions of a model can change the environment, and by that the algorithm influences future training data.
- It's also a holistic approach. Not individual actions are judged, but rather the entire decision process. Compare that to supervised learning where every prediction is judged individually.
- Reinforcement learning is a good approach for playing games, robotics, planning and control problems.

## Limitations

- Very often, reinforcement learning, especially deep reinforcement learning, is just the [wrong approach to a problem](https://www.alexirpan.com/2018/02/14/rl-hard.html).
- Reinforcement learning models can be very difficult to train and reproduce:
  - It requires a lot of episodes, because it's rather sample inefficient.
  - Designing a reward function can be difficult.
  - The training can be unstable, and might get stuck in local optima.
- Reinforcement learning models are trained in artificial environments. It's difficult to transfer the models into the physical world.
- Reinforcement learning requires that the task involves some form of "agent".



## References

- TODO:  AI Fail paper
- Reinforcement Learning, An introduction [@sutton2018reinforcement]





