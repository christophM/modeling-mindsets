# Reinforcement Learning {#reinforcement-learning}

<!-- 
TODO:

- find out examples of reward function: AlphaGo, Starcraft, self-driving cars
- List some reinforcement learning algorithms: Q-learning, MDP?, table-based methods, ... in general, there are many different types of reinforcement learning algorithms (next to deep reinforcement learning): Q-learning, SARSA, ...
- Write about reward shaping somewhere
-->

<!-- topics that were left out

- bellman equations
-

-->

* Reinforcement learning is a framework for modeling interactive decisions of an agent based on rewards.
* Applications in robotics, games, complex systems and simulations.
* Together with [supervised](#supervised) and [unsupervised](#unsupervised-ml), reinforcement learning is a [machine learning](#machine-learning) mindset.


<!-- motivation: joke about how static other mindsets are -->
*Three machine learners attend a conference dinner.
Dinner is served as a self-service buffet.
Service staff constantly replenishes the lavish offerings throughout the evening.
Dumplings are among the most popular dishes, and, sadly not available most of the time.
The machine learners start to plan how to get their fingers on the dumplings. 
The supervised learner starts collecting data.
Surely, the arrival of the delicacy must be predictable!
The unsupervised learner thinks bigger.
What types of dishes are there, and in which cluster will the dumplings be?
The reinforcement learner merely left the table.
The other two were deep in their laptops when the reinforcement learner returned.
With dumplings.
A full plate of dumplings.
The supervised and unsupervised learners gasped in surprise.
"How did you get all these dumplings?" they wanted to know.
"First I tried my luck at the buffet, but of course the dumplings were gone.", the reinforcement learner explained.
"Then I thought about other options I have. I ended up asking one of the waiters for dumplings.
And he just gave me a full plate."
The other two were mind-blown.
How could they have known that proactively interacting with the environment is even an option?*



<!-- application -->
Chess, Go, StarCraft II, Minecraft, various Atari games, ...
Many people enjoy playing these games, or might even do so professionally.
But they are not alone.
Computers can play them to, and they are better at playing than the humans are.
Many of these games are also very complex.
Go has $10^{127}$ possible board positions, more than there are atoms in the universe (around $10^{78}$ to $10^{82}$).
StarCraft II is a complex real-time strategy game, which requires planning, resource management and military tactics.
These achievements were made possible by machine learning.
But it wasn't supervised or unsupervised learning that played us.
It's the third modeling mindset of machine learning.
The only mindset that puts interactions with an environment first.
The only mindset to give computers a "brain".

Welcome to **reinforcement learning**!

## The Reinforcement Learning Mindset

<!-- reinforcement learning theory in a nutshell -->
In reinforcement learning, the model is an agent that interacts with an environment where it has to make decisions.
This environment can be a game, a geographic map, sensor inputs, video inputs for a self-driving car, and so on.
The agent doesn't want to sell you a house; it's not running around with suit and sunglasses fighting Neo; it's not investigating a crime.
The agent in reinforcement learning is the part of the software that interacts with the environment.
It can be an entity that acts in place of a human, like a chess player, or a robotic arm.
But the agent can also be abstract, like a control unit in a smart building ventilation system.
Actions are chosen after the environment was observed.
Then the agent acts, which can change the state of the environment.
But to what end does the agent choose the action?
Similar to humans, the agent is "motivated" by receiving rewards:
Beating the other players in StarCraft, getting the temperature right in the building, collecting coins in Super Mario.
The "brain" of the agent is called the policy, which tells the agent how to behave in which situation.
The policy is a function that takes the observations as input and output an action.
A policy can be deterministic (Do A) or stochastic (Do A with 90% probability and B with 10%).

```{r rl, fig.cap = "Reinforcement Learning."}
r1 = rectFun(c(0, 0))
r2 = rectFun(c(0, 5)) 

p = ggplot(mapping = aes(x = x, y = y)) +
  geom_path(data = r1) +
  annotate("text", label = "Environment", x = 0, y = 5) + 
  geom_path(data = r2) +
  annotate("text", label = "Agent", x = 0, y = 0) + 
  annotate("curve", x = 1, y = 0, xend = 1, yend = 5, size = 1, curvature = 0.5, arrow = arrow(ends = "first"))  +
  annotate("label", x = 1.7, y = 2.5, label = "Observation, Reward") +
  annotate("curve", x = -1, y = 5, xend = -1, yend = 0, size = 1, curvature = 0.5, arrow = arrow(ends = "first")) +
  annotate("label", x = -1.7, y = 2.5, label = "Policy, Action") +
  scale_x_continuous(limits = c(-2.1, 2.1)) +
  theme_void()

add_cc(p)
```

<!--  RL is dynamic -->
Reinforcement learning is dynamic.
In most modeling mindsets, you first collect data and then build your model.
But in reinforcement learning, the data are produced by the agent interacting with the environment.
The agent chooses which states to explore, and therefore which data to generate.
It's like the agent is running it's own experiments and learning from it.
The agent cycles through:

- Observation: Look at the world.
- Action: Interact with the world.
- Reward: Get feedback on the action.

Sometimes data may be collected beforehand.
Alpha Go by the company DeepMind, for example, was initialized by Go moves from human players (in a supervised learning fashion). [@chen2018recurrent]


<!-- RL is holistic -->
In many cases model predictions end up affecting their environment, but are trained without that interaction in mind.
Think about pricing a product.
A higher price means more revenue per sale, but less customers.
A lower price means more customers, but less revenue per sale.
The goal is to find the optimal price.
Now you could have a supervised learning model that predicts number of sales based on price.
Training such a model requires access to training data with varying prices.
Even if such data were available, it might be have an inadequate.
For example, an economic crisis might have had an effect on sales, but also on price choices, making the sales/price connection difficult to analyze.
It would be much more natural to see the dynamic of the problem as it is:
Changing the price changes the "environment", in our case the sales, inventory levels, and so on.
Here, reinforcement learning is a way to run experiments.
We balance exploitation of a good price balance with exploration of different prices.
That's a much more holistic approach to pricing than having an isolated prediction model.


<!-- not all tasks -->
Reinforcement learning is a versatile mindset that can be used for dynamic problems.
But to allow such a holistic approach, a task must involve some kind of agent acting in an environment.
But what is this mysterious agent anyways?
It can be:

- Robotic arm[@gu2017deep]
- Self-driving car steering unit[@kiran2021deep]
- Go-player[@silver2016mastering]
- Cooling control unit[@li2019transforming]
- Image segmentation unit which copies human outlining an image[@wang2018outline]
- ...

<!-- still ml mindset -->
For all it's specialties and differences to supervised and unsupervised learning:
Reinforcement learning is approached in a typical machine learning manner.
Maybe even more so than the other two mindsets.
A motivation of machine learning is to make the computer do smart things.
As humans we can better relate to the computer being smart when it presents and entity, and is smart in "human" things.
The computer is good at calculations, lightyears away from any human on earth.
Not really impressive, because we are already used to it.
But when a computer acts intelligently in a game, maybe in a seemingly human manner, we are impressed.
Reinforcement learning, by letting the computer be an agent, exactly transports this idead of an intelligent computer. 

<!-- ml mindset on technical level as well -->
But also on the technical level is reinforcement learning a typical machine learning mindset.
We don't care how exactly the agents policy is implemented -- as long as it works.
"Working" is measured with clear benchmarks.
In reinforcement learning, we have a simple evaluation metric: the reward.
The more reward an agent collects, the better it is.
The reward is an external signal.



<!-- intuition -->
<!--
Imagine riding your bicycle to work.
You have many options to design the path.
But for now, you always pick the route suggested by your navigation device.
You desire to arrive at work, and that in a timely fashion.
But you also want to optimize the route! 
There is an extra "reward" by riding through a park or seeing something nice on the way, like a nice church.
So you start exploring.
At each intersection you have different actions, stay on route, turn left, turn right.
You either exploit the current best policy by following the navi.
Or you explore and find an improvement for the route.
You are the agent, the route is the environment.
The reward function is a balance between fast route and a nice route.
The rewards are sparse: The time you will know at the end of the route, and the nicety will spark sometimes along the route.
This example also show a tricky tradeoff between immediate and future rewards:
You could cycle all the way from immediate to immediate reward by following only the nicest routes.
But you will never get to your work place.
So the total reward can even end up being negative as you will get fired.
-->

<!-- the core of RL -->
<!--
To decide whether a problem can be solved with reinforcement learning, one has to:

- Define the problem as an agent interacting with an environment
- Define actions that can be taken.
- Define how the environment is encoded: Input pixels? game state?
- Define a reward function. This is very delicate.
- Choose a reinforcement learning algorithm, depending on cardinality of input and action space, difficulty of the problem and so on.
-->

  
###  Reward and Value

<!-- agents are all too human -->
Laying on the couch is easy, convincing yourself to work out can be hard.
Why?
The reward is delayed.
Only after doing many workouts you will slowly receive an award.
Turns out, that's also a problem for reinforcement learning agents.
The reward can be sparse.
Only after an entire game of Go is there a reward, either +1 for a win or -1 for a loss.
If the agent made 100 moves in that game, how should it know which moves where good, and which ones were bad?

<!-- Value function -->
The first solution is to have a value assigned to a state -- even if it doesn't return a reward.
The solution is to assign a value to each state.
In most of the interesting applications we can't just tabulate all possible states and their values.
Instead we have a value function.
The value function accepts a state as input, or possibly a combination of state and action.
The output is the value of the state or state/action.

<!-- what is a value -->
But what IS a value?
In simple terms, the value judges how good it is for the agent to be in this state.
The agent should seek out higher value states.
The value is the expected reward for a specific state, or state-action pair.
The value spreads the rewards across the states, like a knife spreads a serving of jam across bread.
When you exercise, it's because you know the value of exercising.
You imagine the future reward of your actions and value the current state accordingly.

<!-- how to learn value functions? -->
While rewards are directly provided by the environment, the value function has to be estimated.
So, a part of reinforcement learning is also concerned with learning this value function.
The simplest idea: for a given state we predict the expected reward.
Alpha Zero, for example, did exactly that.
By self-play, Alpha Zero collected a dataset of state - reward pairs.
Learning the value function was then turned into a supervised learning task.
They trained a neural network to predict win (+1) or loss (-1) from a given state.
The value-network's output, a number between -1 and +1 allows the agent to plan the next move.
Or we could start from random initial states, follow the current policy of the agent, accumulate the rewards.
Then we average the rewards for each state.
That's called Monte Carlo estimates of the value.
Obviously that only works for environments with not too many states.
<!--
There are different ways to define this value function, depending on whether we define it only for the state, or pairs of state-action.
Also based on whether we assign the value on-policy (the current policy of the agent) or for a theoretical optimal policy.
A way to grapple the value function are the Bellman equations.
-->

```{r rl-trajectory, fig.cap = "Trajectory of a reinforcement learning agent through the state space, with a reward at the end."}
p = ggplot() +
  annotate("segment", x = 0, xend = 8, y = 1, yend = 1, arrow = arrow(), lty = 2) +
  annotate("segment", x = 1, xend = 2, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 1, y = 1, label = "State 0") +
  annotate("segment", x = 2, xend = 3, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 2, y = -1, label = "Actor") +
  annotate("segment", x = 3, xend = 4, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 3, y = 1, label = "State 1") +
  annotate("segment", x = 4, xend = 5, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 4, y = -1, label = "Actor") +
  annotate("segment", x = 5, xend = 6, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 5, y = 1, label = "State 3") +
  annotate("segment", x = 6, xend = 7, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 6, y = -1, label = "Actor") +
  annotate("segment", x = 7, xend = 7, y = 1, yend = 1.4) +
  annotate("label", x = 7, y = 1, label = "State 4") +
  annotate("label", x = 7, y = 1.4, label = "Reward") +
  coord_fixed() +
  theme_void() +
  scale_y_continuous(limits = c(-1.5, 1.5))

add_cc(p)

```

TODO: Add values backpropagated from the reward


### The Learning In Reinforcement Learning

<!-- what should the agent learn? -->
Now here comes the part that can be confusing if you are not familiar with the field.
It's not immediately what the reinforcement agent should learn.
Here are different approaches:

- Learn a complete model of the environment. The agent can query such a model to simulate which would be the best action to take at each time step.
- Learn the state value function. If an agent has access to a value function, it can choose actions that maximize the value.
- Or learn the action value function which takes as input not only the state, but state AND action. 
- Learn the policy directly.

These approaches are not mutually exclusive, but can be mixed and matched.

Oh, and in addition we have many different options how we learn these things.
And that depends a lot on the dimensionality of the environment and the actions space.

TicTacToe and Go are pretty similar games.
I imagine the objections of all Go players reading this, but hear me out.
Two players face each other off in a fierce, round-based game of strategy!
The battlefield is a rectangular board with gridded positions.
Each player positions marks on the grid.
The winner is determined by the constellations of the marks.

Despite some of these conditions being similar, the games a vastly different to play for reinforcement learning (and humans).
TicTacToe as environment is often taught in reinforcement learning entry classes.
The first super-human Go agent [beat](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol) the Go champion Lee Sedol in 2016, which was a huge media spectacle.
The huge difference between TicTacToe and Go:
The size of action and state spaces.

<!-- TicTacToe is easy -->
In TicTacToe, there are at most 9 possible actions and in the realm of $10^3$ possible action-state pairs.
The agent can learn to play TicTacToe by Q-learning, a model-free reinforcement learning approach to learn the value of an action in a particular state.
This basically enumerates the state-action pairs and iteratively updates the values, while playing more and more games.

<!-- Go is different -->
In Go there are $\sim 10^{170}$ possible states.  
We can't enumerate the states.
The agent was trained to play Go using deep reinforcement learning.
Learning a neural network for predicting the value of a position.
And combinining another neural network and a search algorithm (Monte Carlo tree search) for policy decisions.

<!-- Learning a policy -->
<!--
There are many different approaches to learn a policy.

* If the space of state and action are small, you can describe the policies with tables.
* REINFORCE, a Monte-Carlo policy gradient method. Parameterized, differentiable policy function that is learned via gradient.
* Actor Critic Algorithm:
-->

This flexibility in what to learn is both curse and blessing.
A blessing, because it gives us flexibility to adapt the reinforcement learning approach to different tasks.
A curse, because it's not always clear how to appraoch a reinforcement learning task.

CONTINUE HERE

### Compare to Other Mindset

<!-- ML -->
If you take together supervised and unsupervised learning, just by the names it seems like they would make up completely the machine learning mindset.
But reinforcement learning is a third sub- mindset of machine learning.
Reinforcement learning is special because of the treatment of time and the goal-oriented actions.

<!-- RL versus supervised -->
Is reinforcement not learning not just like supervised learning?
In supervised learning we have the labels, in reinforcement learning we have the rewards.
But they are quite different!
The labels in supervised learning appear for every training instance, telling you what the right answer would have been.
Imagine you want to use supervised learning to play Go.
You would need for every move a label what a human would have decided in that case.
So we would build a classifier: Predict the next move.
If it was the same as a human does, the classification was correct, else it was wrong.
This would lead to, at best, human performance of the game.
Also, it completely misses the aspect of exploration, thinking ahead, and working with rewards.
It would rob the playing algorithm of all "creative freedom" to develop a policy that maximizes the reward.
The reward just tells us whether a sequence of actions was good or bad, but can't tell for individual action how meaningful they were.
Rewards are usually delayed, in supervised learning the labels are immediate.
Think of a manager leading a team of programmers.
One manager looks at every ticket and checks whether the job done by the programmer was fulfilled.
He gives feedback for every ticket:
Either it was correctly done, or the manager shows them what the solution would have been.
The other managers defines what the goal is (for example, make the app load under 1 second).
How the team gets there doesn't matter to the manager, she only gives the reward.
The first is obviously the supervised learning analogy, the second manager the reinforcement learning.
From the perspective of reinforcement learning, supervised learning seems so short-sighted, so narrow.
Supervised learning only considers parts of the problem, without connecting actions.
Reinforcement learning is a much more holistic approach, connecting all interactions.


<!-- RL versus unsupervised learning -->
Reinforcement learning is also not unsupervised learning.
The reward is a sparse but strong signal for what to learn.
Unsupervised learning does not have such a unique signal.
Not every reinforcement learning problem has such a clear reward, sometimes they have to be designed.
That brings the two mindsets a bit closer.
But still, they are very different for other reasons:
Unsupervised learning lacks all these ideas of interacting with an environment, delayed rewards and so on. 
In a way, reinforcement learning is more similar to supervised learning.
That's because the rewards resembles a ground truth, even if it works differently from supervised learning.
Putting things together, is there maybe something like unsupervised reinforcement learning?
Indeed, there is.
And the idea is that the reward is not extrinsic, but rather intrinsic.
Similar to clustering: Here we decide on a criterion for what an interesting cluster would look like, without knowing whether the resulting grouping is "true".

<!-- RL versus statistical modeling -->
Reinforcement learning takes a good scoop from statistical modeling.
How we talk about many concepts in reinforcement learning is in statistical terms.
We talk about probablities for actions, Markov decision processes and so on.
The mindset, however, is very different.
And it boils down to reinforcement learning being a machine learning mindset.
How the policy is learned and so on is not as important as getting the job done.
The statistical modeling mindset would be all about modeling variables explictly, relating them to each other, ...
Again, statistics here is the language with which we describe reinforcement learning, but the mindsets are different.
There is an interesting link to [causal inference](#causal-inference):
Due to reinforcement learning time-dependency, and the reward and so on, the actions have to be causal.
Let's say that in StarCraft the winning player often has the most units.
But if an agent would build a lot of worthless units, it would not win the game.
While the mere number of units is correlated with winning, it's not strictly causal.
Having lots of resources and building the right units is causal for winning.
An agent would not learn a policy that proposes non-causal actions, simply because they will not lead to a reward.
However, the agent can learn to rely on non-causal observations, which makes it vulnerable.

<!-- RL in general -->
Reinforcement learning has links to psychology and neuroscience.
It draws from machien learning, but also operations research, control theory, statistics and optimization.
Reinforcement learning is the most dynamic setup.




The connection between deep learning and reinforcement learning is a bit more special, so let's go deeper here (haha):



## Deep Reinforcement Learning

Deep learning and reinforcement learning: A fantastic fusion of approaches.
Learning a policy can be quite challenging.
The input state might be visual, so it's only pixels.
Also the action space might be large.
Fortunately, deep learning can handle such complex and large data!

TODO: Check about reinforcement learning
Deep reinforcement learning, as it is called, allows to learn a parameterized policy.
The idea of deep reinforcement learning is to replace some components in reinforcement learning with deep neural networks.
So the policy is learned by a deep neural network, and the weights are learnable.
This allows to use, for example, the entire screen of a game to serve as input to the policy.
Deep Reinforcement learning is the reason why reinforcement learning has become so hyped.
Basically like anything that deep learning touches becomes a hype topic, but that's another story.

A successful example of deep reinforcement learning is Alpha Zero.
Alpha Zero is a reinforcement learning algorithm that can play Go.
It's predecessor, Alpha Go, has bested the Go world champion Lee Sedol.
While Alpha Go was a mixture of many approaches, Alpha Zero is a pure deep reinforcement learning approach.
By the way, it can also be trained to play other games such as chess or other fully observable games.
Alpha Zero only needs the rules of the game.
It's then trained with self-play, basically playing against itself.
This happens many many times, and with each game, the gameplay is improved.
AlphaZero features two deep neural networks: a value network and a policy network.
The value network accepts the game state as input and outputs a single number, the value of that state, something between -1 and 1.
A -1 would be a certain loss, a +1 a certain win, everything inbetween is less certain.
So this value network can now be used to evaluate how good a certain position would be.
And this can also be used to weight how good certain actions would be, by enumerating the possible actions and asking the value network how good the new positions are.
The value network can be trained by a huge list of state + outcome (who won).
This data set comes from self play.
It's very much like supervised learning, but it's only a part of the entire AlphaZero algorithm. 
It's not about acting, but rather similar to image classification.
The value network gives us a sense of what a good and what a bad board position is.
The policy network also has as input the game state, but it outputs a bunch of probabilities:
For each action, the probability to follow that action, also called the "prior policy".
It's called prior policy since this action is not followed automatically.
The policy network works in tandem with a Monte Carlo tree search algorithm.
The Monte Carlo tree search connects the policy with the value of the board, and expands simulations of next steps in the game.
The policy network and the Monte Carlo tree search are in a more complex interaction with each other, which would go to deep at this point.
The training of the policy network is intertwined / looped with the tree search.


## Impressive to Look At

With all the hype, it's quite surprising how little real world applications there are in practice.
The reason is that reinforcement learning is difficult to get right.
Training can be quite unstable.
And the most difficult is, that, in order to get in enough training, it's almost impossible to train it in the real world.
This means that it's either restricted to simulations or the application itself is completely digital, like a game.
And these are the prime starting points: games and simulations.
That's also why the most impressive headlines were games beating human players.
But if you want to train a robotic arm to grap an item and put it into another spot.
It's harder.
You first need a simulation and train the reinforcement learning agent in a simulation.
But it's also difficult to make the transfer from simulation to reality.
From the simulated robotic arm to the physical one.

## Strengths

- For many problems reinforcement learning is the only viable approach.
- Reinforcement learning is dynamic. To be honest, many, many problems have such a dynamic, they are time-dependent, they require a holistic interaction approach. And yet, we use other mindsets, either because it's simpler, or because of historic reasons.
- This dynamic involves interactions -- the idea that the actions of the agent **changes** the environment. In other mindsets, the model is a mere "observer", which often is a false simplification. For example, the predictions of a model can change the environment, and by that the algorithm influences future training data.
- It's also a holistic approach. Not individual actions are judged, but rather the entire decision process. Compare that to supervised learning where every prediction is judged individually.
- Reinforcement learning is a good approach for playing games, robotics, planning and control problems.

## Limitations

- Very often, reinforcement learning, especially deep reinforcement learning, is just the [wrong approach to a problem](https://www.alexirpan.com/2018/02/14/rl-hard.html).
- Reinforcement learning models can be very difficult to train and reproduce:
  - It requires a lot of episodes, because it's rather sample inefficient.
  - Designing a reward function can be difficult.
  - The training can be unstable, and might get stuck in local optima.
- Reinforcement learning models are trained in artificial environments. It's difficult to transfer the models into the physical world.
- Reinforcement learning requires that the task involves some form of "agent".
- Many different ways of learning: model-free versus model-based, learn policy directly, learn value function, learn action value function, ... This can be quite confusing. 



## References

- TODO:  AI Fail paper
- Reinforcement Learning, An introduction [@sutton2018reinforcement]





