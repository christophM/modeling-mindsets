# Reinforcement Learning {#reinforcement-learning}

<!-- 
TODO: checkout https://spinningup.openai.com/en/latest/spinningup/rl_intro.html

-->

* The world is dynamic and so is the reinforcement learning mindset: The "model" is an actor interacting with its environment, collecting rewards.
* Learning is a mix of exploiting knowledge about beneficial actions and exploring new options.
* Reinforcement learning is useful in robotics, games and simulations.
* Together with [supervised](#supervised) and [unsupervised](#unsupervised-ml), reinforcement learning is a [machine learning](#machine-learning) mindset.


<!-- motivation: joke about how static other mindsets are -->
Three machine learners of different minds attend a conference dinner.
The dinner is served in form of a self-service buffet, and the service staff constantly refills the lavish offerings all night long.
One of the dishes is super popular and not available most of the time: Dumplings.
Once dumplings are refilled, within minutes the plate is empty again.
Also the machine learners can't escape the pull of the dumplings.
So they start to strategize, in their own way, how to get more dumplings.
The supervised learner starts gathering data, trying to predict dumpling servings based on time, appearance of other dishes and so on.
The unsupervised learner expands the goal: Why only look at dumplings? It's better to find out in more general what foods there are.
Maybe dumplings are similar to other dishes offered?
Maybe there are some outlier foods that are much better?
The reinforcement learner was gone.
Where did she go?
The other two hadn't noticed her leaving.
Suddenly she comes back, with a full plate of dumplings.
The supervised and unsupervised learners gasp in surprise.
"How did you find all these dumplings," they demand to know.
She started sharing her success story: "Since my policy of just waiting at the buffet for dumplings didn't work out, I had to explore other actions.
I walked outside where one waitress was on here cigarette break.
We had some small talk, and I just ask her about the dumplings.
She basically then gave me some dumplings straight out of the kitchen".
The other two were flabbergasted.
How should they have known that talking to people, interacting with their environment was always an option?


<!-- applications -->
Chess, Go, StarCraft II, Minecraft, various Atari games, ...
All these games have in common that computers can play them too, beating human players by a long shot.
The computer can play those games with the help of reinforcement learning.
Reinforcement learning is not only to play, but also to  
But reinforcement learning is not only for playing.
It can also steer components in complex systems.
For example, reinforcement learning was used to control the [cooling of Googles datacenters](https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/).
Reinforcement learning has been used to control robots, to control self-driving cars and much more.
It's a versatile mindset that can be used in very dynamic situations.

Welcome to the reinforcement learning mindset!

## The Reinforcement Learning Mindset

<!-- reinforcement learning theory in a nutshell -->
- state, actor,  policy
-

<!-- data structure is different -->
- dynamic
- scattered rewards
- interaction
- exploration vs exploitation like no other mindset

<!-- why it's a machine learning mindset -->

- Dynamic mindset
- instead of just inspecting static dataset, an agent "explores" and interacts with an environment
- machine learning mindset
- different from supervised: no labelled input output pairs needed. supervised would need for every agent decision some information how good the decision was.
- reinforcement learning, in contrast, gets the reward in the end. and has to propagate it all the way back to learn
- finding balance between exploration and exploitation, which is unique to this mindset
- exploitation: exploiting current knowledge of how to get a nice reward
- exploration: venture out into uncharterd territory and find out what rewards await there.
- RL is about learning how to act or what to do
- mapping from environment to actions while maximizing a reward
- typical elements in RL: Trial-and-error search and delayed rewards
- RL is a holistic mindset: It considers the entire interaction of agent with environment
- compare that with supervised: supervised only considers parts of the problem without connecting the prediction problem to the larger picture
- for example: a company sends out advertisement to customers. based on how likely the customer is to buy.
- in this example would be more meaningful to interact with the customer, explore and exploit


- What can be an agent?
  - Robots
  - players in games like Go and starcraft
  - but also componentes in larger system
  - like regulation component in cooling system
  - think of a buildings ventilation controller: environment are the sensors about temp, humidity and air quality. action is where and how much to ventilate
  -

## Reinforcement Learning Theory

- goal: agent should find optimal policy to maximize the reward function
- the model of the  world is described with:
  - state of agent and environment ($S$), for each time step
  - time
    - time is a differentiator to the other mindsets
    - there is of course time series forecasting in supervised and in stats, but time is different in RL
  - an agent
  - a set of possible actions $A$ the agent could take. can also change at each time step
  - the action that was taken for each time step
  - a policy
  - the reinforcement learning algorithm
  - a reward, which the agent receives at the very end of the "game", or at intermediate time points
  - value: the expected long-term value (discounted) of a state?
  - value function: maps from state to value.
- agent interacts with environment via actions which changes the state
- the policy is a mapping/function from the states and a possible action to a probability. The probability of the agent to take a certain action given a certain state $P(a_t = a | s_t = s)$.
- RL can be formalized as Markov Decision process: summarize most important info about current state of environment to achieve rewards
- the agent itself has 4 components:
  - policy: how the agent behaves for given situation
  - reward signal:
  - value function: 
  - (optional) environment model
-

```{r rl, fig.cap = "Reinforcement Learning."}
r1 = rectFun(c(0, 0))
r2 = rectFun(c(0, 5)) 

p = ggplot(mapping = aes(x = x, y = y)) +
  geom_path(data = r1) +
  annotate("text", label = "Environment", x = 0, y = 5) + 
  geom_path(data = r2) +
  annotate("text", label = "Agent", x = 0, y = 0) + 
  annotate("curve", x = 1, y = 0, xend = 1, yend = 5, size = 1, curvature = 0.5, arrow = arrow(ends = "first"))  +
  annotate("label", x = 1.7, y = 2.5, label = "Observation, Reward") +
  annotate("curve", x = -1, y = 5, xend = -1, yend = 0, size = 1, curvature = 0.5, arrow = arrow(ends = "first")) +
  annotate("label", x = -1.7, y = 2.5, label = "Policy, Action") +
  scale_x_continuous(limits = c(-2.1, 2.1)) +
  theme_void()

add_cc(p)
```


### Compare to Other Mindset

* Supervised + unsupervised learning sounds like they would cover all of ML, by set theory
* But RL is a third ML paradigm
* Much more Goal-directed and learning from interactions than the other mindsets
* rewards are possibly delayed: in supervised learning training, it's always immediately clear what the performance for a prediction was
* RL also different from unsupervised learning: goal in unsupervised is to detect hidden patterns
* RL does not, like in supervised learning, get "label" if action was correct or not
* Within RL, methods from other mindsets might be used:
  * A sub-problem might be formulated as supervised problem
* Also links with pychology and neuroscience 
* Draws from AI, ML, but also operations research, control theory, statistics and optimization.
* RL more dynamic setup than any other mindset
* but this is also very restricting: only if you can give a RL algorithm access to such environment it can work with it
* Therefore RL can't work with historical data: if you collected data on what recipes the people like  
* Closest is maybe experimental design where the environment is manipulated to collect data
  * Google Cookies were created by experimental design.
  * The design choices were proposed by Bayesian optimization
  * https://blog.google/technology/research/makings-smart-cookie/
* Compare with deep learning
  * Nice fusion between the two
  * Actually the most success of reinforcement leanring came after fusion of the two
  * Called: deep reinforcement learning. Should get an extra section

```{r rl-trajectory, fig.cap = "Trajectory of a reinforcement learning agent through the state space, with a reward at the end."}
p = ggplot() +
  annotate("segment", x = 0, xend = 8, y = 1, yend = 1, arrow = arrow(), lty = 2) +
  annotate("segment", x = 1, xend = 2, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 1, y = 1, label = "State 0") +
  annotate("segment", x = 2, xend = 3, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 2, y = -1, label = "Actor") +
  annotate("segment", x = 3, xend = 4, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 3, y = 1, label = "State 1") +
  annotate("segment", x = 4, xend = 5, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 4, y = -1, label = "Actor") +
  annotate("segment", x = 5, xend = 6, y = 1, yend = -0.8, arrow = arrow()) +
  annotate("label", x = 5, y = 1, label = "State 3") +
  annotate("segment", x = 6, xend = 7, y = -1, yend = 0.8, arrow = arrow()) +
  annotate("label", x = 6, y = -1, label = "Actor") +
  annotate("segment", x = 7, xend = 7, y = 1, yend = 1.4) +
  annotate("label", x = 7, y = 1, label = "State 4") +
  annotate("label", x = 7, y = 1.4, label = "Reward") +
  coord_fixed() +
  theme_void() +
  scale_y_continuous(limits = c(-1.5, 1.5))

add_cc(p)

```


### Policies

- Policy is rule used for agent
- policy tells what action to take
- policy can be deterministic or stochastic
- policy: $a_t = \mu(s_t)$ or $a_t = \pi(\cdot | s_t)$
- you can see policy as the "brain" of the agent


### Trajectories

- Trajectory is a sequence of states and actions
- $\tau = (s_0, a_0, s_1, a_1, \ldots)$
- First state can be a random sample
- Random sample from start-state distribution
- But this depends on the specific simulation scenario
- If start is expected to be the same, can be just the same start anyways.
-

### Value Functions

- Value function tells us the value of a state
- Value is expected return for a specific state, or state-action pair
- Four way to define value functions:
  - on-policy value function. only depends on the state, and assumes that always acting based on policy
  - on-policy action-value function: taking some action a in state s, but then always following policy
  - optimal value function: expected return for starting in s and always following the optimal policy
  - optimal action-value function: expected return for starting in state s, taking arbitrary action a and the always following the optimal policy.
- without rewards, there is no value
- just a way to encode the reward into each state
- while rewards are provided by the environment, values have to be estimated or calculated
-
-


## Deep Reinforcement Learning

- policy is parameterized
- the connection between the state and the action is parameterized
- more specifically, a neural network is doing the decisions
- weights of the neural network are learnable
- so the policy can be learned by adjusting the neural network weights
-

## The Bellman Equations

TODO: check reinforcement learning book p. 79 
$$V(s) = \arg\max_{a}  R(s, a) + \gamma \sum_{S'} P(s,a,s')V(s') $$

- Value of state s
- defined recursively, based on the value of the next state
- think of the last step where the value is clear when you get the reward, like win or lose a game
-

## Misc

- in general, there are many different types of reinforcement learning algorithms (next to deep reinforcement learning): Q-learning, SARSA, ...
- we talked about RL assuming full observability of all states.
  - but you might also have partial observability only. like fog of war in games, or the fact that you don't even know what's happening in the room next to you.
  - state is always all the information about the environment
  - observation might be partial. that's what the agent gets.
- Model-free vs model-based: Model-based means that the agent either learns a model of the environment or it has access to a model of it.
- Quote RL book: Q-learning: By contrast, Q-learning methods only indirectly optimize for agent performance, by training Q_{\theta} to satisfy a self-consistency equation. There are many failure modes for this kind of learning, so it tends to be less stable.

## Model-free vs model-based

- model-based: has environment model that can be used for planning
- model-free: no model of the world
- 

## Impressive to Look At


- Works well in games and simulations
- fun to see in action:
  - Go in action
  - watch starcraft matches
  - see little figures run around in some world, being adorable and such
- these applications make it seem as though we a just two steps away from AGI
- results are impressive
- but: in very particular enviroment
- hard to carry into the real world
- or hard to translate into different environment
-


## AI fails

- I always looked for opportunities to cite this paper
- mischieveous AI
- malicious compliance
- just too funny 
- TODO: Cite this paper where the reward fails
- example: jumping really high, and instead it learns to make a salto
-

## Strengths

- dynamic
- with interactions in mind
- often more realistic to think of a model interacting with an environemnt
- especially if understanding policy better
- works well in games and simulations

## Limitations

- requires lots of simulations or runs. since so many are required, usually have to resort to simulation
- requires that the task of interest can be sufficiently simulated in some way
- also the transfer from simulation to real world is tricky
- always needs an "agent": can only be used for those types of problems
- I think there is a reason why we are not seeing RL to be used very often.
- The big wins and big headlines are because of games. In those cases, the code could be directly adapted for simulation. And then when applied "in real life" the game was exactly the same environment
- Training can be difficult
- Difficult to define the reward correctly. RL algo might finish task in unexpected ways
- Reproducibility is challenging in RL and often Training fails
- RL doesn't really work yet: https://www.alexirpan.com/2018/02/14/rl-hard.html


## References

- AI Fail paper
- Reinforcement Learning, An introduction [@sutton2018reinforcement]





