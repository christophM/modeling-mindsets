# Bayesian Inference {#bayesian-inference}

<!-- CONTENT TO ADD

- Just one level: The parameter random variables themselves don't have a further distribution
- Bayes Factor: A way to compare models/hypotheses. Also a way to do testing in the Bayesian mindset
- checkout book: https://press-files.anu.edu.au/downloads/press/n1652/pdf/book.pdf
- minimizing some risk function on the posterior delivers point estimates
- whether mean, median or mode or whatever are wanted, implicitly depends on the risk function
- the decision theory + loss function is also a bridge to machine learnign: what if we only look at best possible decision instead of relying on random variables and DGP thoughts?

-->

* Probability is a degree of belief and learning from data means updating the belief.
* Model parameters are random variables with prior and posterior distributions.
* A [statistical modeling mindset](#statistical-modeling) with [frequentism](#frequentist-inference) and [likelihoodism](#likelihoodism) as alternatives.

<!-- relation to others -->
If you haven't read the chapter on [Statistical Modeling](#statistical-modeling), I recommend that you do so first, since Bayesian inference is easier to understand if you have a good understanding of statistical inference.
Bayesian inference is based on probability distributions, interpretation of parameters, and learning from data through the likelihood.
The big twist: Distribution parameters are random variables as well!
Random variables that have a prior distribution.
As such they have a prior distribution.
Prior means before encountering the data.
Learning about the parameters means updating the prior probabilities to the posterior probabilities.

<!-- summary learning with Bayesian inference -->
In Bayesian statistics, probability can be interpreted as the plausibility of an event or our belief about it.
That's different from the more objective framing of probability in [frequentist inference](#frequentist-inference).
Bayesian inference doesn't require to imagine repeated experiments.
You could even apply Bayesian inference to just one data point.
Which you couldn't do with frequentist inference.
We can work with a Bayesian model even without data, by just using the prior distributions of the parameters.[^prior-predictive]
Your model would even work without data, by just using the prior distributions.


<!-- Example of knowledge update in real life -->
<!-- 
Imagine you are about to buy something from a snack machine.
You are a big fan of snack machines in general.
What nobody knows about you: You rate vending machines.
Most snack vending machines get a rating between 6 to 9 out of 10.
Lacking experience with the machine in front of you, the machine's rating will be in this range as well.
Considering it's directly on your daily commute, and it has your favorite snack, you give it a rating of 9/10.
That's before the big betrayals happen. 
One day, a bag of chips got stuck between the tray and the windoe pane.
The day after, the machine refused to give you any change.
Because of these betrayals you had to update your rating to 5/10.
-->


## Bayes Theorem

Bayesians wanna know the distribution of their model parameters given the data: $P(\theta | X)$.
It's a weird inverse probability, because the data-generating process produces the data the other way around:
The data are produced given the parameters and follow a probability distribution $P(X | \theta)$.

<!-- Bayes' theorem has many applications -->
So Bayesians use a trick that earned them their name: They applied the Bayes' theorem.
With the Bayes' theorem, the conditional probability statement can be inverted:

$$\underbrace{P(\theta|X)}_{\text{posterior}} = \frac{\overbrace{P(X | \theta)}^{\text{likelihood}} \cdot  \overbrace{P(\theta)}^{\text{prior}}}{\underbrace{P(X)}_{\text{evidence}}}$$

$P(\theta)$, also called the prior, is the probability distribution of $\theta$ before we have collected any data.
The probability distribution gets updated by multiplying the prior with the likelihood of the data $P(X | \theta)$.
This product is scaled by the probability of the data $P(X)$, also called evidence.
The result is the posterior probability distribution, an updated belief about our parameters $\theta$.

The Bayes' theorem is a generally useful equation for working with probabilities, but we focus on its use for Bayesian inference.
It's not just a simple rearrangement of probabilities, but a powerful mental model: Bayesian updating.
Remember that the data likelihood is the product of the likelihoods for each data point:
$$P(\theta, X) = \prod_{i=1}^n P(\theta, X^{(i)})$.

Here, $X^{(i)}$ is the vector of random variables of the i-th outcome, for example in a drug trial the variables $X^{(i)} could belong to the i-th (not yet observed) patient and contain pain level, blood pressure and iron levels.
Putting that into the Bayes' theorem, we can see how it's related to updating ones belief when new data comes in.
For simplicity, I removed the evidence $P(X)$ which only serves as a normalization so that we may interpret the results as probability:

\begin{eqnarray}
P(\theta|X) & \propto  & P(X | \theta) \cdot  P(\theta) \\ 
            & =        & P(\theta) \cdot \prod_{i=1}^n P(\theta, X^{i}) \\
            & =        & P(\theta) \cdot P_1 \cdot \ldots \cdot P_p,
\end{eqnarray}

where $P_j = P(\theta, X_j)$.
Even with just one data point, we can update our belief about the parameters!
And each time this posterior then becomes -- kind of -- the prior for the next update.
The posterior distribution is the product of prior and likelihood (Figure \@ref(fig:bayesian)).

```{r bayesian, fig.cap = "The posterior distribution (right) is the scaled product of prior times likelihood: prior $\\times$ likelihood $\\propto$ posterior", fig.width = 10, fig.height = 3}
library(ggplot2)
library(tidyr)

x = seq(from = -1, to = 8, length.out = 500)
prior_d = approxfun(x, dgamma(x, shape = 1.4, rate = 0.8))
likelihood_d = approxfun(x, dnorm(x, mean = 3.5, sd = 1.5))

posterior_d = function(x) {
  prior_d(x) * likelihood_d(x) 
}

auc = sum(diff(x)[1] * posterior_d(x))
posterior = function(x) {posterior_d(x) / auc}

dat = data.frame(prior = prior_d(x), likelihood = likelihood_d(x), posterior = posterior(x), x = x)
densities = c("prior", "likelihood", "posterior")
dat = pivot_longer(dat, cols = all_of(densities))
dat$name = factor(dat$name, levels = densities)

ggplot(dat, aes(x = x, y = value)) +
  geom_area(size = 1.1, fill = "lightgrey") +
  geom_line() +
  #coord_fixed(ratio = 20) +
  facet_grid(. ~ name) +
  theme_void() + 
 theme(axis.text.x = element_text(),
      strip.text.x = element_text(size = 18))
```

Next, let's dive into the individual terms of the Bayes' theorem, so that we can update our own beliefs about Bayesian inference.

## Prior Probability 

<!-- prerequisites -->
Bayesians assume that parameters that we find in the likelihood have a prior probability distribution $P(\theta)$.
Let's say we randomly pick a person and want to know how many hours per day they usually work.
The number of daily working hours, the random variable of interest, follows a probability distribution.
For example, the number of work hours might follow a Gaussian distribution.
Bayesians assume that mean and variance of this Gaussian distribution are random variables.

<!-- role of the prior -->
Is that right?
We have to know the distribution of this parameter *before* observing any data?
Yes it is.
It's a consequence of saying that parameters are random variables, and a requirement to work with the Bayes' theorem.
But how could we know anything about the parameters before seeing the data?

### Picking a Prior {-}

<!-- parameter space reasons -->
The first consideration when choosing a prior is the *space* that the parameter lives in.
Is the parameter the mean of a continuous distribution?
Then we need a continuous distribution for the parameter as well, for example Gaussian.
Maybe the mean parameter has to be positive.
Then we should pick a prior distribution that only contains positive values, for example the Gamma distribution.
<!-- expert knowledge reasons -->
Furthermore, expert knowledge can be used to inform the choice of prior.
Maybe we know from other experiments that the mean parameter should be around 1.
So we could assume a Normal distribution for $\theta$: $\theta \sim N(1, 1)$.
In the case that the data follow a Binomial distribution, the Beta distribution is a good prior (see Figure \@ref(fig:priors)).
Depending on the modelers belief about the success probability parameter $p$ of the Binomial distribution, they might choose different parameterizations of the Beta distribution.
Maybe the modeler believes that the parameter must be symmetrically distributed around 0.5.
Or maybe the parameter is more drawn to 0.25?
Another Beta prior might put emphasis on $p$ being 1.
<!-- intuition for alpha = beta = 0.5: https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than -->
But it's also possible to have a prior that puts most probability on 0 and 1 symmetrically.


<!-- uninformative and conjugate priors -->
Without expert knowledge about the parameter, the modeler can use "uninformative" or "objective" priors [@yang1996catalog].
Uninformative priors often yield results similar to results from frequentist inference (for example for confidence/credible intervals). <!-- citation needed -->
Another factor that influences the choice of prior is mathematical convenience.
Conjugate priors are convenient choices.
Conjugate priors remain in the same family of distributions when multiplied with the right likelihood functions. 
A Beta prior distribution multiplied by a Bernoulli likelihood yields, again, a Beta posterior distribution. 

<!-- criticizing the prior-->
While there are all these various strategies for choosing a prior, even "objective" ones, the choice remains subjective.
And this subjective choice of the prior is why many frequentist statisticians reject Bayesian inference. <!-- citation needed -->
<!-- advantages of priors -->
While the prior can be seen as a bug, it can also be seen as a feature.
Thanks to the prior, Bayesian modeling is very flexible.
The prior can be used to constraint and regularize model parameters, especially when data are scarce;
the prior can encode results from other experiments and expert knowledge;
the prior enables natural handling of measurement errors and missing data.

<!-- joint prior? -->
<!-- 
But that sounds overly complex, do we really have to specify all the priors?
There can be many parameters involved in a statistical model.
And in the Bayesian world, every parameter gets a prior.
These priors could even have a complex joint distribution with correlations between the parameters.
But usually, Bayesians specify the prior probabilities independent of each other.
-->

To get the posterior distribution of the parameters -- the ultimate goal in Bayesian inference -- we have to update the prior using data, or rather: the likelihood function.

```{r priors, fig.cap = "Various prior distributions for the success probability p in a Binomial ditribution. Priors here are Beta distributions with shape parameters $\\alpha$ and $\\beta$.", fig.width = 10, fig.height = 2.5}
library("ggplot2")
n = 100000
x = seq(from = 0, to = 1, length.out = 100)
#datbeta1 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 5), dist = Encoding("\u03B1 = 2, \u03B2 = 5"))
datbeta1 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 5), dist = "list(alpha==2,beta==5)")
# intuition for alpha = beta = 0.5: https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than
datbeta2 = data.frame(x = x, y = dbeta(x, shape1 = 0.5, shape2 = 0.5), dist = "list(alpha==0.5,beta==0.5)")
datbeta3 = data.frame(x = x, y = dbeta(x, shape1 = 5, shape2 = 1), dist = "list(alpha==5,beta==1)")
datbeta4 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 2), dist = "list(alpha==2,beta==2)")
 
datbeta = rbind(datbeta1, datbeta2, datbeta3, datbeta4)

ggplot(datbeta, aes(x = x, y = y)) +
geom_area(fill = "lightgrey") +
geom_line(size = 1) +
facet_wrap(. ~ dist, ncol = 4, labeller = label_parsed) + 
scale_x_continuous(breaks = c(0, 0.5, 1), labels = c("0", "0.5", "1")) +
theme_void() +
theme(axis.text.x = element_text(),
      strip.text.x = element_text(size = 18))
```


## Likelihood

<!-- likelihood recap -->
If you have read the chapter [Statistical Modeling](#statistical-modeling), you should be familiar with the likelihood function $P(\theta | X)$.
The likelihood is the probability function of the data, but with a twist:
We switch the role of data and parameters.
The probability function of the data takes the values of the data as input and the parameters are assumed to be given. 
The likelihood function is a function of the parameters:  The model parameters are the variable input and the data are assumed to be fixed.

<!-- where does the likelihood come from -->
To get to the likelihood, we have to make assumptions about how our data is distributed.
So we assume parameterized distributions for our data and turn it into a maximization problem: Finding the right parameter that maximize the likelihood.
That's true at least for [frequentism](#frequentism) and [likelihoodism](#likelihoodism).
But for Bayesians, the likelihood is just one part of the equation.

With all the frequentist inference versus Bayesian inference comparisons it's easy to forget that both approaches are quite similar.
Comparisons of both mindsets are often focused on the (lack of) prior distribution.
And these comparisons overlook the use of the likelihood that is common to both frequentist and Bayesian inference.
Especially in cases with a lot of data, both approaches produce similar results.
That's because the more data are available for the model, the less impact the prior has on the Bayesian results.

Let's turn to the last part of the equation: the evidence $P(X)$.
As we will see, it doesn't matter that much in practice.

## Evidence

The evidence is the marginalized probability of our data.
Marginalized means that the probability of the data is integrated over all possible parameter values: $P(X) = \int_{\Theta} P(X|\theta) P(\theta) d\theta$, where $\Theta$ are all possible parameter values.
Due to this marginalization, $P(X)$ is no longer a function of the parameters $\theta$.
That means in terms of maximizing the posterior probability, $P(X)$ is just a constant factor.
Constant factors don't change where the maximum is, so in search of the maximum, the evidence $P(X)$ can be ignored.
That's why the posterior probability is often expressed proportional to the numerator: 

$$\underbrace{P(\theta|D)}_{\text{posterior}} \propto \overbrace{P(D | \theta)}^{\text{likelihood}} \cdot  \underbrace{P(\theta)}_{\text{prior}}$$

Just one problem: When throwing away $P(X)$ the posterior probability is not a probability at all, because it doesn't integrate to 1, but instead to $P(X)$.
So, how do we finally estimate the posterior probability?

## Posterior Probability Estimation

<!-- Goal of Bayesian -->
The goal of the Bayesian modelers is to estimate the posterior distributions of the parameters.
Once they have that, they can interpret those distributions, make predictions with the model, and draw conclusions.

<!-- Problem: Posterior estimation -->
But how is the posterior estimated?
Ideally, you'd want to have a closed form expression to calculate the posterior.
That's possible in simple cases, for example when conjugate priors are used.
For many Bayesian models it's impossible to get a closed form expression of the posterior.
The main problem is that $P(X)$ might be infeasible to compute.

### Sample From the Posterior with MCMC {-}

<!-- Don't solve exactly, but sample -->
Good news: We don't have to compute the posterior probability.
We can sample from it.
Approaches such as Markov Chain Monte Carlo (MCMC) and derivations of it are used to generate samples from the posterior distribution.

<!-- shortest MCMC explanation -->
The rough idea of MCMC and similar approaches is the following:

* Start with some initial values for the parameters $\theta$.
* Repeat the following steps until a stopping criterion is reached (like pre-determined number of samples):
  1. Propose new parameter values. Proposals are based on a proposal function that takes as input the previous parameters.
  1. Accept or reject the new values, based on an acceptance function. The acceptance function depends on the prior and the likelihood, but not on the evidence.
  1. If parameters are accepted, continue with these new values.

This process produces a "chain" of samples from the posterior distribution.
It can be repeated to produce multiple chains.

MCMC has many variants such as Gibbs sampling and the Metropolis-Hastings algorithm.
Each variant differs in proposal and acceptance functions or other algorithmic steps.

MCMC produces a random walk through the posterior distribution of the parameters.
Regions where the parameters have a high probability also have a higher probability of being visited.
The samples can be seen as samples from the posterior.
But first, some cleaning up needs to happen:
Since MCMC has to start somewhere, it's possible that the first samples are from parameter regions with a low probability.
So the first 1000 or so samples are "burned" meaning they are not used. 
Another issue is the autocorrelation within the chain:
Samples that happen after each other are correlated, since the proposal function usually proposes new parameters close to the values before.
So the chain is sampled at different places, ensuring that there are enough MCMC steps between two samples so that they are independent.

<!-- time and other resources -->
MCMC is complex and time-intensive.
Fortunately, software can hide this complexity and after specifying the model, the MCMC program just starts churning through that posterior.
But it can take time.
More time than a frequentist model would.
A shorter alternative is variational inference.[@blei2017variational]
But while MCMC delivers approximately exact estimates of the posterior probability, variational inference is more of a heuristic.

<!-- TODO: visualize MCMC -->
Frequentists get parameter estimates.
Bayesians get ... samples from the posterior distribution?
That's not the end of the story.
Two more steps are required to get from posterior samples to insights: turning the samples into a distribution and (optional) summarizing the distribution.

## Summarizing the Posterior Samples

<!-- from samples to distribution -->
The samples of the posterior can be visualized with a histogram.
Or, throw a density estimator on top of it, and get a smoother looking distribution.
The modeler gets most information out of the posterior by looking at the entire distribution.

<!-- tables and numbers -->
But for some reason, people love numbers and tables.
What insights can be drawn from the posterior?
It comes down to simplifying or summarizing the posterior.
Summaries can be in the form of points or intervals.
Intervals can be defined via pre-defined boundaries or probability mass.

* Example of a point estimate: The parameter value with the highest posterior probability.
* Example of an interval with pre-defined boundaries: The interval from 10 to infinity gives the posterior probability that the parameter is larger than 10. 
* Example of an interval with pre-defined probability mass: The shortest interval that contains 50% of the posterior probability mass. Or the interval that goes from the 2.5% to the 97.5% quantile (95% credible interval). 


```{r posterior, fig.cap = "Describing the posterior distribution with either intervals, for example the 95% credibility interval, or point estimates, for example the maximum a posteriori estimation (MAP)."}
library("ggplot2")
set.seed(2)
n = 10000
datbeta = data.frame(x = c(rbeta(n/2, shape1 = 2, shape2 = 5), rbeta(n, shape1 = 5, shape2 = 2)))

qs = cumsum(datbeta$x) / sum(datbeta$x)

lower = max(sort(datbeta$x)[qs < 0.025])
upper = min(sort(datbeta$x)[qs > 0.975])

dd = density(datbeta$x)
dd = data.frame(x = dd$x, dens = dd$y)
dd = dd[(dd$x > lower) & (dd$x < upper), ]

f = approxfun(density(datbeta$x))
datbeta$f = f(datbeta$x)
map = datbeta$x[datbeta$f == max(datbeta$f)]

ggplot(datbeta, aes(x = x)) +
#geom_density(fill = "grey", size = 2) + geom_area(aes(x = x, y = dens), data = dd, fill = NA) +
annotate("text", x = map, y = 1.9, label = "MAP", size = 6, vjust = 1) +
annotate("segment", x = map,  y = 1.8, xend = map, yend = 1.68, arrow = arrow(), size = 2) + 
annotate("segment", x = map, y = 0, xend = map, yend = f(map), size = 1, lty = 3) + 
geom_density(size = 1.5, trim = FALSE, fill = "grey", alpha = 0.5)  +
annotate("segment", x = lower + 0.01, y = 0.1, xend = upper - 0.01, yend = 0.1, arrow = arrow(ends = "both"), size = 1.5) + 
annotate("label", x = mean(c(lower, upper)), y = 0.1, label = "95% Credible Interval", size = 6) + 
annotate("segment", x = lower, y = 0, xend = lower, yend = f(lower), size = 1, lty = 2) + 
annotate("segment", x = upper, y = 0, xend = upper, yend = f(upper), size = 1, lty = 2) + 
theme_void()
```


<!-- Credible interval In frequentist statistics, results of estimators are often described with confidence intervals.
That's some uncertainty quantifycation telling us how sure we are about our estimation.
But it's different with Bayesian inference.
Since we get posterior probabilities for our parameters, and from the we can derive credibility or credible intervals.
For example, a 95\% credibility interval contains 95\% of the mass of our parameter.
With 95\% probability, the parameter falls within that interval.
Not in the sense that there is some fixed value for the parameter.
But it's a random variable that can take on different values.
Technically, they work the same as confidence intervals in frequentist statistics.
But they have a different philosophical interpretation. 
-->



## From Model to Real World

<!-- TODO: Improve this part -->

<!-- statistical model -->
Bayesians build [statistical models](#statistical-modeling):
They try to approximate the data-generating process with a construct of  probability distributions of random variables.
These distributions are parameterized, and Bayesians say that these parameters are random variable themselves.
Also, Bayesians think of probability as degrees of belief.
The posterior represents an updated belief about the parameters that represent information about the real world.
That's different from how frequentists learn about the world.
Frequentists assume that there are some true parameter values out there in the world.
And the job of the frequentist is to uncover them.
The Bayesian, in contrast, expresses degrees of belief about the parameters of interest.
Learning from data means updating that belief.
It's not that the world is random.
But our information about the world is incomplete.

## Simulate to Predict

<!-- Natural inclusion of uncertainty in prediction -->
Parameters are random variables.
At first glance, that's a problem if we want to make predictions with the model.
But it's actually a gift, not a problem.
In the Bayesian mindset, predictions are also random variables, and not just point estimates.
The distribution of the prediction reflects the uncertainty of the parameters.
Not getting a definite answer (prediction) seems inconvenient at first.
But it's much more honest and informative than a point estimate.
The modeler in the Bayesian mindset is encouraged to view prediction along with it's uncertainty.

<!-- How to predict -->
To predict, the Bayesian has to simulate.
Prediction means marginalizing the distribution of a data point's predicted value for the new data point $X_{new}$ with respect to the distribution of parameters: 

$$P(X_{new} | X)_{\Theta} = \int P(X_{new} | X, \theta) \cdot P(\theta | X) d\theta$$

Simulation means drawing values for the parameters from the posterior distribution.
For each draw of parameters, the modeler can insert those parameters into the probability distribution of the data, and then draw the prediction.
Repeat this many times, and the result is the posterior predictive density.

<!-- 
## Some Examples of Bayesian inference

What would a linear regression model look like in the Bayesian modeling approach?

For that we have to assume that $Y$ follows a Gaussian distribution given $X$.
The Bayesian mindset tells us that we are interested in the distribution of the parameters of this model.
In this model, the relationship between the mean of the distribution of $Y$ and the features are represented by the model coefficients $\mathbf{\beta}$.

So we would assume that y folllows a Normal distribution:

$$y \sim N(\mathbf{\beta}^TX, \sigma^2 I)$$

Since we are in the Bayesian mindset, we also assume a distribution for the parameters $\mathbf{\beta}$ and $\sigma$.

This you can see by looking at how we would get the posterior probability:

$$P(\beta|Y,X) = \frac{P(y|\beta,X) P(\beta|X)}{P(y|X)}$$

* $P(y|X)$ is the data evidence
* $P(\beta|X)$ is the prior probability for $\beta$
* $P(y|\beta, X)$ is the likelihood function
* $P(\beta|y,X)$ is the posteriori probability of the model parameters

We carry arround the $X$ in the formulas the entire time in the conditioning statement.
This is allowed.


TODO: List some real applications

TODO: List some models

## Justifications for Bayesian Inference

https://en.wikipedia.org/wiki/Bayesian_probability#Justification_of_Bayesian_probabilities


## Empirical Bayes

Combination of frequentism and Bayesianism

https://efron.ckirby.su.domains//papers/2005BayesFreqSci.pdf

-->

<!--  
## How Different is Frequentism Really?

[Frequentist Inference](#frequentist-inference) is the mindset to which Bayesian inference is compared the most often. The debate of which mindset is better has been going on for a long time and is still going on.
Throughout this chapter, we have seen that both mindsets have different ideas of what a probability is and whether to use prior probabilities.
These decisions have far reaching consequences on how to do modeling and how to interpret the models.
But with all the adversary, it's easy to overlook how close mindsets are, especially compared with many other mindset that you will encounter in this book.
Both mindsets are statistical modeling mindsets: They both rely on probability distributions.
And they both assume that interpreting those distributions is a valid way to see the world.
Also, everyone puts so much emphasis on the prior probability.
But, what ways much heavier, is the likelihood that is used by both frequentism and Bayesianism.
In cases where we have very few data points, the prior has a high impact on the result.
And for those data-poor cases frequentist and Bayesian inference will differ the most.
But in all other cases, where there is enough data, a modeler will often get similar results with frequentist and Bayesian inference.
And ultimately, they will come to similar conclusions.

Frequentist confidence interval can be interpreted as a special case of credibility interval (citation needed) with an uninformative prior.
Really, all frequentist method can be expressed as special cases of Bayesian models.

-->

## Strengths

* Bayesianism allows the use of prior information such as expert knowledge.
* Bayesian inference inherits all advantages of [statistical models](#statistical-modeling).
* Bayesianism offers an expressive language to build models that naturally propagate uncertainty.
* Bayesian models handle hierarchical data, measurement error and missing data naturally.
* A more general benefit: Bayesianism is an interesting mental model for how we humans update our own beliefs about the world.
* Arguably, the Bayesian interpretation of probability is more intuitive than the frequentist interpretation: When practitioners misinterpret frequentist confidence intervals, it's often because they interpret them as credible intervals.
* Decoupling of inference and decision. You can first learn those posteriors, and then apply decision theory for making decisions. In frequentist inference, it's common to shape the entire modeling process around the decision (hypothesis tests).

## Limitations

* The choice of prior distributions is subjective.
* Bayesian methods are mathematically demanding and computationally expensive.
* If used solely for decisions, all the additional information about the entire posterior might seem like unnecessary overhead. 
* No causal interpretations are allowed, just associations are modeled.

[^prior-predictive]: This is called the prior predictive simulation and serves to check whether the chosen priors produce reasonable data. The modeler simulates by first sampling parameters from the priors and using those parameters to generate data. Repeated many times, this produces a distribution of data.
