# Bayesian Inference {#bayesian-inference}

<!-- CONTENT TO ADD

- Just one level: The parameter random variables themselves don't have a further distribution
- Bayes Factor: A way to compare models/hypotheses. Also a way to do testing in the Bayesian mindset
- checkout book: https://press-files.anu.edu.au/downloads/press/n1652/pdf/book.pdf
- minimizing some risk function on the posterior delivers point estimates
- whether mean, median or mode or whatever are wanted, implicitly depends on the risk function
- the decision theory + loss function is also a bridge to machine learning: what if we only look at best possible decision instead of relying on random variables and DGP thoughts?

-->

* Probability is a degree of belief, and learning from data means updating belief.
* Model parameters are random variables with prior and posterior distributions.
* A [statistical modeling mindset](#statistical-modeling) with [frequentism](#frequentist-inference) and [likelihoodism](#likelihoodism) as alternatives.

<!-- relation to others -->
If you haven't read the chapter on [Statistical Modeling](#statistical-modeling), I recommend that you do so first, since Bayesian inference is easier to understand if you have a good understanding of statistical inference.
Bayesian inference is based on probability distributions, interpreting parameters, and learning from data through the likelihood.
The twist: distribution parameters are also random variables.
Random variables that have a prior distribution.
Prior means before encountering the data.
Learning about the parameters means updating the prior probabilities to the posterior probabilities.

<!-- summary learning with Bayesian inference -->
In Bayesian statistics, probability can be interpreted as the plausibility of an event or our belief about it.
That's different from the more objective interpretation of probability in [frequentist inference](#frequentist-inference).
In Bayesian inference, it's not necessary to imagine repeated experiments.
One can even apply Bayesian inference to a single data point.
This is not possible with frequentist inference.
A Bayesian model even works without data, by just using the prior distributions. [^prior-predictive]

## Bayes Theorem

Bayesians want to learn the distribution of the model parameters from the data: $P(\theta | X)$.
P(\theta | X) is a strange way of looking at the probabilities involved.
The data-generating process generates data as a function of the parameters: $P(X | \theta)$.
So Bayesians look for the inverse of what the DGP would naturally do.

<!-- Bayes' theorem has many applications -->
To make $P(\theta|X)$ computable, Bayesians use a trick that earned them their name: the Bayes' theorem.
Bayes' theorem can be used to invert the conditional probability:

$$\underbrace{P(\theta|X)}_{\text{posterior}} = \frac{\overbrace{P(X | \theta)}^{\text{likelihood}} \cdot  \overbrace{P(\theta)}^{\text{prior}}}{\underbrace{P(X)}_{\text{evidence}}}$$

$P(\theta)$, also called prior, is the probability distribution of $\theta$ before we have collected any data.
The probability distribution is updated by multiplying the prior by the data likelihood $P(X | \theta)$. [^confusing-likelihood]
This product is scaled by the probability of the data $P(X)$, also called evidence.
The result is the posterior probability distribution, an updated belief about the parameters $\theta$.

Bayes' theorem is a generally useful equation for working with probabilities, but we focus on its use for Bayesian inference.
The theorem is not just a simple rearrangement of probabilities, but a powerful mental model: Bayesian updating.
Remember that the data likelihood is the product of the likelihoods for each data point:
$$P(X | \theta) = \prod_{i=1}^n P(X^{(i)} | \theta)$.
Here, $X^{(i)}$ is the vector of random variables of the i-th outcome.
For example, in a drug trial, the variables $X^{(i)} could belong to the i-th (not yet observed) patient and include pain level, blood pressure and iron level.
Plugging this version of the likelihood into Bayes' theorem, we can see how it relates to updating ones belief with new data.
For simplicity, I have removed the evidence $P(X)$ which only serves as a normalization so we can interpret the results as probability:

\begin{eqnarray}
P(\theta|X) & \propto  & P(X | \theta) \cdot  P(\theta) \\ 
            & =        & P(\theta) \cdot \prod_{i=1}^n P(\theta, X^{i}) \\
            & =        & P(\theta) \cdot P_1 \cdot \ldots \cdot P_p,
\end{eqnarray}

where $P_i = P(X^{(i)} | \theta)$.
Even with just one data point, we can update our belief about the parameters!
And each time, this posterior then becomes -- in a sense -- the prior for the next update.
The posterior distribution is the product of prior and likelihood (Figure \@ref(fig:bayesian)).

```{r bayesian, fig.cap = "The posterior distribution (right) is the scaled product of prior and likelihood: prior $\\times$ likelihood $\\propto$ posterior.", fig.width = 10, fig.height = 3}
library(ggplot2)
library(tidyr)

x = seq(from = -1, to = 8, length.out = 500)
prior_d = approxfun(x, dgamma(x, shape = 1.4, rate = 0.8))
likelihood_d = approxfun(x, dnorm(x, mean = 3.5, sd = 1.5))

posterior_d = function(x) {
  prior_d(x) * likelihood_d(x) 
}

auc = sum(diff(x)[1] * posterior_d(x))
posterior = function(x) {posterior_d(x) / auc}

dat = data.frame(prior = prior_d(x), likelihood = likelihood_d(x), posterior = posterior(x), x = x)
densities = c("prior", "likelihood", "posterior")
dat = pivot_longer(dat, cols = all_of(densities))
dat$name = factor(dat$name, levels = densities)

ggplot(dat, aes(x = x, y = value)) +
  geom_area(size = 1.1, fill = "lightgrey") +
  geom_line() +
  #coord_fixed(ratio = 20) +
  facet_grid(. ~ name) +
  theme_void() + 
 theme(axis.text.x = element_text(),
      strip.text.x = element_text(size = 18))
```

Next, let's explore the individual terms of Bayes' theorem, so that we can update our own beliefs about Bayesian inference.

## Prior Probability 

<!-- prerequisites -->
Bayesians assume that model or distribution parameters a prior probability distribution $P(\theta)$. [^just-one-level]
Let's say we randomly pick a person and want to know how many hours per day they usually work.
The number of hours worked per day, the random variable of interest, follows a probability distribution.
For example, the number of hours worked might follow a Gaussian distribution.
Bayesians assume that mean and variance of this Gaussian distribution are random variables.

<!-- role of the prior -->
How do priors make sense?
How can Bayesians know the distribution of parameters *before* observing any data?
Priors are a consequence of saying that parameters are random variables, and a technical requirement for working with the Bayes' theorem.
But how can we know anything about the parameters before we see the data?

### Picking a Prior {-}

<!-- parameter space reasons -->
The first consideration in choosing a prior is the *space* the parameter is in.
Is the parameter the mean of a continuous distribution?
If so, it makes sense for the parameter to follow a continuous distribution as well, such as a Gaussian distribution.
Maybe the mean of the data distribution has to be positive.
Then the prior distribution should contain only positive values (meaning the probability for negative values should be zero), for example, the Gamma distribution.
<!-- expert knowledge reasons -->
Furthermore, expert knowledge can be used to choose the prior.
Maybe we know from other experiments that the mean parameter should be around 1.
So we could assume a Gaussian distribution for $\theta$: $\theta \sim N(1, 1)$.
In the case where the data follow a Binomial distribution, the Beta distribution is a good prior (see Figure \@ref(fig:priors)).
Depending on what the modelers believes about the success probability parameter $p$ of the Binomial distribution, they might choose different parameterizations of the Beta distribution.
Maybe the modeler believes that the parameter must be symmetrically distributed around 0.5.
Or maybe the parameter is lower, around 0.25?
Another Beta prior might put emphasis on $p$ being 1.
<!-- intuition for alpha = beta = 0.5: https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than -->
It's even possible to have a prior that that places the greatest probability symmetrically on 0 and 1.

<!-- uninformative and conjugate priors -->
Without expert knowledge about the parameter, the modeler can use "uninformative" or "objective" priors [@yang1996catalog].
Uninformative priors often produce results similar to those of frequentist inference (for example for confidence/credible intervals). <!-- citation needed -->
Another factor influencing the choice of prior is mathematical convenience.
Conjugate priors are convenient choices.
Conjugate priors remain in the same family of distributions when multiplied by the right likelihood functions. 
A Beta prior distribution multiplied by a Bernoulli likelihood, in turn, produces a Beta posterior distribution. 

<!-- criticizing the prior-->
Although there are all these different strategies for choosing a prior, even "objective" ones, the choice remains subjective.
And this subjective choice of prior is why many frequentists reject Bayesian inference. <!-- citation needed -->
<!-- advantages of priors -->
While the prior can be seen as a bug, it can also be seen as a feature.
Thanks to the prior, Bayesian modeling is very flexible.
The prior can be used to constrain and regularize model parameters, especially when data are scarce;
the prior can encode results from other experiments and expert knowledge;
the prior allows a natural handling of measurement errors and missing data.

<!-- joint prior? -->
<!-- 
But that sounds overly complex, do we really have to specify all the priors?
There can be many parameters involved in a statistical model.
And in the Bayesian world, every parameter gets a prior.
These priors could even have a complex joint distribution with correlations between the parameters.
But usually, Bayesians specify the prior probabilities independent of each other.
-->

To obtain the posterior distribution of the parameters -- the ultimate goal of Bayesian inference -- we need to update the prior using data, or rather, the likelihood function.

```{r priors, fig.cap = "Various Beta prior distributions for the success probability p in a Binomial distribution. The parameters $\\alpha$ and $\\beta$ influence the shape of the Beta prior.", fig.width = 10, fig.height = 2.5}
library("ggplot2")
n = 100000
x = seq(from = 0, to = 1, length.out = 100)
#datbeta1 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 5), dist = Encoding("\u03B1 = 2, \u03B2 = 5"))
datbeta1 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 5), dist = "list(alpha==2,beta==5)")
# intuition for alpha = beta = 0.5: https://stats.stackexchange.com/questions/362728/whats-the-intuition-for-a-beta-distribution-with-alpha-and-or-beta-less-than
datbeta2 = data.frame(x = x, y = dbeta(x, shape1 = 0.5, shape2 = 0.5), dist = "list(alpha==0.5,beta==0.5)")
datbeta3 = data.frame(x = x, y = dbeta(x, shape1 = 5, shape2 = 1), dist = "list(alpha==5,beta==1)")
datbeta4 = data.frame(x = x, y = dbeta(x, shape1 = 2, shape2 = 2), dist = "list(alpha==2,beta==2)")
 
datbeta = rbind(datbeta1, datbeta2, datbeta3, datbeta4)

ggplot(datbeta, aes(x = x, y = y)) +
geom_area(fill = "lightgrey") +
geom_line(size = 1) +
facet_wrap(. ~ dist, ncol = 4, labeller = label_parsed) + 
scale_x_continuous(breaks = c(0, 0.5, 1), labels = c("0", "0.5", "1")) +
theme_void() +
theme(axis.text.x = element_text(),
      strip.text.x = element_text(size = 18))
```


## Likelihood

<!-- likelihood recap -->
If you have read the chapter [Statistical Modeling](#statistical-modeling), you should be familiar with the likelihood function $P(\theta, X)$.
The likelihood is equivalent to the probability function of the data.
Only that the focus is switched: the likelihood is a function of the parameters, while the probability is a function of the data.

<!-- where does the likelihood come from -->
The Bayesian makes an assumption about how the data are distributed and forms the likelihood function.
This is no different than [frequentism](#frequentism) and [likelihoodism](#likelihoodism).
But for Bayesians, the likelihood is just one part of the equation.

With all the comparisons between frequentist inference and Bayesian inference, it's easy to forget that both approaches are quite similar.
Comparisons between the two mindsets often focus on the (lack of) prior distribution.
This overlooks the fact that both frequentist and Bayesian inference use the likelihood at the core of their models.
Especially in cases with a lot of data, both approaches produce similar results.
That's because the more data are available for the model, the less impact the prior has on the Bayesian results.

Let's now turn to the last part of the equation: the evidence $P(X)$.

## Evidence

The evidence is the marginalized probability of the data.
Marginalized means that the probability of the data is integrated over all possible parameter values: $P(X) = \int_{\Theta} P(X|\theta) P(\theta) d\theta$, where $\Theta$ are all possible parameter values.
Because of this marginalization, $P(X)$ is no longer a function of the parameters $\theta$.
$P(X)$ is just a constant factor in terms of maximizing the posterior probability
Constant factors don't change *where* the maximum is, just how large it is.
In search of the maximum, the evidence $P(X)$ can be ignored.
For this reason, the posterior probability is often expressed as proportional to the numerator:

$$\underbrace{P(\theta|D)}_{\text{posterior}} \propto \overbrace{P(D | \theta)}^{\text{likelihood}} \cdot  \underbrace{P(\theta)}_{\text{prior}}$$

Just one problem: When throwing away $P(X)$, the posterior probability is not a probability at all, because it doesn't integrate to 1, but to $P(X)$.
How can this problem be solved?

## Posterior Probability Estimation

<!-- Goal of Bayesian -->
The goal of the Bayesian modelers is to estimate the posterior distributions of the parameters.
Once the modelers have the posteriors, they can interpret them, make predictions, and draw conclusions about the world.

<!-- Problem: Posterior estimation -->
But how is the posterior estimated?
In the ideal case, the posterior can be written down as a simple formula.
But that's only possible for certain combinations of prior and likelihood, for example when conjugate priors are used.
For many Bayesian models it's impossible to obtain a closed form for the posterior.
The main problem is that $P(X)$ may not be computable.

### Sample From the Posterior with MCMC {-}

<!-- Don't solve exactly, but sample -->
The good news: We don't have to compute the posterior probability.
We can sample from it.
Approaches such as Markov Chain Monte Carlo (MCMC) and derivations thereof are used to generate samples from the posterior distribution.

<!-- shortest MCMC explanation -->
The rough idea of MCMC and similar approaches is as follows:

* Start with some initial values for the parameters $\theta$.
* Repeat the following steps until a stopping criterion is reached (like pre-determined number of samples):
  1. Propose new parameter values. Proposals are based on a proposal function receives as input the previous parameters.
  1. Accept or reject the new values, based on an acceptance function. The acceptance function depends on the prior and the likelihood, but not on the evidence.
  1. If the new parameter are accepted, continue with these new values.

A run of MCMC produces a "chain" of samples from the posterior distribution.
MCMC can be repeated to produce multiple chains.

MCMC has many variants such as Gibbs sampling and the Metropolis-Hastings algorithm.
Each variant differs in proposal and acceptance functions or other algorithmic steps.

MCMC produces a random walk through the posterior distribution of the parameters.
Regions where the parameters have a high probability are also "visited" with a higher probability.
The samples can be seen as samples from the posterior.
But first, some cleaning up needs to happen:
Since MCMC has to start somewhere, it's possible that the first samples will be from parameter regions with low probability.
So the first 1000 or so samples are "burned", meaning they are not used for estimating the posterior.
Another problem is autocorrelation within the chain:
Samples that occur one after the other are correlated since the proposal function usually proposes new parameters that are close to the previous values.
So the chain is sampled at different points to ensure that there are enough MCMC steps between two samples to make them independent.

<!-- time and other resources -->
MCMC sampling can be complex and can take some time to compute.
Fortunately, most probabilistic software runs MCMC automatically.
But this can take time.
More time than fitting a frequentist model would take.
A shorter alternative is variational inference.[@blei2017variational]
But while MCMC delivers approximately exact estimates of the posterior probability, variational inference is more of a heuristic.

<!-- TODO: visualize MCMC -->
Frequentists have their parameter estimates.
Bayesians have ... samples from the posterior distribution?
That's not the end of the story.
There are two more steps required to get from posterior samples to insights: turning the samples to a distribution and (optionally) summarizing the distribution.

## Summarizing the Posterior Samples

<!-- from samples to distribution -->
The posterior samples can be visualized with a histogram or a density estimator for a smoother looking curve.
Visualizing the entire posterior is the most informative way of reporting the Bayesian results.

<!-- tables and numbers -->
People love numbers and tables.
The fewer and simpler, the better.
You won't get your manager to understand posterior distributions.
They demand simple answers!
So, let's simplify the posterior.
Summaries of the posterior can be  points or intervals.
Intervals can be defined via fixed boundaries or fixed probability mass.
Some examples:

* Point estimate: The parameter value with the highest posterior probability.
* Interval with fixed boundaries: The interval from 10 to infinity indicates the probability that the parameter is greater than 10. 
* Interval with fixed probability mass: The shortest interval containing 50% of the posterior probability mass. Or the interval that ranges from the 2.5% quantile to the 97.5% quantile ( called the 95% credible interval). 


```{r posterior, fig.cap = "Describing the posterior distribution with an interval, for example the 95\\% credibility interval, or a point estimate, for example the maximum a posteriori estimation (MAP).", fig.width = 9, fig.height = 3}
library("ggplot2")
set.seed(2)
n = 10000
datbeta = data.frame(x = c(rbeta(n/2, shape1 = 2, shape2 = 5), rbeta(n, shape1 = 5, shape2 = 2)))

qs = cumsum(datbeta$x) / sum(datbeta$x)

lower = max(sort(datbeta$x)[qs < 0.025])
upper = min(sort(datbeta$x)[qs > 0.975])

dd = density(datbeta$x)
dd = data.frame(x = dd$x, dens = dd$y)
dd = dd[(dd$x > lower) & (dd$x < upper), ]

f = approxfun(density(datbeta$x))
datbeta$f = f(datbeta$x)
map = datbeta$x[datbeta$f == max(datbeta$f)]

ggplot(datbeta, aes(x = x)) +
#geom_density(fill = "grey", size = 2) + geom_area(aes(x = x, y = dens), data = dd, fill = NA) +
annotate("text", x = map, y = 1.9, label = "MAP", size = 6, vjust = 1) +
annotate("segment", x = map,  y = 1.8, xend = map, yend = 1.68, arrow = arrow(), size = 2) + 
annotate("segment", x = map, y = 0, xend = map, yend = f(map), size = 1, lty = 3) + 
geom_density(size = 1.5, trim = FALSE, fill = "grey", alpha = 0.5)  +
annotate("segment", x = lower + 0.01, y = 0.1, xend = upper - 0.01, yend = 0.1, arrow = arrow(ends = "both"), size = 1.5) + 
annotate("label", x = mean(c(lower, upper)), y = 0.1, label = "95% Credible Interval", size = 6) + 
annotate("segment", x = lower, y = 0, xend = lower, yend = f(lower), size = 1, lty = 2) + 
annotate("segment", x = upper, y = 0, xend = upper, yend = f(upper), size = 1, lty = 2) + 
theme_void()
```


<!-- Credible interval In frequentist statistics, results of estimators are often described with confidence intervals.
That's some uncertainty quantifycation telling us how sure we are about our estimation.
But it's different with Bayesian inference.
Since we get posterior probabilities for our parameters, and from the we can derive credibility or credible intervals.
For example, a 95\% credibility interval contains 95\% of the mass of our parameter.
With 95\% probability, the parameter falls within that interval.
Not in the sense that there is some fixed value for the parameter.
But it's a random variable that can take on different values.
Technically, they work the same as confidence intervals in frequentist statistics.
But they have a different philosophical interpretation. 
-->



## From Model to World

<!-- statistical model -->
Bayesians build [statistical models](#statistical-modeling) to approximate the data-generating process with probability distributions.
These distributions are parameterized, and Bayesians say that these parameters are themselves random variables.
Learning means updating the model parameters.
But after the "update" there is no definite answer on what the true parameters are.
Instead, the modeler was able to reduce the uncertainty about the parameters whereabouts.
The posterior distribution doesn't encode the uncertainty inherent in nature, as in quantum mechanics.
Instead, the posterior distribution expresses the uncertainty about **information about the world**. 
<!-- compare with frequentist -->
That's different from how frequentists connect their statistical models to the world.
Frequentists assume that there are some unknown but fixed parameters that can be approximated with statistical models.
Uncertainty is a function of the estimators, and conclusions about the world are derived from how these estimators are expected to behave when samples and experiments are repeated.

## Simulate to Predict

<!-- Natural inclusion of uncertainty in prediction -->
Parameters are random variables.
At first glance, that's a problem if we want to make predictions with the model.
But it's actually a gift and not a problem.
In the Bayesian mindset, predictions are also random variables, not just point estimates.
The distribution of a prediction reflects the uncertainty of the parameters.
Not getting a definite answer or prediction seems inconvenient at first.
But it's much more honest and informative than a point estimate.
The modeler in the Bayesian mindset is encouraged to consider the prediction along with its uncertainty.

<!-- How to predict -->
To predict, the Bayesian must simulate.
Prediction means marginalizing the distribution of the prediction for a new data point $X_{new}$ with respect to the distribution of parameters: 

$$P(X_{new} | X)_{\Theta} = \int P(X_{new} | X, \theta) \cdot P(\theta | X) d\theta$$

Simulation means that values for the parameters are drawn from the posterior distribution.
For each draw of parameters, the modeler can insert these parameters into the probability distribution of the data and then draw the prediction.
Repeating this process  many times yields the posterior predictive density.

<!-- 
## Some Examples of Bayesian inference

What would a linear regression model look like in the Bayesian modeling approach?

For that we have to assume that $Y$ follows a Gaussian distribution given $X$.
The Bayesian mindset tells us that we are interested in the distribution of the parameters of this model.
In this model, the relationship between the mean of the distribution of $Y$ and the features are represented by the model coefficients $\mathbf{\beta}$.

So we would assume that y folllows a Normal distribution:

$$y \sim N(\mathbf{\beta}^TX, \sigma^2 I)$$

Since we are in the Bayesian mindset, we also assume a distribution for the parameters $\mathbf{\beta}$ and $\sigma$.

This you can see by looking at how we would get the posterior probability:

$$P(\beta|Y,X) = \frac{P(y|\beta,X) P(\beta|X)}{P(y|X)}$$

* $P(y|X)$ is the data evidence
* $P(\beta|X)$ is the prior probability for $\beta$
* $P(y|\beta, X)$ is the likelihood function
* $P(\beta|y,X)$ is the posteriori probability of the model parameters

We carry arround the $X$ in the formulas the entire time in the conditioning statement.
This is allowed.


TODO: List some real applications

TODO: List some models

## Justifications for Bayesian Inference

https://en.wikipedia.org/wiki/Bayesian_probability#Justification_of_Bayesian_probabilities


## Empirical Bayes

Combination of frequentism and Bayesianism

https://efron.ckirby.su.domains//papers/2005BayesFreqSci.pdf

-->

<!--  
## How Different is Frequentism Really?

[Frequentist Inference](#frequentist-inference) is the mindset to which Bayesian inference is compared the most often. The debate of which mindset is better has been going on for a long time and is still going on.
Throughout this chapter, we have seen that both mindsets have different ideas of what a probability is and whether to use prior probabilities.
These decisions have far reaching consequences on how to do modeling and how to interpret the models.
But with all the adversary, it's easy to overlook how close mindsets are, especially compared with many other mindset that you will encounter in this book.
Both mindsets are statistical modeling mindsets: They both rely on probability distributions.
And they both assume that interpreting those distributions is a valid way to see the world.
Also, everyone puts so much emphasis on the prior probability.
But, what ways much heavier, is the likelihood that is used by both frequentism and Bayesianism.
In cases where we have very few data points, the prior has a high impact on the result.
And for those data-poor cases frequentist and Bayesian inference will differ the most.
But in all other cases, where there is enough data, a modeler will often get similar results with frequentist and Bayesian inference.
And ultimately, they will come to similar conclusions.

Frequentist confidence interval can be interpreted as a special case of credibility interval (citation needed) with an uninformative prior.
Really, all frequentist method can be expressed as special cases of Bayesian models.

-->

## Strengths

* Bayesianism allows the use of prior information such as expert knowledge.
* Bayesian inference inherits all advantages of [statistical models](#statistical-modeling).
* Bayesian inference  provides an expressive language to build models that naturally propagate uncertainty. This makes it easy to work with hierarchical data, measurement errors and missing data. 
* A general benefit: Bayesian updating is an interesting mental model for how we update our own beliefs about the world.
* Bayesian interpretation of probability is arguably more intuitive than frequentist interpretation: When practitioners misinterpret frequentist confidence intervals, it's often because they interpret them as credible intervals.
* Since Bayesian inference always estimates the full posterior, decision based on the posterior always require another step. As a consequence, inference and decision making are decoupled. In frequentist inference, it's common to design the entire inference process around the decision (hypothesis tests).
* Bayesian statistics adheres to the likelihood principle which states that all the evidence from the data relevant to a quantity of interest must be contained in the likelihood function. 

## Limitations

* The choice of prior distributions is subjective. 
* The modeler always has to define a prior which can be tedious when many priors are involved.
* Bayesian methods are mathematically demanding and computationally expensive.
* When used exclusively for decisions, all the additional information about the entire posterior may appear as unnecessary overhead.
* No causal interpretations are allowed, just associations are modeled.

[^prior-predictive]: This is called the prior predictive simulation and is used to check that the chosen priors produce reasonable data. The modeler simulates by first sampling parameters from the priors and using those parameters to generate data. Repeating this several times results in a distribution of data.

[^confusing-likelihood]: In the Bayesian mindset $\theta$ is a random variable, so the notation can be confusing: $P(X | \theta)$ refers to the likelihood function. The same function that we used in frequentist inference. But in the frequentist inference, we might write $P(\theta | X)$ to emphasize that $\theta$ is the input that varies. But this notation refers to the posterior in Bayesian statistics, so we write $P(X | \theta)$.

[^just-one-level]: When I was learning about Bayesian inference, one of my first question was: do the parameters of the priori distributions have priori distributions themselves? Where does it end? The answer: Bayesian stop after one level of priori distributions and don't go full inception.
