# Likelihoodism {#likelihoodism}

<!-- TODO

- read: https://www.stat.fi/isi99/proceedings/arkisto/varasto/roya0578.pdf
- read "Statistical evidence"
-->


* The likelihood is all you need. No priors, no frequentist inference.
* Follows the likelihood principle and works with likelihood ratios.
* A [statistical modeling mindset](#statistical-modeling) with [frequentism](#frequent-inference) and [Bayesianism](#bayesian-inference) as alternatives.

<!-- where to place likelihoodism -->
I studied statistics for 5 years.
Never heard about likelihoodism.
It's the strange sibling of frequentism and Bayesianism that lives in the basement of it's parents.
If you ask the neighbours, they are surprised to learn that there are 3, not 2 children.
There's a good reason why I and probably you as well have never heard about it.
Likelihoodism is an impractical mindset:
likelihoodism doesn't guide decisions as frequentist inference does;
likelihoodism can't tell us what we should believe about the parameters.
How did it get any real estate in the book?
All this frequentist versus Bayesian talk is quite boring.
Taking on this third view of likelihoodism taught me a lot, and gave me a better understanding of statistical modeling.
You'll learn a new statistical modeling perspective that tells you a lot about the other two.

<!-- why learn about it -->
I'll present to you a gross oversimplification, but it's a useful first idea of what likelihoodism is:

* Likelihoodism = Bayesianism - priors, or:
* Likelihoodism = Frequentism - null hypothesis significance testing

TODO: REPRODUCE GANDBERG IMAGE HERE

```{r three-stat-mindsets}

circleFun <- function(center = c(0,0),diameter = 1, npoints = 100){
  r = diameter / 2
    tt <- seq(0,2*pi,length.out = npoints)
    xx <- center[1] + r * cos(tt)
    yy <- center[2] + r * sin(tt)
    return(data.frame(x = xx, y = yy))
}

dat <- circleFun(c(1,-1),2.3,npoints = 100)
#geom <- path will do open circles, geom <- polygon will do filled circles
ggplot(dat,aes(x,y)) + geom <- path()

```


In a sense, likelihoodism is a minimal mindset.
It puts all the focus on the likelihood of the data.
Likelihoodists reject priors, because they are subjective.
Likelihoodists also reject frequentist inference, because it doesn't adhere to the likelihood principle.


<!-- likelihoodism in a nutshell -->
So, what remains?
In the likelihood mindset, you can fit distributions and statistical models.
It just shouldn't involve priors or p-values.
To compare models/hypotheses, the ratio between the two entailed likelihoods are used.

<!-- likelihood principle as foundation -->
The likelihood principle is central to likelihoodism, so let's dive deeper here:

## The Likelihood Principle and Law of Likelihood

<!-- short reminder: likelihood -->
A short recap: The likelihood function is the same as the data density function, but with switched role of data and parameters.
Data $X$ are "fixed", and the likelihood is a function of the parameters $\theta$ $P(\theta | X) = P(X = x | \theta)$.[^not-bayesian]

<!-- likelihood principle -->
As we have seen, the likelihood is common to both frequentism and Bayesianism.
And likelihoodism is the third contender for which the likelihood is everything.
There are mathematical concepts that can tell us for each mindset just how important the likelihood is.
The likelihood principle, and the law of likelihood.

### Likelihood Principle {-}

The likelihood principle says that all the evidence in the sample which are relevant to the parameters must be contained in the likelihood function.
In more concrete terms, all information about the distributional parameters $\theta$ is contained in the likelihood data $P(\theta | X)$. 
If we reverse the statement: If any information about the data influences the parameters but is not part of the likelihood, then the likelihood principle is violated.

The likelihood principle is a consequence of 1) the sufficiency principle and 2) the conditionality principle.[@birnbaum1962foundations]
The sufficiency principle says that a sufficient statistic of a quantity of interest summarizes all relevant evidence.
The conditionality principles says that experiments or samples that were not performed should be ignored.

Bayesianism and likelihoodism adhere to the likelihood principle.
Bayesians use priors, but as long as they don't contain information from the data, it's fine. [^empirical-priors]
Frequentist inference violates the likelihood principle, due to it's interpretation of long-run frequencies.

### Voltmeter Story {-}

CONTINUE HERE

<!-- Voltometer story -->
There is a famous story by TODO.
It's about a frequentist statistician, and about changing the inference based on imagined experiments.

* An engineer measures voltages of electron tubes. The results range from 30V to 99V.
* The statistician computes mean and 95% confidence interval of the measurements.
* Later the statistician finds out that the voltmeter only goes to 100 Volts. Censored data. That means that the analysis has to change. The statistician cannot assume a Gaussian, but has to assume a distribution that is cut off at 100. Like a trimmed Gaussian.
* So far, also the likelihoodist would agree (except that they wouldn't compute confidence intervals).  
* But then the engineer also says that she has another voltmeter that goes to 1000 Volt. If the other voltmeter had maxed out, she would have switched to the 1000V one.
* Alright, back to the non-censored analysis from the beginning. I think here every type of statistician would also agree.
* A few day later it turns out that the 1000V voltmeter was broken. And the engineer admits that in that case she wouldn't have waited with the measurements and just reported 100V.     
* For the likelihoodist, this new information doesn't change anything. In this one experiment, no measurement maxed out, so the analysis can stay the same.
* The frequentist faces a more complex modeling problem now, because the imagined future experiment are now weirdly conditional on the probability that the 1000V voltmeter works.
* Remember the confidence interval interpretation: "Would we repeat the experiment multiple times, the confidence interval has a 95% chance to cover the true mean. With this new information about the experiment ("would have reported the censored measurements if 1000V voltmeter broken") the confidence interval has to involve the probability. 
* Repeating the experiment now involves the probability of the 1000V voltmeter working. It's a mixture of two distributions: If the 1000V device works the statistician would model the data with a Gaussian distribution. If 1000V voltmeter is broken, the statistician would model the data with a trimmed Gaussian distribution.
* This couldn't even be estimated, since we don't know the probability that it works or not, and also can't estimate it.
* It's an ugly situation.
* Truth to be told. Most frequentist statistician would be pragmatic enough to ignore it and just decide for the uncensored model and pretend that this is what the future "imagined" experiments look like.
* Nonetheless, there are still 3 options for confidence interval with different interpretations in the end.
* The experimental setup decide on how to do analysis



### Law of Likelihood {-}

* Stronger than the likelihood principle
* Says how the evidence depends on the likelihood. not only that it has to.
* If data $X$ favors hypothesis $H_1$ over $H_2$ if and only if likelihood ratio is greater than 1:$\frac{P(X | H_1)}{P(X|H_2)} > 1$.
* Provides a clear vision on how to do inference: Not through Null hypothesis testing, but by comparing two models that encode two hypothesis.
* Example: $H_1$ could be that all variables have a linear effect on outcome. $H_2$ could be that two have non-linear effect and the other are all zero.
* LoL can only tell us which hypothesis to favor
* LoL can't tell us what to believe or what to decide
* Frequentists use likelihood ratio tests
* Two models contain the same information about a quantity of interest if their likelihods are proportional [@berger1988likelihood]

Two likelihood functions contain the same evidence when they are proportional to each other.

## Modeling and Likelihood Ratios

<!-- building models -->
When likelihoodists build models, it is very similar to how frequentist build their models.
The big difference is in the interpetation of the models and how likehoodists deal with results.
Or rather, it's in what likelihoodists do NOT do.
Likelihoodists reject the idea of long-run frequences as the frequentists rely on.
This also means that likelihoodists don't use hypothesis tests, p-value, confidence intervals and so on.
Likelihoodists also reject the Bayesian interpretation of probability.
They can't rely on posterior distributions, and their interpretation as beliefs about the parameters.
How then, can likelihoodists say anything about the world with their models?

<!-- comparing models -->
When all is contained in the likelihood, then it is obvious that we should be comparing likelihoods.
For likelihoodists, building a model means creating a hypothesis.
The hypothesis can be: Daily vitamin D intake reduces the number of colds per year.
An alternative hypothesis could be: Daily vitamin D intake reduces the number of colds per year, but also whether the person is a couch potatoe or very active.
Both hypotheses can be modelled.
Let's say $Y$ is the number of colds, $X_1$ is the binary information whether someone takes vitamin D (yes/no) and $X_2$ the random variable representing whether the person is a couch potatoe (potatoe/active). 
As the outcome is a count, the statistician chooses a Poisson distribution for the outcome:

$$Y | X_1, X_2 \sim Poi(\lambda)$$ 

The parameter $\lambda$ describes the intensity of how often colds occur.  
To connect the random variables $X_1$ and $X_2$ with the outcome, we say that the expected outcome is a function of the random variables.

$$E(Y | X_1, X_2) = e^{\beta_0 + \beta_1 X_1}$$

That's for the first model, where we only have vitamin D as variable.
The other model also contains another variable:

$$E(Y | X_1, X_2) = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2}$$

Both can then be optimized with a maximum likelihood appraoch, as presented in [chapter statistical modeling](#statistical-modeling).
This gives us the best fitting parameters.
But it gives us something else as well.
And we haven't talked about it yet.
We get the likelihood.
Which is a number that can be interpreted as the likelihood of the model and its parameterization.
This means for our made-up example, we get two models with different parameter estimates and also two likelihood numbers.

<!-- comparing likelihoods -->
This brings us to likelihood ratios.
It's a simple equation: We divide the likelihood of model for hypothesis $H_1$ by the likelihood of model hypothesis $H_2$:

$$\frac{L(H_1|X)}{L(H_2|X)}$$

This allows one to compare the likelihood of two hypothesis.
A likelihood ratio larger than 1 favors $H_1$, a ratio smaller than 1 favors $H_2$.

<!-- LR in other mindsets -->
Likelihood ratios are also used in Bayesianism and frequentism.
IN Bayesianism as Bayes factor, also for model selection.
IN frequentism likelihood rations are often used as test statistics for hypothesis tests.


<!-- LR in likelihoodism -->
So, inference in the case of likelihoodism simply means accepting the hypothesis that has a larger likelihood?
There are only likelihoods of different hypothesis, and we use the one with LR larger than one.
Both likelihoods are the results of data.
Therefore, they are subject to variance.

As a shortcut, likelihoodists can use rule of thumbs for deciding whether likelihood ratios is already big enough.[@richard2017statistical]
For example a likelihood ratio of 8 is seen as fairly strong, and 32 or more as strong favoring. 

<!-- other problem -->
There is another problem: We can't compare likelihoods for different model classes.
If we would not encode our target $Y$ as number of colds, but as a binary target, like "Three or more colds in a year", then we can't compare the likelihoods of these models.
The first model would assume a Poisson distribution of the target, the second one a Binomial distribution.
The likelihoods would not be comparable any more.

<!-- using likelihood intervals -->
One approximation to make allow making decision based on the likelihood ratio: likelihood intervals.
Likelihood intervals or regions are the likelihoodist analogue to frequentist confidence interval and Bayesian credible intervals.
Confidence intervals are interpreted as coverage probability  and credible intervals as posterior belief about the parameter.
The interpretation of likelihood regions is in terms of relative likelihood.
The likelihood interval of a model parameter $\theta$ (like the coefficient) is the set of all $\theta$ values that yield a relative likelihood greater than a certain threshold:

$$\left\{\theta: \frac{L(\theta| X)}{L(\hat{\theta}| X)} \geq \frac{p}{100}\right\}$$

The $\hat{\theta}$ is the "optimal" parameter that statistician found using maximum likelihood (or similar optimization methods).
Let's say for a logistic regression model coefficient $\beta_j$: $\hat{\beta}_j = 1.1$. <!-- fix vim_ -->
Then an interval might be $[0.9; 1.3]$.
The constant $p$ serves a similar function lke the alpha level for confidence intervals:
It decides the size of the region.
A $p = 1$ would mean that only the maximum likelihood estimate is in teh interval.
A $p = 0$ would mean that all possible parameters are in the interval.
A $p=0.05$ would mean that all $\theta$ with up to a drop of five percent drop in the likelihood are in the interval.  

Problems with likelihood ratios:

* Have to choose distinct hypotheses
*


## Why Frequentism Violates the Likelihood Principle

Personally, I first thought that likelihoodism is just frequentism without hypothesis tests.
But the differences between the two run deeper.
Likelihoodism adheres to the likelihood principle, frequentism violates it.

<!-- motivation for example -->
The following example shows how frequentism violates the likelihood principle, but also a great illustration of the core idea of likelihoodism.
It shows that frequentism violates the likelihood principle.
The criterion why experiments are stopped is a relevant factor for frequentist inference, because it does not adhere to the likelihood principle.
<!-- starting coin example -->
Suppose we have a coin, and we want to figure out whether it's fair, or whether head turns up too often.
Let $\theta$ be the probability of head.
We have two different hypothesis:

So: $H_0: \theta = 0.5$ und $H_1: \theta > 0.5$

The $H_0$-hypothesis basically says that the coin is fair.
And $H_1$ claims that head coms up more often than tails.
We say that $X$ is the number of heads.
And $Y$ the number of trials.

<!-- two types of experiments -->
Let's pretend the outcomes of the two experiments are the following:

1. Toss coin 12 times. We observe 9 heads out of the 12 tosses.
1. Toss coin until 3 times tail was observed. 

Two experiments with the same outcome, but different procedures.
In experiment 1) we fixed $Y$, the number of tosses.
IN experiment 2), we fixed $X$ the number of heads.

<!-- likelihoodist view of the experiment -->
In the likelihoodist view, both experiments have the same likelihood, up to some constant factor.
They are proportional to each other:

$$L(\theta | X = 3) = \binom{12}{3} \theta^3 (1 - \theta)^9 (1 - \theta)^9 = 220 \theta^3 (1 - \theta)^9 $$

And for experiment number two:

<!-- the 2 out of 11 results from the fact that the last toss must be a head -->
$$L(\theta | Y = 12) = \binom{11}{2} \theta^3 (1 - \theta)^9 (1 - \theta)^9 = 55 \theta^3 (1 - \theta)^9$$

<!-- The Likelihood Principle: https://www2.isye.gatech.edu/isyebayes/bank/handout2.pdf -->
So the likelihoodists say that both experiments carry the same evidence.

<!-- Frequentist view on the experiments -->
However, frequentists would come to a different conclusion. [^vidakovic9999]
Before we dive into mathematics, how can it even be, from a point of intuition, that there are different results?
In short: frequentists also incorporate results that did not happen.
For hypothesis tests, $H_1$ often framed as some more extreme event to happen.
But we never observed these, we just take what we observed (here 9 out of 12 are tail) and pretend that we observe the event more often.
Due to the different framing on whether the 9 heads or the 12 trials are fixed, the $H_1$ hypothesis either includes probabilities of fictional experiments where 9,11, or 12 heads were observed.
Or, in the other framing, it includes all experiments where we observed more than 12 repetitions.
Remember number of tails = 3 is fixed, so more repetitions means in favor that $\theta > 0.5$.

When testing $H_0$ vs. $H_1$, we can see:

$$P(X \geq 9| H_0) = \sum_{x = 9}^{12} \binom{x}{12} 0.5^x (1 - 0.5)^{12 - x} = 0.073$$

So when we set the significance level $\alpha = 0.05$ we would not reject the hypothesis of a fair coin.

But for the other experiment:
Here we have to assume a Negative Binomial distribution:

$$P(Y \geq 9 | H_0) = \sum_{y=9}^{\infty}\binom{3 + y - 1}{2} 0.5^y 0.5^3 = 0.0327$$

Why is that?
Because in the second experiment, we have, in the frequentist view, consider all events where also $X==3$ but the numbr of trials running to infinity.
Here we get a significant result and would reject the $H_0$-hypothesis of a fair coin.

<!--

## Lack of priors

Let's take a silly example:

You eat rice.
It tastes funny. A bit like whiskey.
It could be either because it had gone bad, or someone has put something in it.
You have had bad rice, it had tasted differently.
But the taste would be plausible if someone had put whiskey into the rice.
So, two opposing hypotheses:

$$P(funny rice | person added whiskey) > P(funny rice|rice bad)$$

But it's much more likely that the rice has gone bad.
Because whiskey-pouring rice-haters are one of the rarest people on earth.
But without prior probabilites, it's not the right result.

-->

## Thinking the Likelihood Principle further


Sorry that this is becoming a bashing for frequentist inference.
But we have already concluded that likelihoodism has too severe shortcomings to serve as a good modeling mindset.
So let's use what we learned to better understand the other mindsets.

Hypothesis tests by design violate the likelihood principle.
They involve the probabilities of events that never happened, the Null hypothesis.

For frequentist inference, how data are sampled or how the experiments was designed has an influence on the results.
Because these things influence how tests are formulated and so on.

Imagine you have 1000 data points.
How was it decided to stop at 1000 data points?
Will you collect more data depending on the results?
Maybe collect more data if the results were not significant.
If a mindset does not follow the likelihood principle, these decisions will affect the results of the analysis.

## Strengths

* An attractive idea to say that all information is contained in the likelihood. Conforms to the likelihood principle.
* Therefore likelihoodism is also not dependent on the experimental design like frequentism is.
* Likelihoodism inherits all strengths of statistical models.
* No prior distribution needed, and therefore "objective".
* Likelihoodist ideas can enhance reporting of Bayesian results. For example, by also reporting likelihood rations of models without using the priors.

## Limitations 

* Likelihood ratio favoring one hypothesis over another does not allow a decision for one model, or not a degree of belief.
* Likelihoodism does not give guidance for action as in frequentism
* Nor does it give degree of believe as in Bayesianism
* Therefore, little practical value
* Likelihoodism has been critized[@gandenberger2016not], because: no use of priors.

[^not-bayesian]: Not interpreted in the Bayesian way. Just that $X$ are seen as given.

[^empirical-priors]: The likelihood principle is of course violated if the data is used to inform the pior. For example empirical priors.
