# Risk Minimization {#risk-minimization:w}

TODO: Write when you have finished some other chapters (supervised ML, generative models and deep learning).
<!-- TODO

- inductive bias
- inductive learning

-->

<!-- Clarification -->
Machine learning is a broad term and usually means something like algorithms that learn by themselves from data.
But I want to focus on the mindset of learning by doing risk minimization.
Not risk as in investing in Bitcoin and hoping for the best.
But risk as minimization of a loss function, but based on data.
This chapter is also a meta mindset chapter.
We will get into different flavoured mindset of machine learning in other chapters.


<!-- summary of ml -->
The goal of machine learning is to learn some kind of function $f$ that minimized some loss function $L$ over data.
In the ML mindset, the best model is the one that has the lowest generalization error.
As a machine learner you think in loss, features, optimization, benchmarking.
Your goal is to optimize some loss function $L$.
You are allowed to produce any function $f$ that produces the outcome $Y$ from the features $X$.

$$\arg \min_{f} L(X, f(X))$$


<!-- optimization without data -->
Keep in mind that data is involved.
So this loss-based mindset is not about optimizing the Traveling Salesmen problem.
Because Traveling Salesmen is solved by finding one solution for one problem.
But with loss-based mindset your goal is to find some form of model that can also be applied to new data and it's also learned from data.

<!-- comparison  with other mindsets -->
Since loss-based optimization is stub mindset, we should compare it to the other one: statistical modeling.
In the extreme case the modeled relationships between the variables (also called features in machine learning) does not matter at all for machine learning.
If machine learning were a boss, it would say "I don't care how you get the job done, just get it done".
Maximum likelihood estimators are also optimization problems.
Maximizing the likelihood for the conditional mean of a Gausian distribution is the same as minimizing the mean squared error, for example. <!-- TODO: find citation -->
Is statistical modeling a special case of loss-based modeling?
No, because optimization is not that central.
It's a technique required for e.g. maximum likelihood estimation.
But there are other estimation means, such as Markov Chain Monte Carlo sampling.
And if a statistician does not have to use optimization, they don't care.
In purely [statistical mindsets](#statistical-modeling), it's important that we work with probability distributions.
And our focus is to understand how the random variables are connected to each other.
But in the purely risk minimization setup, we only care about optimization and generalization, or a specific form thereof.

<!-- one method, many mindsets -->
Sometimes you can see the same methods through two mindset.
The linear regressino model assumes that $Y$ given the variables $X$ follow a Normal distribution.
That's the statistical modeling mindset.
But you could also see it as an optimization problem with constraints.
Given $X$, find a linear function that minimizes the mean squared error.


## Examples of Loss functions

* Mean squared error between some label and a prediction model
*


## Loss Optimization

The output of the training process is a machine learning model $f$.
Thereby it does not really matter what form $f$ takes on, as long as the generalization error is low.
We have to types of metrics involved here:
The loss function, that's the one that is directly optimized during the training phase.
Then there is the evaluation metric.
Can be the same as the loss that is optimized, but sometimes different.
Especially when the evaluation loss is not derivatible.

How is the loss function chosen?
The choice is quite central to the problem that should be solved with machine learning.
Working with a classification problem with two possible classes, like healthy or unhealthy?
Use the logistic loss:

$$L(x_i, y_i) = y_i log(f(x_i)) + (1 - y_i) log(1 - f(x_i))$$

Here $f(x_i)$ would be the probability for one class (e.g. healthy).
This loss function potentially gives the same weight to every data point.
It would be possible to give a higher loss to certain data points:

$$L(x_i, y_i, w_i) = w_i[log(f(x_i)) + (1 - y_i) log(1 - f(x_i))]$$

The weight $w_i$ could for example increase the weight of patients that have an expensive premium health insurance (\*evil corporate laughing in the background\*).
A higher loss means that a wrong prediction for these data points are more costly and the model is pushed more in the direction of getting these predictions rights.
In theory, you could build custom loss function for any application.
Not all models can directly optimize a loss function though.
Many models already have built-in loss functions.
But you can always do hyperparameter optimization with regards to the evaluation loss.

* loss-based example example from supervised learning
* clustering
* find some loss function
* e.g. hierarchical clustering
* are based on distances between points
* maximize distance between 


