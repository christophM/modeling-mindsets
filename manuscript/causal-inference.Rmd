# Causal Inference {#causal-inference}

* A model is a good generalization of the world if it encodes causality.
* Causal models connect random variables through directed cause and effect relationships.
* Causal inference is a two-step process in which first a causal model is assumed and constructed and then translated into a [statistical](#statistical-modeling) or [machine learning](#machine-learning) model.

<!-- TODO 

- add personal story at the beginning
- Add some story about causalists to the beginning?
- write about randomized tests / A-B tests, but more as in that  I describe it in another mindset
-->

"Thank you so much for this statistical model," the ecologist says to the statistician, and continues, "Nice p-values, and insightful findings! Would it be correct to say that the droughts **caused** the crop failures?"
The statistician looks at the ecologist, a hint of concern at the corner of the eyes.
As if practiced, the statistician says: "Correlation does not imply causation".
Unsatisfied, the ecologist responds: "But it would make so much sense to conclude that the drought was the cause!"
The statistician grimaces and clenches the teeth as if in pain.
"Correlation does not imply causation", the statistician repeats, the words having a weird melody, as if in prayer.
"But without a causal interpretation, how can the results be applied to advance science? I want to understand **why** the crop failures happened!", the ecologist insists.
"Correlation does not imply causation. Correlation does not imply causation. Correlation ...", the statistician now chants, eyes closed shut, face twisted as if in great pain.
The ecologist slowly retreats, shocked by the strong reactions of the statistician.
Sometimes at night, when the wind howls outside, the ecologist hears the statistician's mantra in the wind.

## Does The Drug Help?   

<!-- SCQM story -->

A while back, I worked with a rheumatologist on an important medical question:
Do TNF-alpha blockers reduce the long-term symptoms  of patients with axial spondyloarthritis, a chronic disease that is associated with inflammation of the spine.
In the long-term, the joints in the spine can fuse to due to new bone formation (ossification). 
TNF-alpha blockers, given regularly as injection or infusion, work really well to reduce inflammation.
To understand whether TNF-alpha blockers also help against the ossification, a clinical trial might have given the best evidence.
But withholding TNF-alpha blockers would be unethical due to their proven efficacy, and also the study would require a long-term observation.
The next best option was to use observational data from hospitals and medical practices.
The registry for rheumatic patients  I was working for maintained a huge data base of patients with axial spondyloarthritis, holding insightful data about the patients health-related history: doctor visits, blood values, x-ray images, and so on.
In collaboration with the rheumatologist I built a statistical model to answer whether TNF-alpha blockers help against ossification.
For these patients, we had x-ray images two years apart, from which radiologists had scored the progression of the new bone formation in the spine.
To predict the progression, the model included various variables that were measured at the time of the first x-ray: the age of the patients, the disease duration, inflammation levels, medication used, and so on.
The result of the analysis was that the drug didn't reduce ossification.
The lead statistician of the patient registry happened to participate in a course on causal inference around the same time.
She had the epiphany that we approached the modeling question the wrong way.
She drew a diagram visualizing how the drug, the inflammation and the ossification might be related.
She drew the graph like this (Figure \@ref(fig:tnfdat):

```{r tnfdag, fig.cap = "The drug was known to influence inflammation (reduce it). Inflammation was thought to cause ossification (new bone formation). More inflammation, more ossification. The drug has, potentially, two pathways to reduce ossification: directly, or indirectly via reducing inflammation.", fig.height = 3, fig.width = 9}
arr = arrow()
nodes = data.frame(x = c(1, 2, 3),
                   y = c(1, 2, 1),
                   label = c("Drug", "Inflamed\nSpine", "New\nBone"))

edges = data.frame(x =    c(1, 2, 1),
                   xend = c(2, 3, 3),
                   y =    c(1, 2, 1),
                   yend = c(2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.7, 2.5)
edges$xend2 = edges$xend + c(-radius, - radius, -radius)
edges$yend2 = edges$yend + c(-radius, radius, 0)
edges$x2 = edges$x + c(radius, radius, radius)
edges$y2 = edges$y +  c(radius, -radius, 0)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 25, color = "darkgrey") +
  geom_point(size = 23, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  annotate("text", label = "?", x = 2, y = 1.15, size = 10) +
#  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```


<!-- the solution of the story -->
It became immediately clear what the problem with our current model was:
Inflammation was a potential mediator of the effect of TNF-alpha blockers on long-term ossification.
Figure \@ref(fig:tnfdag) shows that the effect of the drug can be split into a direct effect and an indirect effect.
By having the inflammation level as part of the model, the indirect effect of the treatment was "swallowed".
Any changes to the ossification based on reducing inflammation was reflected in the coefficient for inflammation levels.
The way we set up the initial model, the causal interpretation of the coefficient for TNF-alpha blockers concerned only the direct effect.
But we were actually interested in the total effect.
The total effect is the direct effect of the medication plus all indirect effects, in this case indirectly via reduction of inflammation.
So we adjusted the model accordingly, removing the inflammation variable. [^adjust-inflammation], so that the coefficient for TNF-alpha blockers would contain the total and not only the direct effect.
Now the model clearly showed that TNF-alpha blockers reduce ossification by decreasing the inflammation levels.
It sounds like common sense in hindsight, but coming from a frequentist mindset, my mind was blown.
This moment was such a revelation and got me interested in causal inference.

CONTINUE HERE

## Causality

<!-- what is causality -->
<!--$X$ is a cause of $Y$ if changing $X$ changes the probability distribution $P(Y)$. -->
We all have an intuition about causality.
Rain is a possible cause for a wet lawn.
A drug can be a cause of getting healthy.
An environment policy can be a cause of reduced CO2 emissions.
In terms of random variable, we can express causality in terms of distributions and interventions:
If you **force** a random variable to have take on a certain value, how would the distribution of another random variable change?
A cause is different from association:
An association is only a statement about observation.
We know that having a  wet lawn does not *cause* your neighbours lawn to be wet.
How do we know it? Try watering your lawn for one year, every day, and see whether the probability for your neighbors lawn being wet has changed.
But the two lawns two are associated:
When you observe that your lawn is wet, the probability that your neighbors lawn is wet is high.
The reason for association is, of course, the rain.
Such shared causes are called confounders.

<!-- not for the statisticians -->
The archetypal statistician avoids speaking of causality.
At least that's my experience after getting a Bachelor and Master in statistics.
What I learned about causality in those 5 years can be summarized in two statements: 1) Always add all confounders when building a statistical model, and 2) correlation does not imply causation.
We were taught not to causally interpret statistical models.
We were taught to ignore the elephant in the room.
Causality was presented as an unreachable goal that should not even be attempted with statistical modeling.
We were taught how to dance around the topic.
A random variable can only be associated with the outcome, but we may not speak about the variable causing the outcome.

<!-- a statisticians mantra goes against science -->
"Correlation does not imply causation" truly is a mantra that you hear multiple times when you learn about statistics.
I find that very curious, especially given that statistical modeling is supposed to be THE research tool of our times.
Isn't research all about detecting how the world works?
The "how", at least for me, implies that scientists are supposed to uncover causal structures.
And the truth is that, in the end, the results are, very often, interpreted causally, by the domain experts, by lay persons, and by the media.
So shouldn't everyone at least attempt to make the model reflect causality as much as possible?
Fortunately, some people think that we should put causality first.

Welcome to the **causal inference** mindset.

## The Causal Mindset

<!-- causal inference framework in a nutshell -->
The causal inference mindset puts causality in the center of modeling.
The goal of causal inference is to identify and quantify the **causal** effect a random variables had on the outcome of interest.

<!-- Relation to other mindsets -->
I would say that causal inference is a [statistical modeling mindset](#statistical-modeling), because it relies on probability distributions and random variables.
Causal inference could also be seen as an "add-on" to other mindsets like [frequentist](#frequentism) or [Bayesian](#bayesian) inference, but also for [machine learning](#machine-learning).
But it would be wrong to think causal inference as just a cherry on top of other mindsets. 
It's much more than just adding a new type of method to another mindset, like adding support vector machines to supervised learning.
Causal inference challenges the culture of statistical modeling.
It requires the modelers to think more about the data-generating process, to be explicit about causes and effects.

<!-- mindsets without causality are broken -->
It's kind of surprising just how many models are "broken" because they ignore causal thinking.
A lack of causal considerations can mean that the analysis of a research paper is invalid or that a machine learning model in a product is vulnerable to changes in the data distribution or adversarial attacks.
Take Google Flu prediction model as an example.
Google predicted outbreaks of the flu based on frequencies of certain search terms.
Clearly, the prediction model was not a causal model.
If it were causal, it would mean that you can cause flu outbreaks by searching on Google for certain terms.
The flu detection model missed, for example, the nonseasonal 2009 flu. [@lazer2014parable]
The machine learning model quickly declined in performance because the search patterns changed over time.
No causalist would have signed off on such a non-causal model.
A model that relies on only associations is as ephemeral as a fruit fly.
A model only generalizes well when it encodes causal relationships.
A causal flu model might rely on the virulence of the current flu strains, the number of vaccinated people, predictions of how cold the winter would be and so on.
But never based on search terms.

<!-- data can't speak for itself -->
You can look as hard at the data as you want to, but it won't reveal the causal structures that produced it.
You can automatically infer associations from the data, but even the simplest causal structures are ambiguous.
The amount of sunshine on a given day can be considered causal for the number of park visitors.
In a dataset, both features would appear as columns with numbers in it.
And if we would compute the correlation, we would find out that sunshine and park visitors are positively correlated.
The more sun, the more people.
The more people, the more sun.
The causal relationship is clear: The sun couldn't care less about park visitors.
Instead, the sun is the cause of park visits.
But this causal direction is not clear for your computer.
No matter what choose as a target, the computer will oblige and fit the model.
Breaking news: The government forbid visits to the park, in an effort to cool down the current heat wave.
The causal mindset requires thinking even more about the data-generating process, making assumptions about causal relationships.
These assumptions are an attack surface to critizice the mindset.
But on the other hand, making these assumptions explicit allows to address and discuss derivating opinions.

CONTINUE HERE

<!-- encoding causality -->
The best way to visualize causal models is with a directed acyclic graph that you have already seen in Figure \@ref(fig:tnfdag).
As I said, at the core causality is a statistical modeling mindset.
As such it employs probability distributions.
But, as we learned, in the causal mindset, these distributions cannot capture the causal structures.
The joint distribution of lawn wet and rain can tell us about associations between the two, but not the causal directions.
Causal inference put's a causal model *on top* of the joint distribution.
A causal model can be expressed in visual form: the directed acyclical graph.

## Directed Acyclical Graphs

<!-- short DAG intro -->
The causal inference mindset naturally comes with a visual language to lay out the causal relationships of the random variables.
Directed Acyclical Graphs (DAGs) are graphical models of variables and their causal relationships.
From DAGs you can directly see which variable is a cause of another variable.
Variables are nodes.
Causal directions are arrows.
It's an acyclic graph, meaning we are not allowed to have arrows going in circles.
For example, another arrow from $Y$ to $X_1$ would make this a cyclic graph.
Acyclicality is an assumption needed for causal inference.

```{r dag, fig.cap = "A Directed Acyclic Graph (DAG) with 5 variables.", out.width = "\\textwidth", cache = TRUE}
arr = arrow()
nodes = data.frame(x = c(1, 3, 2, 2, 2),
                   y = c(2, 2, 3, 2, 1),
                   label = c("Y", "X1", "X2", "X3", "X4"),
                   role = c("target", "include", "include", "exclude", "exclude"))

edges = data.frame(x =    c(2, 2, 2, 3, 1, 3),
                   xend = c(1, 3, 1, 2, 2, 2),
                   y =    c(3, 3, 2, 2, 2, 2),
                   yend = c(2, 2, 2, 2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.9, 3.1)
edges$xend2 = edges$xend + c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$yend2 = edges$yend + c(radius, radius, 0, 0, radius, radius)
edges$x2 = edges$x - c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$y2 = edges$y -  c(radius, radius, 0, 0, radius, radius)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 21, color = "darkgrey") +
  geom_point(size = 20, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```

<!-- Structures in a DAG -->
What can we see from the DAG in Figure \@ref(fig:dag)?
Variables $X_2$ and $X_3 are direct causes for target $Y$.
We can also see that $X_5$ does not directly influence $Y$.
So the DAG already shows us a lot about the dependencies and independencies.

<!-- TODO: Visualize Fork, Pipe, Collider, Descendant, see p. 185 in statistical rethinking -->

<!-- DAG in Bayesian -->
DAGs are simple visual tools, not specific to causal inference: These graphs are basically a bunch of circles that are connected with arrows, and you are not allowed to go in circles.
DAGs also make an appearance in [Bayesianism](#bayesian), or more specifically in Bayesian networks aka probabilistic graphical models.
There they *might* represent causality, but they don't have to.

<!--
* Elements: chain, fork, collider
* Roles of variables: confounders, mediators
* Also conditional dependence and conditional independence
* Conditioning model on a variable can make two variables conditional dependent to independent or vice versa.
* Goal: When modeling from, e.g. X1 to Y, make sure some paths are blocked
* A variable is seen as a cause of another variable if changing it's value changes the value of the other variable
* DAG is a tool to discuss causality
* Also a tool to detect which variables to condition on and which not in your model
* A part of causal inference is just about the identification of DAGs
* While you can't fully discover causal relationships, it's possible to make some progress
* For example, X -> Y and Y <- X would have same correlation in data.
* But you can detectd when X and Y are independent just from data
* We can see that $X_3$ is not only a cause of $Y$, but also of $X_2$, therefore being a confounder to $Y$.
-->

DAGs visually encode the causal model.
How do we know which variables to include, where to place the arrows and in which directions they have to point?
There are a bunch of indicators that help us build up the causal model:

* Direction of time: If one variable comes earlier, it might be the cause of the other. But confounders could be problematic here.
* Domain experts: In many cases layman knowledge does not suffice. But respective experts such as  medical doctors, engineers and ecologists have a good idea how the graph should look like (once they have been introduced to the graph language).
* Causal structure learning: There are numerous approaches to detect the causal structure from the data. We can't perfectly deduct all relationships, but there is a lot: For example if two variables are completely independent from each other, we know that there is no arrow between the two. Causal structure learning can identify sets of plausible DAGs, sometimes leaving out which ways the arrows have to point.
* Everyday knowledge. See rain/lawn example.

<!-- subjectivity -->
The causal model has substantial subjectivity built in.
It's a criticism of the causal mindset as well.
But thanks to DAGs, these causal decisions are made explicit.
This makes it possible to discuss them.

<!-- how to calculate P(Y|do(X)) -->
So, causalists got themselves a language to talk about causality.
They have a tools to visualize causal structures.
But how do we learn something about the world?
How do we estimate the causal effect of a variable on some outcome we are interested in?
And is it even always possible to calculate causal effects?
But how do they translate to a model, like with numbers and stuff?

## Many Frameworks For Causality

There are many "schools" or frameworks of causal inference, each with their own notation and approaches. [@hernan2010causal]
One such school is around the works of Judea Pearl, who invented the do-calculus $P(Y|do(X))$, a mathematical language to express interventions on data.[@pearl2012calculus]
But really, you find various frameworks and approaches that have causality at the center.
Here is an overview.
You can see these as various entry points for diving deeper into the world of causal inference.

* A huge part of causal inference works puts the focus on designing the experiment rather than a causal model that operates on observational data. The claim for causality is generated through randomization instead of pure modeling. Here we have clinical trials, A/B - tests.
* Natural experiments are observational data which originate from a data-generating process that looks very similar to an experiment. When John Snow investigated cholera cases, he worked with a natural experiment involving to water companies: John Snow identified contaminated drinking water as the source of cholera, because the customers of one water company got sick of cholera much more often. The association of households to one company or the other served as a natural experiment, randomizing factors such as age, comorbidities, education, and so on.
* Propensity score methods aim to emulate group versus control group setups for also comparing the effect of a binary variable on some outcome. One of the groups, usually the control group, is created by "matching" data points that are similar to the treatment group.  
* Do-calculus, structural causal models, and the backdoor criterion offer a very general modeling language and tools for causal inference. [@pearl2009causal]
* The **potential outcomes framework**[@athey2016recursive] is another big causal framework, mostly useful for studying the causal effect of binary variables, and originated from the social sciences. For example drug treatment effects on a disease outcome.
* Causal discovery or structure identification is a subset of causal inference that aims to discover causal relationships from merely observational data. Against what I said earlier, it is possible to automatically derive a set of DAG's from pure observational data. But: You have to make some assumptions, and there will be multiple possible DAGs.
* And there are many individual methods for causal inference. To name an individual approach: "honest causal forests", for example, are based on random forests which are typically used in in supervised learning. Honest causal forests are designed to model heterogeneity in treatment effects.[@athey2016recursiv]
* 
* ...

All approaches have in common that they assume a causal model.
This causal model can be rather implicit, like deciding on mechanisms for generating the control and treatment groups, or rather explicit, by drawing a DAG.
The causal model is then translated into a statistical estimator.
For setups that design the experiments, the estimates can be very simple, such as just comparing the means of two groups, for example to get the treatment effect of a drug.
Or, especially with observational data, involve translating a causal model into a statistical estimator.
And that's the case we will have a closer look next.

## From Causal to Conditional

<!-- option 1: experiments  -->
Based on our DAG, we could start designing an experiment, in which we control the variable of interest $X$.
That's being done already, of course.
For example, that's what randomized clinical trials do:
The goal of these trials is to understand the *causal* effect of a treatment on a disease.
So in those trials the random variable "treatment" is controlled.
I considered this as a distinct mindset in [design-based inference](#design-based).
Anyways, controlled experiments are expensive, time-consuming.
Sometimes they are not feasible, unethical or outright impossible.
Imagine you study data on the country level.
You couldn't just interfere with a countries GDP or number of new births.

<!-- observational data it is -->
Observational data it is, in so many cases.
With observational data, the first casualty is causality -- at least from the point of view of non-causalists.
But observational data is when causalists become excited and start stretch-exercises for their hand-wrists to warm up for the statistical programming ahead.

<!-- from causal to observational -->
But causalists claim to have a way to calculate causal effects, even without experiments.
It's a bit of an industry secret, but I am willing to spill it: The secret is LHC.
LHC stands for large hadron collider.
The causalists collide particles at a high-energy -- producing black holes in the process.
Each black hole contains a parallel universe, and with enough of those we can study what-if scenarios.
Joke aside, there is no magical ingredient for the estimation.
The "secret": We can't estimate causal estimands directly.
Causalists translate causal estimands into statistical expressions that can be estimated using observational data.
In the end, causal inference is just boring old stats.

<!-- recipe -->
But one step after another: How do we answer causal questions with models and data?
Causalists follow these steps [@pearl2009causal]:

1. Formulate causal estimand, like: What is the causal effect of $X_1$ on $Y$?
1. Build a causal model: Draw DAG.
1. Identify whether causal estimand can be estimated, given the causal model. Requires translating it into conditional probability statements.
1. Estimate the (translated) target quantity.

CONTINUE HERE

<!-- Step 1: Causal estimand-->
Let's have a look at the individual steps.
The first question is, what causal relationship we want to study.
This can then be expressed as some causal estimand, for example how would the air pollution be reduced when we ban car traffic in the inner city.
By choosing the target and the possible cause, we can now proceed to building the causal model.

<!-- Step 2: Causal Model -->
The causal model can be build using visual tools like the DAG.
Besides the target and the potential causal variable, all other variables that are relevant to both should be included in the causal model.
It's kind of collecting all the nodes for the DAG.
But we also need the arrows that connect the variables, with the direction of the arrows showing the causal direction.
Identifying between which variables an arrow should be and in which direction it should point can be narrowed down, in parts, automatically.
But in the end, a lot of those causal directions will be based on domain knowledge and subjectivity.
But in the end, we do have a DAG.
Not all approaches and frameworks will necessarily encourage or require to draw such a DAG, but you always have to decide on what the confounders are and so on.  

CONTINUE HERE

<!-- Step 3: Identify -->
In the identification step the causalists find out whether the causal estimand can be even answered with the observational data at hand.
This means that the causalist has to check whether the assumptions of the causal inference hold, given the causal model.
If that's the case, the causal estimand can be transformed into a statistical estimand.
Or a machine learning model, depending on where the causal model was put on top.
For some approaches, the process is not so complicated, especially when methods are based on randomized experiments, or are restricted to certain variable constellations.
Identification can be a complicated process.
But there are also many "simple" rules that tell you how to turn a causal estimand into a conditional estimand.
It's important to know that in the end we usually have some regression model, and our goal is to pick the right subset of random variables to use in the model (to adjust for).
The simpler rules are visualized in Figure \@ref(fig:dag-rules).

* Include confounders. Confounder are variables that cause both the variable of interest and the outcome. For example in Figure \@ref(fig:dag) $X_3$ confounds $X_2$ and $Y$. So we have to include it in our statistical estimand.
* Exclude mediators. When we want to measure the causal effect of $X_1$ on $Y$, we have make sure not to include $X_2$. If we include $X_2$ it will completely block the path, meaning we will find out that $X_1$ does not influence $Y$. But it does. Just via $X_2$. In general don't condition on descendants of treatment variable.
* Exclude colliders. If not $Y$ were our target, but $X_2$ and $X_3$ the effect of interest, then $Y$ would be a collider. Adding colliders adds a fake causal path. For example: sweating -> wet shirt <- it's raining. sweating and rain not correlated (or just weakly). But when we condition on wet shirt, they suddenly are strongly correlated! Bc. when shirt wet, but no rain, we can conclude that the person is likely to be sweating.
* There are more such rules, but by doing the above, you already exclude some bad non-causal paths.

```{r dag-rules, dependson = "dag", fig.cap = "To understand the causal effect of X1 on Y, we have to build a regression model with $Y$ as target and X1 and X2 as predictor variables. Roles: Y is the target, X1 the variable of interest, X2 a confounder of X1 and X2, X3 a mediator to the effect of X1 on Y, and X4 is a collider."}
library(ggplot2)
p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 22, color = "black") +
  geom_point(size = 20, aes(color = role)) +
  geom_text(aes(label = label), color = 'white') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  scale_color_manual(values = c("grey", "black", "darkgrey"), guide = "none") +
  theme_void()

add_cc(p)
```

<!-- Step 3: Estimation -->
For the estimation, we need to make assumptions about how the random variable are (conditionally) distributed.
But really, we are in the frequentist/Bayesian mindset again.
But it could also be supervised machine learning.
And you can read up on those chapters again, so I don't have to repeat myself here.
Then, usually for the interpretation, you interpret the regression coefficient as you normally would.
Except that you may interpret the effect as a cause.
If you are interested in different variables, you have to repeat all the steps for all variables, as you might need different models.


## Strengths

* Arguably, causality is central to modeling the world. Causal inference is the mindset that takes causes to heart.
* Causal inference allows to argue causal questions: An important ingredient for generalization.
* I'd say in most cases the modeler actually wants causality. Scientists make causal interpretations. In marketing you want to know how measures impact customers causally.
* Causal models are more robust against changes in the environment. Or rather: Models that use non-causal, merely associative information might break more easily. For example: Google had a flu detector which predicted flu waves based on searches for flu-related terms. But searches are not causal, so the model broke.
* The mindset is flexible and can enhance many other mindsets such as frequentism, Bayesianism, machine learning. Inherits strengths from the statistical modeling approach it builds upon.
* DAGs make causal assumptions explicit. I would go as far and say that the subtext of many frequentist and Bayesian analyses is already a causal one. Or at least once the results get out of the hand of the modeller, the journalists, manager, decision maker and so on are likely to make causal interpretation anyway. Causal inference talks explicitly about causality, and especially about the causal structure assumptions that were made for the model. These can then be challenged and discussed.
 
## Limitations

* There is no commonly accepted way to do causal inference yet <!-- citation needed -->. Do calculus by Pearl and co is a strong contender though.
* Some people that causality does not exist <!-- citation needed -->
* Building the causal model is an inherently subjective task. The causalist can never be sure whether the causal model is correct. Some assumptions about causal directions and arrows can never be verified.
* Confounders, causes of both variable of interest and target,  are especially tricky. For a causal interpretation, you have to assume that you found all the confounders. But you can't prove that you have.
* Inherits limitations from the statistical modeling approach it builds upon (except for lack of causality).
* Using non-causal variables can enhance predictive performance, but make your model non-causal. So causal inference is not with predictive power first.
* Causal inference assumes directionality, as becomes very visual in DAGs, which even have the word directed in it. So, no feedback loops are allowed. This is a strong restrictions. Often potential feedback loops can be built around by introducing time: Instead of saying that the populations of predators and prey are in a feedback loop, you operationalize these two variables as four: Predator at time $t-1$ causally affects predator at time $t$; prey at time $t-1$ also causally affects predator and prey at time $t$ .

## Further Reading

* 

[^adjust-inflammation]: The attentive reader might object that I referred to inflammation now as both confounder and mediator. Both is correct, if we distinguish different time points. The initial model had inflammation after treatment begin as variable, so it acted as mediator. We later also adjusted the model for inflammation before treatment, when it acts a confounder.

