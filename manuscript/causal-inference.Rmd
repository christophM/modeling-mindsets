# Causal Inference {#causal-inference}

* Causal models place random variables into cause-and-effect relationships.
* A model is a good generalization of the world if it encodes causality.
* Causal inference is not a standalone modeling mindset, but causal models are either integrated or translated into a [statistical](#statistical-modeling) or [machine learning](#machine-learning) models.

<!-- TODO 

- Visualize Fork, Pipe, Collider, Descendant, see p. 185 in statistical rethinking 
- add personal story at the beginning
- Add some story about causalists to the beginning?
- write about randomized tests / A-B tests, but more as in that  I describe it in another mindset
-->

"Thank you so much for this statistical model," the ecologist says to the statistician, and continues, "Nice p-values, and insightful findings! Can I conclude from the model that droughts **caused** the crop failures?"
The statistician looks at the ecologist, a hint of concern at the corner of the eyes.
As if practiced, the statistician says: "Correlation does not imply causation".
Unsatisfied, the ecologist responds: "But it would make so much sense to conclude that the drought was the cause!"
The statistician grimaces and clenches the teeth as if in pain.
"Correlation does not imply causation", the statistician repeats, the words having a weird melody, as if in prayer.
"But how can we advance science without causality? I want to understand **why** the crop failures happened!", the ecologist insists.
"Correlation does not imply causation. Correlation does not imply causation. Correlation ...", the statistician now chants, eyes closed shut, face twisted as if in great pain.
The ecologist slowly retreats, shocked by the strange reaction of the statistician.
Sometimes at night, when everything is silent, the ecologist can still hear the haunting calls of the statistician.

## Causality for the Rescue

<!-- SCQM story -->

A while back, I worked with a rheumatologist on an important medical question:
Do TNF-alpha blockers reduce the long-term symptoms  of patients with axial spondyloarthritis, a chronic disease that is associated with inflammation of the spine.
In the long-term, the joints in the spine can fuse due to new bone formation (ossification). 
TNF-alpha blockers, given regularly as injection or infusion, work really well to reduce inflammation.
To understand whether TNF-alpha blockers also help against the ossification, a clinical trial might have given the best evidence.
But withholding TNF-alpha blockers would be unethical due to their proven efficacy, and also the study would require a long-term observation.
The next best option was to use observational data from hospitals and medical practices.
The registry for rheumatic patients  I was working for maintained a huge database of Swiss patients with axial spondyloarthritis.
This database was holding insightful data about the patients health-related history: doctor visits, blood values, x-ray images, medication history, and so on.
In collaboration with the rheumatologist I fitted a statistical model to answer whether TNF-alpha blockers help against ossification.
For these patients, we had x-ray images of the spine that were two years apart, from which radiologists had quantified the progression of bone formation.
To predict the progression, the model included various variables that were measured at the time of the first x-ray: the age of the patients, the disease duration, inflammation levels, medication used, and so on.
The result of the analysis was that the drug didn't reduce ossification.
This finding was somewhat in line with preliminary research of others.

By chance, the lead statistician of the patient registry happened to participate in a course on causal inference around the same time.
She had the epiphany that we approached the modeling question the wrong way.
She drew a diagram visualizing how the drug, the inflammation and the ossification might be related.
Figure \@ref(fig:tnfdag) shows a reduced version of this graph:

```{r tnfdag, fig.cap = "The drug was known to influence inflammation (reduce it). Inflammation was thought to cause ossification (new bone formation). More inflammation, more ossification. The drug has, potentially, two pathways to reduce ossification: directly, or indirectly via reducing inflammation.", fig.height = 3, fig.width = 9}
arr = arrow()
nodes = data.frame(x = c(1, 2, 3),
                   y = c(1, 2, 1),
                   label = c("Drug", "Inflamed\nSpine", "New\nBone"))

edges = data.frame(x =    c(1, 2, 1),
                   xend = c(2, 3, 3),
                   y =    c(1, 2, 1),
                   yend = c(2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.7, 2.5)
edges$xend2 = edges$xend + c(-radius, - radius, -radius)
edges$yend2 = edges$yend + c(-radius, radius, 0)
edges$x2 = edges$x + c(radius, radius, radius)
edges$y2 = edges$y +  c(radius, -radius, 0)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 25, color = "darkgrey") +
  geom_point(size = 23, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  annotate("text", label = "?", x = 2, y = 1.15, size = 10) +
#  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```


<!-- the solution of the story -->
It became immediately clear what the problem with our current model was:
Inflammation was a potential mediator of the effect of TNF-alpha blockers on long-term ossification.
Figure \@ref(fig:tnfdag) shows that the effect of the drug can be split into a direct effect and an indirect effect.
The total effect is the direct effect of the drug plus all indirect effects, in this case via reducing inflammation.
We were interested in the total effect, but the way we set up the model, the coefficient for the drug could only be interpreted as the direct effect.
The indirect effect was fully reflected in the coefficient for the inflammation level, which had also added to our model.
We adjusted the model accordingly, removing the inflammation variable.[^adjust-inflammation]
After the removal, the coefficient for TNF-alpha blockers could now be interpreted as the total effect of the drug, and not only the direct effect.
Now the model clearly showed that TNF-alpha blockers reduce ossification by decreasing the inflammation levels.
It sounds like common sense in hindsight, but coming from a frequentist mindset, my mind was blown.
This moment was such a revelation and got me interested in causal inference.

## Causality

<!-- what is causality -->
<!--$X$ is a cause of $Y$ if changing $X$ changes the probability distribution $P(Y)$. -->
We all have an intuition about causality.
Rain is a possible cause for a wet lawn.
A drug can be a cause of getting healthy.
An environment policy can be a cause of reduced CO2 emissions.
Causality can be expressed as imagined interventions on random variables:
If you **force** a random variable to take on a certain value, how would the distribution of another random variable change (in the real world)?
A cause is different from association:
An association is only a statement about observation.
We know that having a  wet lawn does not *cause* your neighbours lawn to be wet.
How do we know it? Try watering your lawn for one year, every day, and see whether the probability for your neighbors lawn being wet has changed.
But the wetness of the two lawns are associated:
When you observe that your lawn is wet, the probability that your neighbors lawn is wet is high.
The reason for this association is, of course, rain.
Such shared causes are called confounders.

<!-- not for the statisticians -->
The archetypal statistician avoids speaking of causality.
At least that's my experience after getting a Bachelor and Master in statistics.
What I learned about causality in those 5 years can be summarized in two statements: 1) Always add all confounders when building a statistical model, and 2) correlation does not imply causation.
We were taught not to causally interpret statistical models, treating causality as an unreachable goal.
We were taught to ignore the elephant in the room.

<!-- a statisticians mantra goes against science -->
"Correlation does not imply causation" truly is a mantra that you hear multiple times when you learn about statistics.
I find that very curious, especially given that statistical modeling is supposed to be THE research tool of our times.
Isn't research all about detecting how the world works?
The "how", at least for me, implies that scientists are supposed to uncover causal structures.
The truth is that most results are interpreted causally anyways.
By the domain experts, by lay persons, and by the media.
Whether the statistician likes it, or not.
So, shouldn't everyone at least attempt to make the model reflect causality as much as possible?
Fortunately, some people think that we should put causality first.

Welcome to the **causal inference** mindset.

## The Causal Mindset

<!-- causal inference framework in a nutshell -->
The causal inference mindset puts causality in the center of modeling.
The goal of causal inference is to identify and quantify the **causal** effect a random variables has on the outcome of interest.

<!-- Relation to other mindsets -->
Causal inference could be seen as an "add-on" to other mindsets like [frequentist](#frequentism) or [Bayesian](#bayesian) inference, but also for [machine learning](#machine-learning).
But it would be wrong to think causal inference as just a cherry on top of other mindsets. 
It's much more than just adding a new type of method to another mindset, like adding support vector machines to supervised learning.
Causal inference challenges the culture of statistical modeling.
It requires the modelers to think more about the data-generating process, to be explicit about causes and effects.

<!-- mindsets without causality are broken -->
It's kind of surprising just how many models are "broken" because they ignore causal thinking.
A lack of causal considerations can mean that the analysis of a research paper is invalid or that a machine learning model in a product is vulnerable to changes in the data distribution or adversarial attacks.
Take the Google Flu prediction model as an example.
Google predicted outbreaks of the flu based on frequencies of certain search terms.
Clearly, the prediction model was not a causal model.
If it were causal, it would mean that you can cause flu outbreaks by searching on Google for certain terms.
The flu detection model missed, for example, the nonseasonal 2009 flu. [@lazer2014parable]
The machine learning model quickly declined in performance because the search patterns changed over time.
No causalist would have signed off on such a non-causal model.
A model that relies on only associations is as ephemeral as a fruit fly.
A model only generalizes well when it encodes causal relationships.
A causal flu model might rely on the virulence of the current flu strains, the number of vaccinated people, forecasts of how cold the winter will be, and so on.

<!-- data can't speak for itself -->
You can look as hard at the data as you want to, but it won't reveal the causal structures that produced it.
You can automatically infer associations from the data, but even the simplest causal structures are ambiguous.
The amount of sunshine on a given day can be considered causal for the number of park visitors.
In a dataset, both features would appear as columns with numbers in it.
And if we would compute the correlation, we would find out that sunshine and park visitors are positively correlated.
The more sun, the more people.
The more people, the more sun.
The causal relationship is clear to any human:
Park visitors don't control the sunshine, even the smokiest BBQ won't produce enough clouds to change the hours of sunshine.
The sun is a cause of park visits.
But this causal direction is not clear for your computer.
No matter which of the variables you choose as the target, the computer will oblige and fit the model.
*Breaking news: The government forbid visits to the park, in an effort to cool down the current heat wave.*

Causal directions requires making assumptions.
These assumptions are often not be testable, and therefore a subjective choice of the modeler.
This creates an attack surface for criticism of the causal inference mindset.
But on the other hand, causal inference makes causal assumptions explicit and encourages discussions.
If two people have different opinions about some causal direction, when they both adopted the causal inference mindset, they have a shared way of communicating about these difference.

Let's have a look at the best way to make causal structures explicit: The directed acyclic graph.

## Directed Acyclic Graph (DAG)

<!-- short DAG intro -->
Causal inference comes with a tool to visualize causal relationships: Directed Acyclic Graphs, or short, DAGs.
A DAG, such as the one in Figure \@ref(fig:dag) makes it simple to understand which variable is a cause to another variable.
Variables are visualized as nodes and the causal direction is visualized with an arrow.
DAGs have to be acyclic, meaning arrows are not allowed to go in circles.
For example, adding an arrow from $Y$ to $X_1$ in Figure \@ref(fig:dag) would make the DAG cyclic and most causal frameworks can't handle that.  

```{r dag, fig.cap = "A directed acyclic graph (DAG) with 5 variables.", out.width = "\\textwidth", cache = TRUE}
arr = arrow()
nodes = data.frame(x = c(1, 3, 2, 2, 2),
                   y = c(2, 2, 3, 2, 1),
                   label = c("Y", "X1", "X2", "X3", "X4"),
                   role = c("target", "include", "include", "exclude", "exclude"))

edges = data.frame(x =    c(2, 2, 2, 3, 1, 3),
                   xend = c(1, 3, 1, 2, 2, 2),
                   y =    c(3, 3, 2, 2, 2, 2),
                   yend = c(2, 2, 2, 2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.9, 3.1)
edges$xend2 = edges$xend + c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$yend2 = edges$yend + c(radius, radius, 0, 0, radius, radius)
edges$x2 = edges$x - c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$y2 = edges$y -  c(radius, radius, 0, 0, radius, radius)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 21, color = "darkgrey") +
  geom_point(size = 20, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```

<!-- Structures in a DAG -->
What can we see from the DAG in Figure \@ref(fig:dag)?
Variables $X_2$ and $X_3$ are direct causes for target $Y$.
$X_1$ only indirectly influences $Y$ via $X_3$.
And $X_4$ is not a cause of $Y$, but instead $Y$, together with $X_1$, causes $X_4$. 


<!--
* Elements: chain, fork, collider
* Roles of variables: confounders, mediators
* Also conditional dependence and conditional independence
* Conditioning model on a variable can make two variables conditional dependent to independent or vice versa.
* Goal: When modeling from, e.g. X1 to Y, make sure some paths are blocked
* A variable is seen as a cause of another variable if changing it's value changes the value of the other variable
* DAG is a tool to discuss causality
* Also a tool to detect which variables to condition on and which not in your model
* A part of causal inference is just about the identification of DAGs
* While you can't fully discover causal relationships, it's possible to make some progress
* For example, X -> Y and Y <- X would have same correlation in data.
* But you can detectd when X and Y are independent just from data
* We can see that $X_3$ is not only a cause of $Y$, but also of $X_2$, therefore being a confounder to $Y$.
-->

But how do we know where to put arrows, and in which direction to point them?
There are various approaches:

* Good old common sense, such as knowing that park visitors don't control the sun.
* Domain expertise
* Direction of time: We know that the elevator comes because you pushed the button and not the other way around.
* Causal structure learning: To some degree, we can automatically learn causal structures. But usually this results in multiple, ambiguous DAGs.

The DAG is a great tool to build causal models, but not all approach rely on DAGs.

## Many Frameworks For Causality

There are confusingly many "schools", frameworks and individual models for causal inference.
And they can differ in notation and approaches. [@hernan2010causal]
I found this lack of unity to be the biggest entry barrier to the causal inference mindset.
You can choose different introductory books to Bayesian inference, and the basic notation, language and presented methods will be mostly the same.
But for causal modeling, it's more diverse.
So don't despair too much, it's not you, it's causal inference.
Anyhow, here is a short, non-exhaustive overview of causal modeling approaches, to give you an idea of what's out there:

* A huge part of causal inference is more about designing experiments rather than causal modeling of observational data, such as clinical trials or A/B-tests. Claims to causality are derived from randomization and intervention.
* Observational data can similar to an experiment, which some people call "natural experiments". When John Snow investigated cholera, he had access to data from a natural experiment. John Snow identified contaminated drinking water as the source of cholera, because the customers of one water company got sick of cholera much more often than customers from the other.
* Propensity score matching attempts to estimate the effect of an intervention by matching data points to account for differences in other variables.
* Probably the most general and coherent framework of causal inference is by the statistician Judea Pearl. This "school" includes the do-calculus[@pearl2012calculus],   structural causal models, front- and backdoor criteria and many other tools for causal inference. [@pearl2009causal]
* The potential outcomes framework [@athey2016recursive] is another larger causal "school", mostly used for studying causal effects of binary variables.
* Causal discovery or structure identification is a subset of causal inference that aims to construct DAGs from observational data.
* Mediation analysis allows to study how causal effects are mediated by other variables.
* There are many individual methods that aim to provide causal modeling. One example is "honest causal forests", which are based on random forests and designed to model heterogeneity in treatment effects.[@athey2016recursive]
* ...

All approaches have in common that they assume a causal model.
This causal model can be very explicit, for example if it involves drawing a DAG.
But it could also be more hidden within the assumptions of some method about which variable to include in the model and so on.
The final estimate, however, is always a plain statistical estimator, or a machine learning model or so.
But how do we get from a causal model to a statistical estimator?


## From Causal Model to Statistical Estimator

<!-- observational data it is -->
In many cases we can't do experiments because they are infeasible, too expensive, or too time-consuming.
But often we have observational data from which we want to infer causal effects.
With observational data, the first casualty is causality -- at least from the point of view of non-causalists.
When they see observational data, causal modelers start to stretch and warm up their wrists in anticipation of all the DAG-drawing and modeling to come.

<!-- from causal to observational -->
Causal modelers claim that you can estimate causal effects even for observational data.
I am willing to reveal their secret:
Causal modelers use high-energy particle colliders to produce black holes.
Each black hole contains a parallel universe that lets them study what-if scenarios.
Joke aside, there is no magical ingredient for estimating causal effects.
Causal modeling is mostly a recipe to translate causal models into statistical estimators, roughly following these 4 steps:[@pearl2009causal]


1. Formulate causal estimand.
1. Construct causal model.
1. Identify statistical model.
1. Estimate effect.


<!-- Step 1: Causal estimand-->
Let's have a look at the individual steps.
The step is to formulate the causal estimand.
That means defining the causes and effects that we are interested in.
The estimand can be the effect of a treatment on a disease outcome.
It can be the causal influence of supermarket layout on shopping behavior.
Or it can be question of how much climate change can be attributed to a specific heat wave.

<!-- Step 2: Causal Model -->
Once the causal estimand is formulated, we can derive the causal model.
The causal model can be build in the form of DAG, but it doesn't have to.
Besides the target and the potential causes, all other variables that are relevant to both should be included as nodes in the causal model.
Then the modeler has to draw the causal relationships as arrows to finish the DAG.
Not all approaches and frameworks will encourage or require to draw such a DAG.
But I'd argue that, at least implicitly, the DAG is always in the causal modelers head, and a DAGs is the most meaningful "storage" and visualization of causal models.

<!-- Step 3: Identify -->
In the identification step the causal modeler translates the causal model into a statistical estimator.
Not all causal estimands can be estimated with observational data.
Especially if a confounder is missing, maybe it wasn't measured, then we can't estimate the causal effect.
Identification can be a complicated, but there are also many simple rules that tell you which variables to include in the statistical model and which to exclude:

* Include all confounders. Confounders are variables that are cause to both the variable of interest and the outcome. For example in Figure \@ref(fig:dag-rules) $X_2$ confounds $X_1$ and $Y$.
* Exclude colliders. $X_4$ is a collider for $Y$ and $X_1$. Adding colliders to a model opens an unwanted path.
* Exclude mediators. If we want to measure the causal effect of $X_1$ on $Y$, we have to exclude $X_3$. Including $X_3$ would block the path between $X_1$ and $Y$, and we would falsely find that $X_1$ does not influence $Y$.

```{r dag-rules, dependson = "dag", fig.cap = "To understand the causal effect of X1 on Y, we have to build a regression model with $Y$ as target and $X_1$ and $X_2$ as predictor variables."}
library(ggplot2)
p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 22, color = "black") +
  geom_point(size = 20, aes(color = role)) +
  geom_text(aes(label = label), color = 'white') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  scale_color_manual(values = c("grey", "black", "darkgrey"), guide = "none") +
  theme_void()

add_cc(p)
```

<!-- Step 3: Estimation -->
For the estimation, we have to make assumptions distribution of the random variables.
But mostly, we estimate and interpret the model with a frequentist, Bayesian, likelihoodist or supervised learning mindset.
But of course with the additional benefit that we now may interpret the causal effect.

For estimating causal effects of other variables, all steps have to be repeated.
The reason is that the identification might lead to a different set of variables that the model has to be adjusted for. 

## Strengths

* Causality is central to modeling the world, and causal inference is **the** mindset to embrace that fact.
* I believe most modelers actually want causal models. Clearly, scientists want causal explanations to understand the world better. But also in industry, such as marketing, you want to understand how actions causally affect outcomes.
* Only causal models will generalize well, because they are more robust against changes in the environment. Or rather: Non-causal models break more easily, since they are built on associations.
* Causal inference is a rather flexible mindset that enhances many other mindsets such as frequentism, Bayesianism, machine learning.
* DAGs make causal assumptions explicit. If you only have one take-away from this chapter, or from causal inference in general, it should be DAGs as a method to think and communicate.
* You might say that causal modeling with observational data is not possible. The truth is, that models, once out of the hand of the modeler, will in many cases be interpreted causally. Then why not make an effort to introduce some of the best practices from causal inference?

## Limitations

* Many modelers stay away from causal inference for observational data, saying that causal models are either not possible or too tricky. 
* Confounders, causes of both variable of interest and target  are especially tricky. For a causal interpretation, you have to assume that you found all the confounders. But you can never prove that you have identified all confounders.
* There are many schools and approaches to causal inference. This can be very confusing for people entering the field.
* Causal modeling requires subjective decisions. The causalist can never be sure whether the causal model is correct.
* Predictive performance and causality can be in conflict: Using non-causal variables can enhance predictive performance, but make your model non-causal.

## Further Reading

* Free book: Causal Inference: What If [@hernan2010causal]

[^adjust-inflammation]: The attentive reader might object that I referred to inflammation now as both confounder and mediator. Both is correct, if we distinguish different time points. The initial model used inflammation after treatment start, so it acted as mediator. We later also adjusted the model for inflammation before treatment start, acting as a confounder.

