# Causal Inference {#causal-inference}

* A model is a good generalization of the world if it encodes causality.
* Causal models connect random variables through directed cause and effect relationships.
* Causal inference is a two-step process in which first a causal model is assumed and constructed and then translated into a [statistical](#statistical-modeling) or [machine learning](#machine-learning) model.

<!-- TODO 

- Visualize Fork, Pipe, Collider, Descendant, see p. 185 in statistical rethinking 
- add personal story at the beginning
- Add some story about causalists to the beginning?
- write about randomized tests / A-B tests, but more as in that  I describe it in another mindset
-->

"Thank you so much for this statistical model," the ecologist says to the statistician, and continues, "Nice p-values, and insightful findings! Would it be correct to say that the droughts **caused** the crop failures?"
The statistician looks at the ecologist, a hint of concern at the corner of the eyes.
As if practiced, the statistician says: "Correlation does not imply causation".
Unsatisfied, the ecologist responds: "But it would make so much sense to conclude that the drought was the cause!"
The statistician grimaces and clenches the teeth as if in pain.
"Correlation does not imply causation", the statistician repeats, the words having a weird melody, as if in prayer.
"But without a causal interpretation, how can the results be applied to advance science? I want to understand **why** the crop failures happened!", the ecologist insists.
"Correlation does not imply causation. Correlation does not imply causation. Correlation ...", the statistician now chants, eyes closed shut, face twisted as if in great pain.
The ecologist slowly retreats, shocked by the strong reactions of the statistician.
Sometimes at night, when the wind howls outside, the ecologist hears the statistician's mantra in the wind.

## Does The Drug Help?   

<!-- SCQM story -->

A while back, I worked with a rheumatologist on an important medical question:
Do TNF-alpha blockers reduce the long-term symptoms  of patients with axial spondyloarthritis, a chronic disease that is associated with inflammation of the spine.
In the long-term, the joints in the spine can fuse to due to new bone formation (ossification). 
TNF-alpha blockers, given regularly as injection or infusion, work really well to reduce inflammation.
To understand whether TNF-alpha blockers also help against the ossification, a clinical trial might have given the best evidence.
But withholding TNF-alpha blockers would be unethical due to their proven efficacy, and also the study would require a long-term observation.
The next best option was to use observational data from hospitals and medical practices.
The registry for rheumatic patients  I was working for maintained a huge data base of patients with axial spondyloarthritis, holding insightful data about the patients health-related history: doctor visits, blood values, x-ray images, and so on.
In collaboration with the rheumatologist I built a statistical model to answer whether TNF-alpha blockers help against ossification.
For these patients, we had x-ray images two years apart, from which radiologists had scored the progression of the new bone formation in the spine.
To predict the progression, the model included various variables that were measured at the time of the first x-ray: the age of the patients, the disease duration, inflammation levels, medication used, and so on.
The result of the analysis was that the drug didn't reduce ossification.
The lead statistician of the patient registry happened to participate in a course on causal inference around the same time.
She had the epiphany that we approached the modeling question the wrong way.
She drew a diagram visualizing how the drug, the inflammation and the ossification might be related.
She drew the graph like this (Figure \@ref(fig:tnfdag):

```{r tnfdag, fig.cap = "The drug was known to influence inflammation (reduce it). Inflammation was thought to cause ossification (new bone formation). More inflammation, more ossification. The drug has, potentially, two pathways to reduce ossification: directly, or indirectly via reducing inflammation.", fig.height = 3, fig.width = 9}
arr = arrow()
nodes = data.frame(x = c(1, 2, 3),
                   y = c(1, 2, 1),
                   label = c("Drug", "Inflamed\nSpine", "New\nBone"))

edges = data.frame(x =    c(1, 2, 1),
                   xend = c(2, 3, 3),
                   y =    c(1, 2, 1),
                   yend = c(2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.7, 2.5)
edges$xend2 = edges$xend + c(-radius, - radius, -radius)
edges$yend2 = edges$yend + c(-radius, radius, 0)
edges$x2 = edges$x + c(radius, radius, radius)
edges$y2 = edges$y +  c(radius, -radius, 0)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 25, color = "darkgrey") +
  geom_point(size = 23, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  annotate("text", label = "?", x = 2, y = 1.15, size = 10) +
#  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```


<!-- the solution of the story -->
It became immediately clear what the problem with our current model was:
Inflammation was a potential mediator of the effect of TNF-alpha blockers on long-term ossification.
Figure \@ref(fig:tnfdag) shows that the effect of the drug can be split into a direct effect and an indirect effect.
By having the inflammation level as part of the model, the indirect effect of the treatment was "swallowed".
Any changes to the ossification based on reducing inflammation was reflected in the coefficient for inflammation levels.
The way we set up the initial model, the causal interpretation of the coefficient for TNF-alpha blockers concerned only the direct effect.
But we were actually interested in the total effect.
The total effect is the direct effect of the medication plus all indirect effects, in this case indirectly via reduction of inflammation.
So we adjusted the model accordingly, removing the inflammation variable. [^adjust-inflammation], so that the coefficient for TNF-alpha blockers would contain the total and not only the direct effect.
Now the model clearly showed that TNF-alpha blockers reduce ossification by decreasing the inflammation levels.
It sounds like common sense in hindsight, but coming from a frequentist mindset, my mind was blown.
This moment was such a revelation and got me interested in causal inference.

## Causality

<!-- what is causality -->
<!--$X$ is a cause of $Y$ if changing $X$ changes the probability distribution $P(Y)$. -->
We all have an intuition about causality.
Rain is a possible cause for a wet lawn.
A drug can be a cause of getting healthy.
An environment policy can be a cause of reduced CO2 emissions.
In terms of random variable, we can express causality in terms of distributions and interventions:
If you **force** a random variable to have take on a certain value, how would the distribution of another random variable change?
A cause is different from association:
An association is only a statement about observation.
We know that having a  wet lawn does not *cause* your neighbours lawn to be wet.
How do we know it? Try watering your lawn for one year, every day, and see whether the probability for your neighbors lawn being wet has changed.
But the two lawns two are associated:
When you observe that your lawn is wet, the probability that your neighbors lawn is wet is high.
The reason for association is, of course, the rain.
Such shared causes are called confounders.

<!-- not for the statisticians -->
The archetypal statistician avoids speaking of causality.
At least that's my experience after getting a Bachelor and Master in statistics.
What I learned about causality in those 5 years can be summarized in two statements: 1) Always add all confounders when building a statistical model, and 2) correlation does not imply causation.
We were taught not to causally interpret statistical models.
We were taught to ignore the elephant in the room.
Causality was presented as an unreachable goal that should not even be attempted with statistical modeling.
We were taught how to dance around the topic.
A random variable can only be associated with the outcome, but we may not speak about the variable causing the outcome.

<!-- a statisticians mantra goes against science -->
"Correlation does not imply causation" truly is a mantra that you hear multiple times when you learn about statistics.
I find that very curious, especially given that statistical modeling is supposed to be THE research tool of our times.
Isn't research all about detecting how the world works?
The "how", at least for me, implies that scientists are supposed to uncover causal structures.
And the truth is that, in the end, the results are, very often, interpreted causally, by the domain experts, by lay persons, and by the media.
So shouldn't everyone at least attempt to make the model reflect causality as much as possible?
Fortunately, some people think that we should put causality first.

Welcome to the **causal inference** mindset.

## The Causal Mindset

<!-- causal inference framework in a nutshell -->
The causal inference mindset puts causality in the center of modeling.
The goal of causal inference is to identify and quantify the **causal** effect a random variables had on the outcome of interest.

<!-- Relation to other mindsets -->
I would say that causal inference is a [statistical modeling mindset](#statistical-modeling), because it relies on probability distributions and random variables.
Causal inference could also be seen as an "add-on" to other mindsets like [frequentist](#frequentism) or [Bayesian](#bayesian) inference, but also for [machine learning](#machine-learning).
But it would be wrong to think causal inference as just a cherry on top of other mindsets. 
It's much more than just adding a new type of method to another mindset, like adding support vector machines to supervised learning.
Causal inference challenges the culture of statistical modeling.
It requires the modelers to think more about the data-generating process, to be explicit about causes and effects.

<!-- mindsets without causality are broken -->
It's kind of surprising just how many models are "broken" because they ignore causal thinking.
A lack of causal considerations can mean that the analysis of a research paper is invalid or that a machine learning model in a product is vulnerable to changes in the data distribution or adversarial attacks.
Take Google Flu prediction model as an example.
Google predicted outbreaks of the flu based on frequencies of certain search terms.
Clearly, the prediction model was not a causal model.
If it were causal, it would mean that you can cause flu outbreaks by searching on Google for certain terms.
The flu detection model missed, for example, the nonseasonal 2009 flu. [@lazer2014parable]
The machine learning model quickly declined in performance because the search patterns changed over time.
No causalist would have signed off on such a non-causal model.
A model that relies on only associations is as ephemeral as a fruit fly.
A model only generalizes well when it encodes causal relationships.
A causal flu model might rely on the virulence of the current flu strains, the number of vaccinated people, predictions of how cold the winter would be and so on.
But never based on search terms.

<!-- data can't speak for itself -->
You can look as hard at the data as you want to, but it won't reveal the causal structures that produced it.
You can automatically infer associations from the data, but even the simplest causal structures are ambiguous.
The amount of sunshine on a given day can be considered causal for the number of park visitors.
In a dataset, both features would appear as columns with numbers in it.
And if we would compute the correlation, we would find out that sunshine and park visitors are positively correlated.
The more sun, the more people.
The more people, the more sun.
The causal relationship is clear: The sun couldn't care less about park visitors.
Instead, the sun is the cause of park visits.
But this causal direction is not clear for your computer.
No matter what choose as a target, the computer will oblige and fit the model.
Breaking news: The government forbid visits to the park, in an effort to cool down the current heat wave.
The causal mindset requires thinking even more about the data-generating process, making assumptions about causal relationships.
These assumptions are an attack surface to criticize the mindset.
But on the other hand, making these assumptions explicit allows to address and discuss deviating opinions about causal directions.

And the best way to make causal structures explicit is the directed acyclic graph.

## Directed Acyclic Graph

<!-- short DAG intro -->
Causal inference comes with a tool to visualize causal relationships: Directed Acyclic Graphs, or short, DAGs.
A DAG, such as the one in Figure \@ref(fig:dag) makes it simple to understand which variable is a cause to another variable.
Variables are visualized as nodes and the causal direction is visualized with an arrow.
DAGs have to be acyclic, meaning arrows are not allowed to go in circles.
For example, adding an arrow from $Y$ to $X_1$ in Figure \@ref(fig:dag) would make the DAG cyclic and most causal frameworks can't handle that.  

```{r dag, fig.cap = "A directed acyclic graph (DAG) with 5 variables.", out.width = "\\textwidth", cache = TRUE}
arr = arrow()
nodes = data.frame(x = c(1, 3, 2, 2, 2),
                   y = c(2, 2, 3, 2, 1),
                   label = c("Y", "X1", "X2", "X3", "X4"),
                   role = c("target", "include", "include", "exclude", "exclude"))

edges = data.frame(x =    c(2, 2, 2, 3, 1, 3),
                   xend = c(1, 3, 1, 2, 2, 2),
                   y =    c(3, 3, 2, 2, 2, 2),
                   yend = c(2, 2, 2, 2, 1, 1))
multi = 1.2
radius = 0.2 
lmts = c(0.9, 3.1)
edges$xend2 = edges$xend + c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$yend2 = edges$yend + c(radius, radius, 0, 0, radius, radius)
edges$x2 = edges$x - c(radius, -radius, multi * radius, multi * radius, -radius, radius)
edges$y2 = edges$y -  c(radius, radius, 0, 0, radius, radius)

p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 21, color = "darkgrey") +
  geom_point(size = 20, color = "lightgrey") +
  geom_text(aes(label = label), color = 'black') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  theme_void()

add_cc(p)
```

<!-- Structures in a DAG -->
What can we see from the DAG in Figure \@ref(fig:dag)?
Variables $X_2$ and $X_3$ are direct causes for target $Y$.
$X_1$ only indirectly influence $Y$ via $X_3$.
And $X_4$ is not a cause of $Y$, but instead $Y$, together with $X_1$ causes $X_4$. 


<!--
* Elements: chain, fork, collider
* Roles of variables: confounders, mediators
* Also conditional dependence and conditional independence
* Conditioning model on a variable can make two variables conditional dependent to independent or vice versa.
* Goal: When modeling from, e.g. X1 to Y, make sure some paths are blocked
* A variable is seen as a cause of another variable if changing it's value changes the value of the other variable
* DAG is a tool to discuss causality
* Also a tool to detect which variables to condition on and which not in your model
* A part of causal inference is just about the identification of DAGs
* While you can't fully discover causal relationships, it's possible to make some progress
* For example, X -> Y and Y <- X would have same correlation in data.
* But you can detectd when X and Y are independent just from data
* We can see that $X_3$ is not only a cause of $Y$, but also of $X_2$, therefore being a confounder to $Y$.
-->

But how do we know where to put arrows, and in which direction to point them?

* Good old common sense, such as knowing that park visitors can't be the cause of more or less sunshine.
* Domain experts can often tell you in more specific cases what the most up to date knowledge about causal directions is. 
* Direction of time: We know that the elevator comes because you pushed the button and not the other way around.
* Causal structure learning: To some degree, we can automatically learn causal structures. But usually this results in sets of DAGs that might be plausible, leaving the user still with putting in some assumptions.

<!-- subjectivity -->
Building a causal model, be it in the form of a DAG or otherwise, requires to make assumptions.
These assumptions might not be testable, and therefore a subjective choice of the modeler.
That's what causal inference gets criticized for as well, even to the degree that causality can't be known.
But even if we can't know for sure if we got the right causal model, we can at least sit around a DAG, and point the fingers at the arrows we disagree with.

## Many Frameworks For Causality

There are many "schools" or frameworks of causal inference, each with their own notation and approaches. [@hernan2010causal]
That's what I found to be the biggest entry barrier to the causal inference mindset.
If you want to get into, say Bayesian inference, you can take any Bayesian introduction book and they will share a canon, a common set of tools and a shared language.
But for causal inference, the field is much more split.
So don't despair too much, it's not you, it's the causal inference field.
Anyhow, here is a short overview of approaches.
The overview is far from exhaustive, but should give you a better impression of what's out there:

* A huge part of causal inference is more about designing experiments rather than causal model for observational data, such as clinical trials or A/B-tests. Claims to causality are derived from randomization and intervention instead of causal modeling.
* Sometimes observational data can also have the character of an experiment, which is often called "natural experiments". When John Snow investigated cholera, he had access to data from a natural experiment. John Snow identified contaminated drinking water as the source of cholera, because the customers of one water company got sick of cholera much more often than customers from the other. The association of households to one company or the other served as a natural experiment, randomizing factors such as age, comorbidities, education, and so on.
* Propensity score matching attempt to to estimate the effect of an invention, such as a treatment, by matching data points to account for differences in other variables.
* Probably the most general and coherent framework of causal inference is by the statistician Judea Pearl. This "school" includes the do-calculus[@pearl2012calculus],   structural causal models, front- and backdoor criteria and many other tools for causal inference. [@pearl2009causal]
* The **potential outcomes framework** [@athey2016recursive] is another larger causal "school", mostly used for studying causal effects of binary variables.
* Causal discovery or structure identification is a subset of causal inference that aims to discover causal relationships from merely observational data.
* Then there are so many individual methods that aim to provide causal modeling. One example is "honest causal forests", which are based on random forests and designed to model heterogeneity in treatment effects.[@athey2016recursive]
* ...

All approaches have in common that they assume a causal model.
This causal model can be very explicit, for example if it involves drawing a DAG.
But it could also be more hidden within the assumptions of some method about which variable to include in the model and so on.
The final estimate, however, is always a plain statistical estimator, or a machine learning model or so.
But how do we get from a causal model to a statistical estimator?


## From Causal Model to Statistical Estimator

<!-- observational data it is -->
For this section, we assume that, for some reason, we can't do an experiment.
Instead, we have observational data, for which we want to do causal inference.
With observational data, the first casualty is causality -- at least from the point of view of non-causalists.
Observational data is when causalists get excited and start stretch their hand-wrists to warm up for all the DAG-drawing and modeling.

<!-- from causal to observational -->
Causalists claim that you can estimate causal effects, even for merely observational data.
I am willing to reveal their secret:
Causalists use large hadron colliders to collide particles with high energy -- producing black holes in the process. 
Each black hole contains a parallel universe that lets them study what if scenarios.
Joke aside, there is no magical ingredient for estimating causal effects.
Causal modeling is mostly a recipe to translate causal models into statistical estimators:[@pearl2009causal]


1. Formulate causal estimand, like: What is the causal effect of a medication on disease cure?
1. Build a causal model: More concretely, identify relevant variables, draw a DAG and so on. 
1. Identify whether causal effect can be estimated.
1. Estimate the (translated) target quantity.


<!-- Step 1: Causal estimand-->
Let's have a look at the individual steps.
The first question is, what causal relationship we want to study.
This can then be expressed as some causal estimand, for example how would the air pollution be reduced when we ban car traffic in the inner city.
By choosing the target and the possible cause, we can now proceed to building the causal model.

<!-- Step 2: Causal Model -->
The causal model can be build using visual tools like the DAG.
Besides the target and the potential causal variable, all other variables that are relevant to both should be included in the causal model.
It's kind of collecting all the nodes for the DAG.
But we also need the arrows that connect the variables, with the direction of the arrows showing the causal direction.
Identifying between which variables an arrow should be and in which direction it should point can be narrowed down, in parts, automatically.
But in the end, a lot of those causal directions will be based on domain knowledge and subjectivity.
But in the end, we do have a DAG.
Not all approaches and frameworks will necessarily encourage or require to draw such a DAG, but you always have to decide on what the confounders are and so on.  

<!-- Step 3: Identify -->
In the identification step the causalists find out whether the causal estimand can be even answered with the observational data at hand.
This means that the causalist has to check whether the assumptions of the causal inference hold, given the causal model.
Not all causal effects can be estimated.
For example, we want to measure the causal effect a treatment has on a health outcome.
Both the decision to treat and the health outcome might be influence by a third variable, such as the education status of the patient.
Education, in this example, is a classic confounder.
If we haven't measured education, then we can't reliably estimate the causal effect of the treatment.
Identification can be a complicated process.
But there are also many "simple" rules that tell you how to turn a causal estimand into a conditional estimand.
Identification boils down to a selection of variables to adjust for in the statistical estimate.
To give you an idea, here are a few simple rules:

* Include all confounders. Confounders are variables that are cause to both the variable of interest and the outcome. For example in Figure \@ref(fig:dag-rules) $X_2$ confounds $X_1$ and $Y$.
* Exclude colliders. In DAG in Figure \@ref(fig:dag-rules), $X_4$ is a collider for $Y$ and $X_1$. Adding colliders to a model opens an unwanted path.
* Exclude mediators. When we want to measure the causal effect of $X_1$ on $Y$, we have make sure not to include $X_3$. Including $X_2$ would block the path between $X_1$ and $Y$, and we would falsely find that $X_1$ does not influence $Y$.

```{r dag-rules, dependson = "dag", fig.cap = "To understand the causal effect of X1 on Y, we have to build a regression model with $Y$ as target and $X_1$ and $X_2$ as predictor variables. Roles: $Y$ is the target, $X_1$ the variable of interest, $X_2$ a confounder of $X_1$ and $X_2$, $X_3$ a mediator to the effect of $X_1$ on $Y$, and $X_4$ is a collider."}
library(ggplot2)
p = ggplot(nodes, aes(x = x, y = y)) +
  geom_segment(aes(xend = xend2, yend = yend2, x = x2, y = y2), data = edges, arrow = arr, color = "black", size = 1) +
  geom_point(size = 22, color = "black") +
  geom_point(size = 20, aes(color = role)) +
  geom_text(aes(label = label), color = 'white') +
  scale_x_continuous(limits = lmts) +
  scale_y_continuous(limits = lmts) +
  scale_color_manual(values = c("grey", "black", "darkgrey"), guide = "none") +
  theme_void()

add_cc(p)
```

<!-- Step 3: Estimation -->
For the estimation, we have to make assumptions how the random variable are distributed.
But mostly, we can just estimate and interpret our model whatever mindset we are coming from, be it frequentist, Bayesian, likelihoodist inference or machine learning.
Also the interpretation of parameters is as usual.
But there is this additional benefit that we now may interpret effect as causal.

All of the causal modeling steps have to be potentially repeated if we are interested in the causal effect of another variable.
This is because, the identification might lead to a different set of variables that we have to adjust our model for.

## Strengths

* Causality is central to modeling the world, and causal inference is **the** mindset to embrace that fact.
* I believe most modelers actually want causal model. Clearly, scientists want causal explanations to understand the world better. But also in industry, such as marketing, you want to understand how actions causally affect outcomes.
* Only causal models will generalize well, because they are more robust against changes in the environment. Or rather: Non-causal models break more easily, since they are built on associations.
* Causal inference is a rather flexible mindset that enhances many other mindsets such as frequentism, Bayesianism, machine learning.
* DAGs make causal assumptions explicit. If you only have one take-away from this chapter, or from causal inference in general, it should be DAGs as a method to think and communicate.
* You might say that causal modeling with observational data is not possible. The truth is, that models, once out of the hand of the modeler, will in many cases be interpreted causally. Then why not make an effort to introduce some of the best practices from causal inference?

## Limitations

* Many modelers stay away from causal inference for observational data, saying that causal models are either not possible or too tricky. 
* Confounders, causes of both variable of interest and target  are especially tricky. For a causal interpretation, you have to assume that you found all the confounders. But you can't prove that you have identified all confounders.
* There are many schools and approaches to causal inference. This can be very confusing for people entering the field.
* Causal modeling requires subjective decisions. The causalist can never be sure whether the causal model is correct.
* Using non-causal variables can enhance predictive performance, but make your model non-causal. So causal inference and predictive performance are at odds with each other.

## Further Reading

* Free book: Causal Inference: What If [@hernan2010causal]

[^adjust-inflammation]: The attentive reader might object that I referred to inflammation now as both confounder and mediator. Both is correct, if we distinguish different time points. The initial model had inflammation after treatment begin as variable, so it acted as mediator. We later also adjusted the model for inflammation before treatment, when it acts a confounder.

