<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Likelihoodism | Modeling Mindsets</title>
  <meta name="description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Likelihoodism | Modeling Mindsets" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Likelihoodism | Modeling Mindsets" />
  
  <meta name="twitter:description" content="Statistics, machine learning, causality, … The best data scientists don’t mindlessly follow just one approach. The best data scientists have all modeling mindsets at their disposal – and use the right tool for the right job." />
  

<meta name="author" content="Christoph Molnar" />


<meta name="date" content="2022-05-11" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayesian-inference.html"/>
<link rel="next" href="causal-inference.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<link rel="stylesheet" type="text/css" href="css/cookieconsent.min.css" />
<script src="javascript/cookieconsent.min.js"></script>
<script>
window.addEventListener("load", function(){
window.cookieconsent.initialise({
  "palette": {
    "popup": {
      "background": "#000"
    },
    "button": {
      "background": "#f1d600"
    }
  },
  "position": "bottom-right",
  "content": {
    "message": "This website uses cookies for Google Analytics so that I know how many people are reading the book and which chapters are the most popular. The book website doesn't collect any personal data."
  }
})});
</script>

<style>

#cta-button-desktop:hover, #cta-button-device:hover {
  background-color:   #ffc266; 
  border-color:   #ffc266; 
  box-shadow: none;
}
#cta-button-desktop, #cta-button-device{
  color: white;
  background-color:  #ffa31a;
  text-shadow:1px 1px 0 #444;
  text-decoration: none;
  border: 2px solid  #ffa31a;
  border-radius: 10px;
  position: fixed;
  padding: 5px 10px;
  z-index: 10;
  }

#cta-button-device {
  box-shadow: 0px 10px 10px -5px rgba(194,180,190,1);
  display:none;
  right: 20px;
  bottom: 20px;
  font-size: 20px;
 }

#cta-button-desktop {
  box-shadow: 0px 20px 20px -10px rgba(194,180,190,1);
  display:display;
  padding: 8px 16px;
  right: 40px;
  bottom: 40px;
  font-size: 25px;
}

@media (max-width : 450px) {
  #cta-button-device {display:block;}
  #cta-button-desktop {display:none;}
}


</style>





<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modeling Mindsets</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Summary</a></li>
<li class="chapter" data-level="1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html"><i class="fa fa-check"></i><b>1</b> What This Book is About</a>
<ul>
<li class="chapter" data-level="1.1" data-path="what-this-book-is-about.html"><a href="what-this-book-is-about.html#who-this-book-is-for"><i class="fa fa-check"></i><b>1.1</b> Who This Book is For</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="models.html"><a href="models.html"><i class="fa fa-check"></i><b>2</b> Models</a></li>
<li class="chapter" data-level="3" data-path="mindsets.html"><a href="mindsets.html"><i class="fa fa-check"></i><b>3</b> Mindsets</a></li>
<li class="chapter" data-level="4" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>4</b> Statistical Modeling</a>
<ul>
<li class="chapter" data-level="4.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#random-variables"><i class="fa fa-check"></i><b>4.1</b> Random Variables</a></li>
<li class="chapter" data-level="4.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#probability-distributions"><i class="fa fa-check"></i><b>4.2</b> Probability Distributions</a></li>
<li class="chapter" data-level="4.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#assuming-a-distribution"><i class="fa fa-check"></i><b>4.3</b> Assuming a Distribution</a></li>
<li class="chapter" data-level="4.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-model"><i class="fa fa-check"></i><b>4.4</b> Statistical Model</a>
<ul>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#maximum-likelihood-estimation"><i class="fa fa-check"></i>Maximum Likelihood Estimation</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-hypothesis"><i class="fa fa-check"></i>Statistical Hypothesis</a></li>
<li class="chapter" data-level="" data-path="statistical-modeling.html"><a href="statistical-modeling.html#interpreting-parameters"><i class="fa fa-check"></i>Interpreting Parameters</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="statistical-modeling.html"><a href="statistical-modeling.html#joint-or-conditional-distribution"><i class="fa fa-check"></i><b>4.5</b> Joint or Conditional Distribution</a></li>
<li class="chapter" data-level="4.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html#regression-models"><i class="fa fa-check"></i><b>4.6</b> Regression Models</a></li>
<li class="chapter" data-level="4.7" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-evaluation"><i class="fa fa-check"></i><b>4.7</b> Model Evaluation</a></li>
<li class="chapter" data-level="4.8" data-path="statistical-modeling.html"><a href="statistical-modeling.html#data-generating-process-dgp"><i class="fa fa-check"></i><b>4.8</b> Data-Generating Process (DGP)</a></li>
<li class="chapter" data-level="4.9" data-path="statistical-modeling.html"><a href="statistical-modeling.html#drawing-conclusions-about-the-world"><i class="fa fa-check"></i><b>4.9</b> Drawing Conclusions About the World</a></li>
<li class="chapter" data-level="4.10" data-path="statistical-modeling.html"><a href="statistical-modeling.html#strengths"><i class="fa fa-check"></i><b>4.10</b> Strengths</a></li>
<li class="chapter" data-level="4.11" data-path="statistical-modeling.html"><a href="statistical-modeling.html#limitations"><i class="fa fa-check"></i><b>4.11</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="frequentist-inference.html"><a href="frequentist-inference.html"><i class="fa fa-check"></i><b>5</b> Frequentist Inference</a>
<ul>
<li class="chapter" data-level="5.1" data-path="frequentist-inference.html"><a href="frequentist-inference.html#frequentist-probability"><i class="fa fa-check"></i><b>5.1</b> Frequentist probability</a></li>
<li class="chapter" data-level="5.2" data-path="frequentist-inference.html"><a href="frequentist-inference.html#estimators-are-random-variables"><i class="fa fa-check"></i><b>5.2</b> Estimators are Random Variables</a></li>
<li class="chapter" data-level="5.3" data-path="frequentist-inference.html"><a href="frequentist-inference.html#null-hypothesis-significance-testing"><i class="fa fa-check"></i><b>5.3</b> Null Hypothesis Significance Testing</a></li>
<li class="chapter" data-level="5.4" data-path="frequentist-inference.html"><a href="frequentist-inference.html#confidence-intervals"><i class="fa fa-check"></i><b>5.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="5.5" data-path="frequentist-inference.html"><a href="frequentist-inference.html#strengths-1"><i class="fa fa-check"></i><b>5.5</b> Strengths</a></li>
<li class="chapter" data-level="5.6" data-path="frequentist-inference.html"><a href="frequentist-inference.html#limitations-1"><i class="fa fa-check"></i><b>5.6</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="bayesian-inference.html"><a href="bayesian-inference.html"><i class="fa fa-check"></i><b>6</b> Bayesian Inference</a>
<ul>
<li class="chapter" data-level="6.1" data-path="bayesian-inference.html"><a href="bayesian-inference.html#bayes-theorem"><i class="fa fa-check"></i><b>6.1</b> Bayes Theorem</a></li>
<li class="chapter" data-level="6.2" data-path="bayesian-inference.html"><a href="bayesian-inference.html#prior-probability"><i class="fa fa-check"></i><b>6.2</b> Prior Probability</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#picking-a-prior"><i class="fa fa-check"></i>Picking a Prior</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="bayesian-inference.html"><a href="bayesian-inference.html#likelihood"><i class="fa fa-check"></i><b>6.3</b> Likelihood</a></li>
<li class="chapter" data-level="6.4" data-path="bayesian-inference.html"><a href="bayesian-inference.html#evidence"><i class="fa fa-check"></i><b>6.4</b> Evidence</a></li>
<li class="chapter" data-level="6.5" data-path="bayesian-inference.html"><a href="bayesian-inference.html#posterior-probability-estimation"><i class="fa fa-check"></i><b>6.5</b> Posterior Probability Estimation</a>
<ul>
<li class="chapter" data-level="" data-path="bayesian-inference.html"><a href="bayesian-inference.html#sample-from-the-posterior-with-mcmc"><i class="fa fa-check"></i>Sample From the Posterior with MCMC</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="bayesian-inference.html"><a href="bayesian-inference.html#summarizing-the-posterior-samples"><i class="fa fa-check"></i><b>6.6</b> Summarizing the Posterior Samples</a></li>
<li class="chapter" data-level="6.7" data-path="bayesian-inference.html"><a href="bayesian-inference.html#from-model-to-world"><i class="fa fa-check"></i><b>6.7</b> From Model to World</a></li>
<li class="chapter" data-level="6.8" data-path="bayesian-inference.html"><a href="bayesian-inference.html#simulate-to-predict"><i class="fa fa-check"></i><b>6.8</b> Simulate to Predict</a></li>
<li class="chapter" data-level="6.9" data-path="bayesian-inference.html"><a href="bayesian-inference.html#strengths-2"><i class="fa fa-check"></i><b>6.9</b> Strengths</a></li>
<li class="chapter" data-level="6.10" data-path="bayesian-inference.html"><a href="bayesian-inference.html#limitations-2"><i class="fa fa-check"></i><b>6.10</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="likelihoodism.html"><a href="likelihoodism.html"><i class="fa fa-check"></i><b>7</b> Likelihoodism</a>
<ul>
<li class="chapter" data-level="" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-principle"><i class="fa fa-check"></i>Likelihood Principle</a></li>
<li class="chapter" data-level="" data-path="likelihoodism.html"><a href="likelihoodism.html#law-of-likelihood"><i class="fa fa-check"></i>Law of Likelihood</a></li>
<li class="chapter" data-level="" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-ratio"><i class="fa fa-check"></i>Likelihood Ratio</a></li>
<li class="chapter" data-level="" data-path="likelihoodism.html"><a href="likelihoodism.html#likelihood-intervals"><i class="fa fa-check"></i>Likelihood Intervals</a></li>
<li class="chapter" data-level="7.1" data-path="likelihoodism.html"><a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle"><i class="fa fa-check"></i><b>7.1</b> Why Frequentism Violates the Likelihood Principle</a></li>
<li class="chapter" data-level="7.2" data-path="likelihoodism.html"><a href="likelihoodism.html#stopping-criteria"><i class="fa fa-check"></i><b>7.2</b> Stopping Criteria</a></li>
<li class="chapter" data-level="7.3" data-path="likelihoodism.html"><a href="likelihoodism.html#strengths-3"><i class="fa fa-check"></i><b>7.3</b> Strengths</a></li>
<li class="chapter" data-level="7.4" data-path="likelihoodism.html"><a href="likelihoodism.html#limitations-3"><i class="fa fa-check"></i><b>7.4</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="causal-inference.html"><a href="causal-inference.html"><i class="fa fa-check"></i><b>8</b> Causal inference</a></li>
<li class="chapter" data-level="9" data-path="supervised-ml.html"><a href="supervised-ml.html"><i class="fa fa-check"></i><b>9</b> Supervised Machine Learning</a></li>
<li class="chapter" data-level="10" data-path="deep-learning.html"><a href="deep-learning.html"><i class="fa fa-check"></i><b>10</b> Deep Learning</a></li>
<li class="chapter" data-level="11" data-path="design-based-inference.html"><a href="design-based-inference.html"><i class="fa fa-check"></i><b>11</b> Design-based Inference</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li> 
<li><a href="https://christophmolnar.com/impressum/" target="_blank">Impressum</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modeling Mindsets</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">

<a id="cta-button-desktop" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank"> Buy Book </a>

<a id="cta-button-device" href="https://leanpub.com/modeling-mindsets" rel="noopener noreferrer" target="blank">Buy</a>
  

<div id="likelihoodism" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Likelihoodism<a href="likelihoodism.html#likelihoodism" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- TODO

- read: https://www.stat.fi/isi99/proceedings/arkisto/varasto/roya0578.pdf
- Link he blog by Gandenberger
- 
-->
<!-- CONTENT TO ADD 

* Instead of null hypothesis testing or beliefs about params, it's about testing one hypothesis over another
* Likelihoodism invites to compare models via likelihood
* Likelihoodism interprets data as evidence, not as means for making decisions [@richard2017statistical]
* Imagine researcher coming to a statistician with only 50 data points. Results look promising, but not significant. Statistician when frequentist will discourage researcher to collect more data as this will change the p-values.
* Likelihoodism is simple, objective.A
* Wenn zwei sich streiten freut sich der Dritte
* Seemingly: Frequentist is dominant, Bayesian tries to gain ground, but can't bc subjective elements. so everyone stays with weird frequentism.
* Frequentism: illogical, especially hypothesis testing. Bayes: subjective.
* Interpret likelihood ratios, not probabilities
* Likelihood ratio = strength of evidence
* "The significance-test approach to measuring the evidence is wrong because its dependence on the sample space leads to different answers in situations where the evidence is the same." [@richard2017statistical] 

-->
<ul>
<li>The likelihood function is all you need (law of likelihood).</li>
<li>Statistical models or hypotheses are compared by the ratio of their likelihoods.</li>
<li>A <a href="statistical-modeling.html#statistical-modeling">statistical modeling mindset</a> with <a href="#frequent-inference">frequentism</a> and <a href="bayesian-inference.html#bayesian-inference">Bayesianism</a> as alternatives.</li>
</ul>
<!-- metaphor -->
<p>A frequentist, a Bayesian, and a likelihoodist walk into a wine bar to taste wines.
The sommelier brings the first wines.
The Bayesian wants to hear the sommelier’s opinion first before “analyzing” the data – drinking the wine.
The frequentist asks the sommelier about the tasting procedure: Is the numbers of wines pre-determined? Does the tasting conclude when the customer found a suitable wine? How are subsequent wines selected? Etc.
The likelihoodist politely tells the sommelier to fuck off.</p>
<!-- problem with frequentism -->
<p><a href="frequentist-inference.html#frequentist-inference">Frequentist inference</a> has a long list of limitations.
But still, it’s the dominant mindset when it comes to statistical modeling in science and elsewhere.
Bayesian analysis has seen a rise thanks to increased computational power for sampling from posteriors with MCMC.
But using subjective prior probabilities just doesn’t sit right with many people.
Could there be another way to “reform” the frequentist mindset, but without priors?
A mindset without the flawed hypothesis testing and p-values?</p>
<p>Welcome to the <strong>likelihoodist</strong> mindset.</p>
<!-- underdog -->
<p>I studied statistics for 5 years, worked as statistician and data scientist for 3 years, then did my PhD studies for 4.5 years.
In those 12 years of statistics I never learned about likelihoodism.
It’s fair to say that likelihoodism is an underdog.
Likelihoodism lurks in the shadows while Bayesianism and Frequentism are having an epic fight.</p>
<!-- why learn about it -->
<p>Likelihoodism is the purist among the statistical modeling mindsets.
A mindset that completely focuses on the likelihood function of statistical model.
Likelihoodism is an attempt to make statistics as objective as possible.
It doesn’t rely on subjective prior probabilities, like in Bayesian inference.
Frequentist inference also has a subjective component: As we will see later in this chapter, it depends on the intention of analyst, such as stopping criteria for collecting data and how the experiment is set up.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:three-stat-mindsets"></span>
<img src="figures/three-stat-mindsets-1.png" alt="How Bayesianism, frequentism and likelihoodism overlap and differ. Figure inspired by [Greg Gandenberger](https://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/)" width="\textwidth" />
<p class="caption">
FIGURE 7.1: How Bayesianism, frequentism and likelihoodism overlap and differ. Figure inspired by <a href="https://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/">Greg Gandenberger</a>
</p>
</div>
<!-- short reminder: likelihood -->
<p><strong>All three mindsets use likelihood functions in different ways.</strong>
A short recap: The likelihood function is the same as the data density function, but with switched role of data and parameters.
Data <span class="math inline">\(X\)</span> are “fixed”, and the likelihood is a function of the parameters <span class="math inline">\(\theta\)</span> <span class="math inline">\(P(\theta | X) = P(X = x | \theta)\)</span>.<a href="#fn7" class="footnote-ref" id="fnref7"><sup>7</sup></a>
The likelihood bridges observed data and theoretic distributions.
<!-- use of the likelihood function -->
Bayesians use the likelihood function to update the prior of the parameters to the posterior distribution.
Frequentists use the likelihood to construct “imagined” experiments that teach us about long-run frequencies, such as hypothesis tests and confidence intervals.
Likelihoodists use the likelihood as evidence from the data for a given hypothesis.
Likelihoodists puts all the focus on the likelihood and thereby reject the non-likelihood elements in the other mindsets:
Likelihoodists reject priors, because they are subjective;
Likelihoodists also reject the frequentists dependence on “imagined” experiments, because they violate the likelihood principle and are, arguably, subjective as well.</p>
<!-- likelihood principle as foundation -->
<p>But what is this likelihood principle?</p>
<div id="likelihood-principle" class="section level3 unnumbered hasAnchor">
<h3>Likelihood Principle<a href="likelihoodism.html#likelihood-principle" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>“The likelihood principle asserts that two observations that generate identical likelihood functions are equivalent as evidence”<span class="citation"><sup><a href="#ref-richard2017statistical" role="doc-biblioref">8</a></sup></span>
As a consequence, all the evidence that comes from the data about a quantity of interest <span class="math inline">\(\theta\)</span> has to come from the likelihood function <span class="math inline">\(P(\theta;X)\)</span>.
If we reverse the statement: If any information about the data influences the parameters but is not part of the likelihood, then the likelihood principle is violated.</p>
<p>Let’s say we want to estimate the average waiting time for the bus.
The exponential distribution is a typical contender as distribution assumption, so we would say that the waiting times <span class="math inline">\(X \sim Exp(\lambda)\)</span>, where <span class="math inline">\(\lambda\)</span> can be interpreted as the inverse wait time, so the expected waiting time would be <span class="math inline">\(\frac{1}{\lambda}\)</span>.
We collected <span class="math inline">\(n\)</span> waiting times $x_1, , <span class="math inline">\(x_n\)</span>.
The likelihood function is <span class="math inline">\(P(\lambda; x_1, \ldots, x_n) = \lambda^n exp(-\lambda \sum_{i=n}^n x^{(i)}\)</span>.</p>
<p>In all three mindsets, it would make sense to work with this likelihood.
Bayesians would, in addition, assume a prior distribution for <span class="math inline">\(\lambda\)</span>.
The relevant question of whether the likelihood principle is violated or not is what we do with the results after.
The Bayesians get a posterior distribution for <span class="math inline">\(\lambda\)</span> which is interpreted as belief about the parameter.
The likelihoodist might report the likelihood region for the parameter <span class="math inline">\(\lambda\)</span> which can be interpreted as relative evidence of a range of parameter values compared to the maximum likelihood estimate for <span class="math inline">\(\lambda\)</span>.
Both the Bayesian and the likelihoodist adhere to the principle: All evidence from the parameter about <span class="math inline">\(\lambda\)</span> comes through the likelihood.
Bayesians use priors, but as long as they don’t contain information from the data, it’s fine. <a href="#fn8" class="footnote-ref" id="fnref8"><sup>8</sup></a></p>
<p>The frequentist might test whether the parameter is larger than a certain value.
By advising such a test, the frequentist has to “imagine” experiments, or at least assume a distribution under the null hypothesis.
Frequentist inference violates the likelihood principle, due to it’s interpretation of long-run frequencies.
We will later see a few examples of how frequentism violates the likelihood principle.</p>
<p>The likelihood principle is a consequence of 1) the sufficiency principle and 2) the conditionality principle.<span class="citation"><sup><a href="#ref-birnbaum1962foundations" role="doc-biblioref">9</a></sup></span>
The sufficiency principle says that a sufficient statistic <span class="math inline">\(S(X)\)</span> of a quantity of interest summarizes all relevant evidence from the data.
The conditionality principles says that experiments or samples that were not performed should be ignored.
Many frequentist approaches directly violate the conditionality principle.
Statistical hypothesis tests are designed around experiments that never happen.</p>
<p>Apart from priors, the law of likelihood is what distinguishes likelihoodism from Bayesian inference.</p>
</div>
<div id="law-of-likelihood" class="section level3 unnumbered hasAnchor">
<h3>Law of Likelihood<a href="likelihoodism.html#law-of-likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The law of likelihood says<span class="citation"><sup><a href="#ref-hacking1965logic" role="doc-biblioref">10</a></sup></span>:</p>
<ul>
<li>Given
<ul>
<li>Hypotheses A and B, and data <span class="math inline">\(X=x\)</span></li>
<li>Likelihood of A is larger than B: <span class="math inline">\(P_A(X=x) &gt; P_B(X=x)\)</span></li>
</ul></li>
<li>Then
<ul>
<li>Observation <span class="math inline">\(X=x\)</span> is evidence supporting A over B.</li>
<li>The likelihood ratio <span class="math inline">\(P_A(x)/P_B(x)\)</span> measures the strenght of that evidence.</li>
</ul></li>
</ul>
<p>The hypotheses can be, for example, just different parameter values of a distribution.
Going back to the bus waiting time example, A could be that <span class="math inline">\(\lambda = 1\)</span> and B could be that <span class="math inline">\(\lambda = 0.5\)</span>.
If, for example <span class="math inline">\(P(1;x)/P(0.5;x) = 4\)</span> then that’s the strenght of evidence for the statistical hypothesis that <span class="math inline">\(\lambda = 1\)</span>.
The differences in statistical hypotheses might also be more complex, such as regression models with different covariates or assumptions.</p>
<!-- stronger law -->
<p>The law of likelihood is stronger than the likelihood principle.
While the likelihood principle just says that the all evidence from the data must be in the likelihood, the law of describes how evidence looks like.
And that’s also where Bayesian and likelihoodist inference differ.
Bayesians are not guided by the law of inference, but by Bayesian updating.
But comparing likelihoods does have a place in Bayesian inference.</p>
<!-- clear vision -->
<p>The law of likelihood gives clear guidance on how to do inference:
Not through hypothesis testing, but through comparing different hypothesis using likelihood ratios.
The larger the ratio, the larger the evidence for one hypothesis over another.</p>
<!-- shortcomings -->
<p>Unfortunately, the likelihoodism mindset doesn’t entail guidance for belief or action.
The likelihood ratio can say which hypothesis is favored, when we follow the law of likelihood.
But it can’t tell us whether the evidence is enough to decide for a hypothesis or not.
The likelihood ratio also doesn’t reflect a degree of belief about the parameters.</p>
<!--

### Voltmeter Story {-}

CONTINUE HERE

There is a famous story by TODO.
It's about a frequentist statistician, and about changing the inference based on imagined experiments.

* An engineer measures voltages of electron tubes. The results range from 30V to 99V.
* The statistician computes mean and 95% confidence interval of the measurements.
* Later the statistician finds out that the voltmeter only goes to 100 Volts. Censored data. That means that the analysis has to change. The statistician cannot assume a Gaussian, but has to assume a distribution that is cut off at 100. Like a trimmed Gaussian.
* So far, also the likelihoodist would agree (except that they wouldn't compute confidence intervals).  
* But then the engineer also says that she has another voltmeter that goes to 1000 Volt. If the other voltmeter had maxed out, she would have switched to the 1000V one.
* Alright, back to the non-censored analysis from the beginning. I think here every type of statistician would also agree.
* A few day later it turns out that the 1000V voltmeter was broken. And the engineer admits that in that case she wouldn't have waited with the measurements and just reported 100V.     
* For the likelihoodist, this new information doesn't change anything. In this one experiment, no measurement maxed out, so the analysis can stay the same.
* The frequentist faces a more complex modeling problem now, because the imagined future experiment are now weirdly conditional on the probability that the 1000V voltmeter works.
* Remember the confidence interval interpretation: "Would we repeat the experiment multiple times, the confidence interval has a 95% chance to cover the true mean. With this new information about the experiment ("would have reported the censored measurements if 1000V voltmeter broken") the confidence interval has to involve the probability. 
* Repeating the experiment now involves the probability of the 1000V voltmeter working. It's a mixture of two distributions: If the 1000V device works the statistician would model the data with a Gaussian distribution. If 1000V voltmeter is broken, the statistician would model the data with a trimmed Gaussian distribution.
* This couldn't even be estimated, since we don't know the probability that it works or not, and also can't estimate it.
* It's an ugly situation.
* Truth to be told. Most frequentist statistician would be pragmatic enough to ignore it and just decide for the uncensored model and pretend that this is what the future "imagined" experiments look like.
* Nonetheless, there are still 3 options for confidence interval with different interpretations in the end.
* The experimental setup decide on how to do analysis

-->
<!-- 
## Modeling and Likelihood Ratios

When likelihoodists build models, it is very similar to how frequentist build their models.
The big difference is in the interpetation of the models and how likehoodists deal with results.
Or rather, it's in what likelihoodists do NOT do.
Likelihoodists reject the idea of long-run frequences as the frequentists rely on.
This also means that likelihoodists don't use hypothesis tests, p-value, confidence intervals and so on.
Likelihoodists also reject the Bayesian interpretation of probability.
They can't rely on posterior distributions, and their interpretation as beliefs about the parameters.
How then, can likelihoodists say anything about the world with their models?

When all is contained in the likelihood, then it is obvious that we should be comparing likelihoods.
For likelihoodists, building a model means creating a hypothesis.
The hypothesis can be: Daily vitamin D intake reduces the number of colds per year.
An alternative hypothesis could be: Daily vitamin D intake reduces the number of colds per year, but also whether the person is a couch potatoe or very active.
Both hypotheses can be modelled.
Let's say $Y$ is the number of colds, $X_1$ is the binary information whether someone takes vitamin D (yes/no) and $X_2$ the random variable representing whether the person is a couch potatoe (potatoe/active). 
As the outcome is a count, the statistician chooses a Poisson distribution for the outcome:

$$Y | X_1, X_2 \sim Poi(\lambda)$$ 

The parameter $\lambda$ describes the intensity of how often colds occur.  
To connect the random variables $X_1$ and $X_2$ with the outcome, we say that the expected outcome is a function of the random variables.

$$E(Y | X_1, X_2) = e^{\beta_0 + \beta_1 X_1}$$

That's for the first model, where we only have vitamin D as variable.
The other model also contains another variable:

$$E(Y | X_1, X_2) = e^{\beta_0 + \beta_1 X_1 + \beta_2 X_2}$$

Both can then be optimized with a maximum likelihood appraoch, as presented in [chapter statistical modeling](#statistical-modeling).
This gives us the best fitting parameters.
But it gives us something else as well.
And we haven't talked about it yet.
We get the likelihood.
Which is a number that can be interpreted as the likelihood of the model and its parameterization.
This means for our made-up example, we get two models with different parameter estimates and also two likelihood numbers.

This brings us to likelihood ratios.
It's a simple equation: We divide the likelihood of model for hypothesis $H_1$ by the likelihood of model hypothesis $H_2$:

$$\frac{L(H_1|X)}{L(H_2|X)}$$

This allows one to compare the likelihood of two hypothesis.
A likelihood ratio larger than 1 favors $H_1$, a ratio smaller than 1 favors $H_2$.

-->
</div>
<div id="likelihood-ratio" class="section level3 unnumbered hasAnchor">
<h3>Likelihood Ratio<a href="likelihoodism.html#likelihood-ratio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The likelihood ratio is simply the likelihood of one model divided by the likelihood of another.
Remember, a statistical model can embody a statistcal hypothesis.
If we want to compare hypotheses <span class="math inline">\(H_1\)</span> and <span class="math inline">\(H_2\)</span>, we can use the likelihood ratio:</p>
<p><span class="math display">\[\frac{P(H_1;X)}{P(H_2;X)}\]</span></p>
<p>Likelihood ratios are also used in Bayesianism and frequentism.
In Bayesianism as Bayes factor, also for model selection.
In frequentism likelihood rations are often used as test statistics for hypothesis tests.</p>
<p>In likelihoodism, the likelihood ratio is interpreted as evidence.
As a shortcut, likelihoodists can use rule of thumbs for deciding whether likelihood ratios is already big enough.
For example a likelihood ratio of 8 is seen as fairly strong, and 32 or more as strong favoring.<span class="citation"><sup><a href="#ref-richard2017statistical" role="doc-biblioref">8</a></sup></span></p>
</div>
<div id="likelihood-intervals" class="section level3 unnumbered hasAnchor">
<h3>Likelihood Intervals<a href="likelihoodism.html#likelihood-intervals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- using likelihood intervals -->
<p>Likelihood intervals or regions are the likelihoodist analogue to frequentist confidence interval and Bayesian credible intervals.
The interpretation of likelihood regions is in terms of, you guessed it, relative likelihood.
The likelihood interval of a model parameter <span class="math inline">\(\theta\)</span> (like a coefficient in a linear model) is the set of all <span class="math inline">\(\theta\)</span> values that yield a relative likelihood greater than a certain threshold:</p>
<p><span class="math display">\[\left\{\theta: \frac{L(\theta| X)}{L(\hat{\theta}| X)} \geq \frac{p}{100}\right\}\]</span></p>
<p>The <span class="math inline">\(\hat{\theta}\)</span> is the “optimal” parameter that the modeler found using maximum likelihood (or a similar optimization methods).
Let’s say for a logistic regression model coefficient <span class="math inline">\(\beta_j\)</span>: <span class="math inline">\(\hat{\beta}_j = 1.1\)</span>. <!-- fix vim_ -->
Then an interval might be <span class="math inline">\([0.9; 1.3]\)</span>.
The constant <span class="math inline">\(p\)</span> serves a similar function like the alpha level for confidence intervals:
It decides the size of the region.
See Figure <a href="likelihoodism.html#fig:likelihood-interval">7.2</a>.
Any point within that interval can be seen as a different hypothesis.
And these hypotheses are compared to the hypothesis with the model that <span class="math inline">\(\theta = \hat{\theta}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihood-interval"></span>
<img src="figures/likelihood-interval-1.png" alt="A 1/4 and a 1/16 likelihood ratio interval." width="\textwidth" />
<p class="caption">
FIGURE 7.2: A 1/4 and a 1/16 likelihood ratio interval.
</p>
</div>
</div>
<div id="why-frequentism-violates-the-likelihood-principle" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Why Frequentism Violates the Likelihood Principle<a href="likelihoodism.html#why-frequentism-violates-the-likelihood-principle" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In short, many frequentist methods violates the likelihood principle because they require “imagined” experiments.
Experiments that never happened, but we require their theoretic distribution to compute, for example, p-values.
It’s also dependent on stopping criteria for collecting data.</p>
<!-- motivation for example -->
<p>You can skip this example if it’s too tedious.
The following example shows how frequentism violates the likelihood principle, but also a great illustration of the core idea of likelihoodism.
The criterion why experiments are stopped is a relevant factor for frequentist inference, because it does not adhere to the likelihood principle.</p>
<!-- starting coin example -->
<p>Suppose we have a coin.
We want to figure out whether it’s fair, or whether maybe head turns up too often.
We’ll call <span class="math inline">\(\theta\)</span> the probability of head.
We have two different hypothesis:</p>
<p>So: <span class="math inline">\(H_0: \theta = 0.5\)</span> und <span class="math inline">\(H_1: \theta &gt; 0.5\)</span></p>
<p>The <span class="math inline">\(H_0\)</span> hypothesis means the coin is fair.
And <span class="math inline">\(H_1\)</span> claims that head comes up more often than tails.
We need two random variables: the number of heads <span class="math inline">\(X\)</span>, and the number of coin tosses <span class="math inline">\(Y\)</span> .</p>
<!-- two types of experiments -->
<p>We conduct two experiments with a different setup, but with the same results:</p>
<ol style="list-style-type: decimal">
<li>Toss the coin 12 times. We observe 9 heads out of the 12 tosses.</li>
<li>Toss coin until 3 times tail was observed. The third head comes up at the 12-th toss.</li>
</ol>
<p>In experiment 1) we fixed <span class="math inline">\(Y\)</span>, the number of tosses.
IN experiment 2), we fixed <span class="math inline">\(X\)</span> the number of heads.</p>
<!-- likelihoodist view of the experiment -->
<p>Both experiments produce the same likelihood, up to a constant factor. Experiment 1)</p>
<p><span class="math display">\[L(\theta | X = 3) = \binom{12}{3} \theta^3 (1 - \theta)^9 (1 - \theta)^9 = 220 \theta^3 (1 - \theta)^9 \]</span></p>
<p>And experiment 2):</p>
<!-- the 2 out of 11 results from the fact that the last toss must be a head -->
<p><span class="math display">\[L(\theta | Y = 12) = \binom{11}{2} \theta^3 (1 - \theta)^9 (1 - \theta)^9 = 55 \theta^3 (1 - \theta)^9\]</span></p>
<p>So the likelihoodists say that both experiments carry the same evidence.
And they would get the same likelihood intervals for both experiments, for example.</p>
<!-- Frequentist view on the experiments -->
<p>Frequentists would come to different conclusion for both experiments. [^vidakovic9999]
Frequentists incorporate results that did not happen, but are dependent on how the experiments are conducted.
For both hypothesis tests, <span class="math inline">\(H_1\)</span> is framed as getting a more extreme event than the one observed, both in the direction of a higher probability for heads.
But we never observed more extreme events.
We just take what we observed (9 times head) and pretend that we observe the event more often.
What “more extreme” means depends on whether we are in experiment 1) or 2).<br />
When the number of tosses is fixed, the <span class="math inline">\(H_1\)</span> includes probabilities of fictional experiments where 9,11, or 12 heads were observed.
In the other experiment, the number of tails is fixed.
More extreme in experiment 2) means all possible experiments where we observe more than 12 tosses.
Which includes, for example that we tossed it 10234 times.
Remember that number of tails <span class="math inline">\(X = 3\)</span> is fixed: more repetitions means that <span class="math inline">\(\theta &gt; 0.5\)</span> is more likely.</p>
<p>When testing <span class="math inline">\(H_0\)</span> vs. <span class="math inline">\(H_1\)</span> in experiment 1), we get:</p>
<p><span class="math display">\[P(X \geq 9| H_0) = \sum_{x = 9}^{12} \binom{x}{12} 0.5^x (1 - 0.5)^{12 - x} = 0.073\]</span></p>
<p>For a significance level of <span class="math inline">\(\alpha = 0.05\)</span> we would not reject the hypothesis of a fair coin.</p>
<p>For experiment 2) we assume a negative Binomial distribution:</p>
<p><span class="math display">\[P(Y \geq 9 | H_0) = \sum_{y=9}^{\infty}\binom{3 + y - 1}{2} 0.5^y 0.5^3 = 0.0327\]</span></p>
<p>The p-value is now smaller than 0.05, and with that the coin is significantly unfair.</p>
</div>
<div id="stopping-criteria" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Stopping Criteria<a href="likelihoodism.html#stopping-criteria" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This is becoming a bashing for frequentist inference.
For frequentist inference, how data are sampled or how the experiments was designed has an influence on the results.</p>
<p>This has much more subtle consequences than I’ve illustrated so far.
Imagine you are a statistician, and a domain experts ask you to do an analysis with 1000 data points.
As a frequentist, you need to know how these 1000 data points were collected.
What was the stopping criterion?
If the domain expert just planned to collect 1000, that’s fine.
But if the expert would say that, depending on the outcome of the analysis, she would collect even more data, than this changes the analysis, in a violation of the likelihood principle.</p>
</div>
<div id="strengths-3" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Strengths<a href="likelihoodism.html#strengths-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Likelihoodism inherits all strengths of statistical models.</li>
<li>It’s coherent modeling approach that all information is contained in the likelihood. Frequentism is rather fragmented with different approaches such as statistical tests and confidence intervals.</li>
<li>Like Bayesian inference, likelihoodist inference conforms to the likelihood principle.</li>
<li>Therefore likelihoodism is also not dependent on the experimental design like frequentism is.</li>
<li>Arguably the most objective of the statistical modeling mindsets. Works with prior distributions and without imagined experiments.</li>
<li>Likelihoodist ideas can enhance reporting of Bayesian results. For example, by also reporting likelihood ratios of models without using the priors.</li>
<li>Compared to frequentism: A null hypothesis significance test might reject <span class="math inline">\(H_0\)</span>, even though the evidence for <span class="math inline">\(H_0\)</span> is larger than for some point <span class="math inline">\(H_1\)</span>. Likelihoodism does not have this problem.</li>
</ul>
</div>
<div id="limitations-3" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Limitations<a href="likelihoodism.html#limitations-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Likelihoodism doesn’t give guidance in the form of belief or decision. Evidence is less practical, and the statistician has no clear answer on what do or think which model is the correct one. This is the strongest argument against likelihoodism, and maybe the reason why we don’t see it in practice.</li>
<li>The statistical hypotheses hypotheses to be compared with likelihood ratios have to be specific, simple hypothesis where all parameters are specified. Compound hypothesis for intervals of parameters are not possible. Likelihoodism can’t compare <span class="math inline">\(\theta &gt; 0\)</span> versus <span class="math inline">\(\theta \leq 0\)</span>. Only, for example <span class="math inline">\(\theta = 1\)</span> against <span class="math inline">\(\theta = 0\)</span>.</li>
<li>Likelihoodism allows only relative claims. It can’t give the probability that some statistical hypothesis is true – only how its evidence compares to another hypothesis.</li>
</ul>
<p>I found<span class="citation"><sup><a href="#ref-gandenberger2016not" role="doc-biblioref">11</a></sup></span> a good source on criticism of likelihoodism.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-richard2017statistical" class="csl-entry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline">Richard R. Statistical evidence: A likelihood paradigm. Routledge; 2017. </div>
</div>
<div id="ref-birnbaum1962foundations" class="csl-entry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline">Birnbaum A. On the foundations of statistical inference. Journal of the American Statistical Association. 1962;57(298):269–306. </div>
</div>
<div id="ref-hacking1965logic" class="csl-entry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline">Hacking I. Logic of statistical inference. 1965; </div>
</div>
<div id="ref-gandenberger2016not" class="csl-entry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline">Gandenberger G. Why i am not a likelihoodist. Ann Arbor, MI: Michigan Publishing, University of Michigan Library; 2016. </div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="7">
<li id="fn7"><p>Not interpreted in the Bayesian way. Just that <span class="math inline">\(X\)</span> are seen as given.<a href="likelihoodism.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>The likelihood principle is of course violated if the data is used to inform the pior. For example empirical priors.<a href="likelihoodism.html#fnref8" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayesian-inference.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="causal-inference.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": false,
"twitter": true,
"linkedin": true,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/christophM/modeling-mindsets/edit/master/manuscript/likelihoodism.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
